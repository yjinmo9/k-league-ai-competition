{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8477ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/744013603.py:205: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/744013603.py:263: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train.drop(columns=drop_cols).fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/744013603.py:264: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns]).fillna(0)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.76 GB / 16.00 GB (23.5%)\n",
      "Disk Space Avail:   79.83 GB / 460.43 GB (17.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_xbin_10min\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_x_bin\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3784.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (1.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 13891, Val Rows: 1544\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 599.30s of the 599.30s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.9/3.5 GB\n",
      "\t-2.1559\t = Validation score   (-log_loss)\n",
      "\t76.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 522.75s of the 522.75s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.3/3.7 GB\n",
      "\t-2.1198\t = Validation score   (-log_loss)\n",
      "\t314.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 207.93s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.2 GB\n",
      "\tEnsemble Weights: {'CatBoost': 0.636, 'LightGBM': 0.364}\n",
      "\t-2.1045\t = Validation score   (-log_loss)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 392.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 45308.9 rows/s (1544 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_xbin_10min\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       4.29 GB / 16.00 GB (26.8%)\n",
      "Disk Space Avail:   79.52 GB / 460.43 GB (17.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_ybin_10min\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_y_bin\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 14\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4444.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (1.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 13891, Val Rows: 1544\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 599.43s of the 599.43s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.7/4.2 GB\n",
      "\t-1.8086\t = Validation score   (-log_loss)\n",
      "\t56.99s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 542.36s of the 542.36s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.2/3.9 GB\n",
      "\t-1.7928\t = Validation score   (-log_loss)\n",
      "\t320.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 222.22s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.9 GB\n",
      "\tEnsemble Weights: {'CatBoost': 0.583, 'LightGBM': 0.417}\n",
      "\t-1.7776\t = Validation score   (-log_loss)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 377.88s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 33725.5 rows/s (1544 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_ybin_10min\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved submission: submission_bin_softavg_10min_full.csv\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  66.803431   4.271621\n",
      "1     153363_2  37.273295  62.977507\n",
      "2     153363_6  35.577678  65.158605\n",
      "3     153363_7  52.682555   4.893350\n",
      "4     153363_8  82.271261   7.022204\n",
      "5     153363_9  76.906888  67.139524\n",
      "6    153363_10  57.111227   5.347243\n",
      "7    153363_12  66.878253   4.063411\n",
      "8    153363_13  32.663202  66.192351\n",
      "9    153363_15  59.366657   4.496272\n",
      "Columns: ['game_episode', 'end_x', 'end_y']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# ======================\n",
    "# 0. 설정\n",
    "# ======================\n",
    "BASE_PATH = \"open_track1/\"  # ✅ 너 환경에 맞게 폴더 경로만 확인\n",
    "\n",
    "PATH_TRAIN = os.path.join(BASE_PATH, \"train.csv\")\n",
    "PATH_TEST_INDEX = os.path.join(BASE_PATH, \"test.csv\")  # test 파일들의 path가 적힌 인덱스\n",
    "PATH_MATCH_INFO = os.path.join(BASE_PATH, \"match_info.csv\")\n",
    "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "# last K events\n",
    "K = 20\n",
    "\n",
    "# bin 설정\n",
    "BIN_X = 5.0\n",
    "BIN_Y = 5.0\n",
    "NX = int(np.ceil(105 / BIN_X))  # 21\n",
    "NY = int(np.ceil(68 / BIN_Y))   # 14\n",
    "\n",
    "# 학습 시간 제한: 10분\n",
    "TIME_LIMIT = 600\n",
    "PRESETS = \"medium_quality\"  # ⭐ 10분 제한에서는 good_quality보다 medium이 안정적/빠름\n",
    "\n",
    "HYPERPARAMETERS = {\n",
    "    \"GBM\": {},\n",
    "    \"CAT\": {},\n",
    "}\n",
    "\n",
    "TOP_K_SOFTAVG = 3  # 확률 가중 평균에서 사용할 top-k\n",
    "\n",
    "# ======================\n",
    "# 1. 데이터 로드\n",
    "# ======================\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test_index = pd.read_csv(PATH_TEST_INDEX)\n",
    "match_info = pd.read_csv(PATH_MATCH_INFO)\n",
    "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
    "\n",
    "# test 이벤트 파일들 로드 (test.csv에 path가 존재한다고 가정)\n",
    "test_events_list = []\n",
    "for _, row in test_index.iterrows():\n",
    "    test_path = os.path.join(BASE_PATH, str(row[\"path\"]).lstrip(\"./\"))\n",
    "    df_ep = pd.read_csv(test_path)\n",
    "    test_events_list.append(df_ep)\n",
    "\n",
    "test_events = pd.concat(test_events_list, ignore_index=True)\n",
    "\n",
    "# train/test 합치기\n",
    "train[\"is_train\"] = 1\n",
    "test_events[\"is_train\"] = 0\n",
    "events = pd.concat([train, test_events], ignore_index=True)\n",
    "\n",
    "# ======================\n",
    "# 2. 기본 정렬 + episode 인덱스\n",
    "# ======================\n",
    "# (컬럼명은 네 데이터 기준: game_episode, time_seconds, action_id 사용)\n",
    "events = events.sort_values(\n",
    "    [\"game_episode\", \"time_seconds\", \"action_id\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
    "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
    "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
    "\n",
    "# ======================\n",
    "# 3. 기본 수치 피처 (시간/이동)\n",
    "# ======================\n",
    "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
    "events[\"dt\"] = (events[\"time_seconds\"] - events[\"prev_time\"]).fillna(0)\n",
    "\n",
    "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
    "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
    "events[\"dist\"] = np.sqrt(events[\"dx\"]**2 + events[\"dy\"]**2)\n",
    "\n",
    "# dt=0 회피\n",
    "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
    "\n",
    "# 구역/레인\n",
    "events[\"x_zone\"] = (events[\"start_x\"] / (105/7)).astype(int).clip(0, 6)\n",
    "\n",
    "events[\"lane\"] = pd.cut(\n",
    "    events[\"start_y\"],\n",
    "    bins=[0, 68/3, 2*68/3, 68],\n",
    "    labels=[0, 1, 2],\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "# ======================\n",
    "# 4. 공간/각도/코너 관련 피처\n",
    "# ======================\n",
    "# 골대 중앙(105, 34) 기준 각도\n",
    "events[\"angle_to_goal\"] = np.arctan2(\n",
    "    34 - events[\"start_y\"],\n",
    "    105 - events[\"start_x\"]\n",
    ") * 180 / np.pi\n",
    "\n",
    "# 코너까지 거리 (상단/하단 코너)\n",
    "events[\"dist_corner_top\"] = np.sqrt((105 - events[\"start_x\"])**2 + (0 - events[\"start_y\"])**2)\n",
    "events[\"dist_corner_bottom\"] = np.sqrt((105 - events[\"start_x\"])**2 + (68 - events[\"start_y\"])**2)\n",
    "events[\"dist_to_nearest_corner\"] = events[[\"dist_corner_top\", \"dist_corner_bottom\"]].min(axis=1)\n",
    "\n",
    "# 코너 지역 플래그 (대충 x>100 & y near sideline)\n",
    "events[\"is_corner_area\"] = (\n",
    "    (events[\"start_x\"] > 100) &\n",
    "    ((events[\"start_y\"] < 5) | (events[\"start_y\"] > 63))\n",
    ").astype(int)\n",
    "\n",
    "# 코너 상호작용\n",
    "events[\"angle_goal_x_corner\"] = events[\"angle_to_goal\"] * events[\"is_corner_area\"]\n",
    "events[\"dist_corner_x_angle\"] = events[\"dist_to_nearest_corner\"] * events[\"angle_to_goal\"]\n",
    "\n",
    "# 핵심 3개 (네가 추가했던 것 유지)\n",
    "events[\"dist_to_sideline\"] = events[\"start_y\"].apply(lambda y: min(y, 68 - y))\n",
    "\n",
    "events[\"angle_to_goal_center\"] = np.arctan2(\n",
    "    34 - events[\"start_y\"],\n",
    "    105 - events[\"start_x\"]\n",
    ")  # radians\n",
    "\n",
    "events[\"time_pos_inter\"] = events[\"ep_idx_norm\"] * events[\"start_x\"]\n",
    "\n",
    "# ======================\n",
    "# 5. 라벨 생성 (Train의 마지막 이벤트 end_x/end_y -> bin)\n",
    "# ======================\n",
    "train_events = events[events[\"is_train\"] == 1].copy()\n",
    "\n",
    "last_events = (\n",
    "    train_events\n",
    "    .groupby(\"game_episode\", as_index=False)\n",
    "    .tail(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "last_events[\"target_x_bin\"] = (last_events[\"end_x\"] / BIN_X).astype(int).clip(0, NX - 1)\n",
    "last_events[\"target_y_bin\"] = (last_events[\"end_y\"] / BIN_Y).astype(int).clip(0, NY - 1)\n",
    "\n",
    "labels = last_events[[\"game_episode\", \"target_x_bin\", \"target_y_bin\"]].copy()\n",
    "\n",
    "# ======================\n",
    "# 6. episode-level meta 구성 (마지막 이벤트 기준)\n",
    "# ======================\n",
    "# (데이터에 team_id, is_home, period_id 등이 있다고 가정)\n",
    "ep_meta = last_events[[\n",
    "    \"game_episode\", \"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"\n",
    "]].rename(columns={\"team_id\": \"final_team_id\"}).copy()\n",
    "\n",
    "# 경기 시간(분) 간단 변환\n",
    "ep_meta[\"game_clock_min\"] = np.where(\n",
    "    ep_meta[\"period_id\"] == 1,\n",
    "    ep_meta[\"time_seconds\"] / 60,\n",
    "    45 + ep_meta[\"time_seconds\"] / 60\n",
    ")\n",
    "\n",
    "# final_team_id 붙이기\n",
    "events = events.merge(\n",
    "    ep_meta[[\"game_episode\", \"final_team_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
    "\n",
    "# ======================\n",
    "# 7. 누수 방지: 마지막 이벤트 마스킹\n",
    "# ======================\n",
    "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
    "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
    "\n",
    "mask_last = events[\"is_last\"] == 1\n",
    "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
    "    events.loc[mask_last, col] = np.nan\n",
    "\n",
    "# ======================\n",
    "# 8. 카테고리 인코딩\n",
    "# ======================\n",
    "events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA__\")\n",
    "events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA__\")\n",
    "\n",
    "events[\"type_id\"] = LabelEncoder().fit_transform(events[\"type_name\"])\n",
    "events[\"res_id\"] = LabelEncoder().fit_transform(events[\"result_name\"])\n",
    "\n",
    "# team_id가 문자열일 수도 있으니 처리\n",
    "if events[\"team_id\"].dtype == \"object\":\n",
    "    events[\"team_id_enc\"] = LabelEncoder().fit_transform(events[\"team_id\"].astype(str))\n",
    "else:\n",
    "    events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
    "\n",
    "# ======================\n",
    "# 9. lastK 추출\n",
    "# ======================\n",
    "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(lambda s: s.max() - s)\n",
    "lastK = events[events[\"rev_idx\"] < K].copy()\n",
    "\n",
    "def assign_pos(df):\n",
    "    df = df.sort_values(\"event_idx\")\n",
    "    L = len(df)\n",
    "    df[\"pos_in_K\"] = np.arange(K - L, K)\n",
    "    return df\n",
    "\n",
    "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos)\n",
    "\n",
    "# ======================\n",
    "# 10. wide feature 생성 (pivot)\n",
    "# ======================\n",
    "num_cols = [\n",
    "    \"start_x\", \"start_y\", \"end_x\", \"end_y\",\n",
    "    \"dx\", \"dy\", \"dist\", \"speed\", \"dt\",\n",
    "    \"ep_idx_norm\", \"x_zone\", \"lane\",\n",
    "    \"is_final_team\",\n",
    "    \"angle_to_goal\", \"dist_to_nearest_corner\", \"is_corner_area\",\n",
    "    \"angle_goal_x_corner\", \"dist_corner_x_angle\",\n",
    "    \"dist_to_sideline\", \"angle_to_goal_center\", \"time_pos_inter\"\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"type_id\", \"res_id\", \"team_id_enc\",\n",
    "    \"is_home\", \"period_id\", \"is_last\"\n",
    "]\n",
    "\n",
    "wide = lastK[[\"game_episode\", \"pos_in_K\"] + num_cols + cat_cols].copy()\n",
    "\n",
    "wide_num = wide.pivot_table(\n",
    "    index=\"game_episode\", columns=\"pos_in_K\",\n",
    "    values=num_cols, aggfunc=\"first\"\n",
    ")\n",
    "wide_cat = wide.pivot_table(\n",
    "    index=\"game_episode\", columns=\"pos_in_K\",\n",
    "    values=cat_cols, aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "wide_num.columns = [f\"{c}_{int(p)}\" for c, p in wide_num.columns]\n",
    "wide_cat.columns = [f\"{c}_{int(p)}\" for c, p in wide_cat.columns]\n",
    "\n",
    "X = pd.concat([wide_num, wide_cat], axis=1).reset_index()\n",
    "\n",
    "# episode meta 붙이기\n",
    "X = X.merge(\n",
    "    ep_meta[[\"game_episode\", \"game_id\", \"game_clock_min\", \"final_team_id\", \"is_home\", \"period_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 라벨 붙이기 (train만 존재)\n",
    "X = X.merge(labels, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# ======================\n",
    "# 11. Train / Test 분리\n",
    "# ======================\n",
    "train_mask = X[\"game_episode\"].isin(labels[\"game_episode\"])\n",
    "X_train = X[train_mask].copy()\n",
    "X_test = X[~train_mask].copy()\n",
    "\n",
    "y_train_x = X_train[\"target_x_bin\"].astype(int)\n",
    "y_train_y = X_train[\"target_y_bin\"].astype(int)\n",
    "\n",
    "drop_cols = [\"game_episode\", \"game_id\", \"target_x_bin\", \"target_y_bin\"]\n",
    "\n",
    "X_train_feat = X_train.drop(columns=drop_cols).fillna(0)\n",
    "X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns]).fillna(0)\n",
    "\n",
    "# ======================\n",
    "# 12. AutoGluon 학습 (10분 제한)\n",
    "# ======================\n",
    "# X-bin\n",
    "train_x = X_train_feat.copy()\n",
    "train_x[\"target_x_bin\"] = y_train_x\n",
    "\n",
    "predictor_x = TabularPredictor(\n",
    "    label=\"target_x_bin\",\n",
    "    problem_type=\"multiclass\",\n",
    "    eval_metric=\"log_loss\",\n",
    "    path=\"ag_models_xbin_10min\"\n",
    ").fit(\n",
    "    train_data=train_x,\n",
    "    time_limit=TIME_LIMIT,\n",
    "    presets=PRESETS,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# Y-bin\n",
    "train_y = X_train_feat.copy()\n",
    "train_y[\"target_y_bin\"] = y_train_y\n",
    "\n",
    "predictor_y = TabularPredictor(\n",
    "    label=\"target_y_bin\",\n",
    "    problem_type=\"multiclass\",\n",
    "    eval_metric=\"log_loss\",\n",
    "    path=\"ag_models_ybin_10min\"\n",
    ").fit(\n",
    "    train_data=train_y,\n",
    "    time_limit=TIME_LIMIT,\n",
    "    presets=PRESETS,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 13. 확률 → 연속 좌표 복원 (RMSE 대응 핵심)\n",
    "# ======================\n",
    "def proba_to_coord(proba: np.ndarray, bin_size: float, top_k: int = 3) -> float:\n",
    "    \"\"\"\n",
    "    proba: (num_bins,) 확률벡터\n",
    "    bin_size: BIN_X or BIN_Y\n",
    "    \"\"\"\n",
    "    top_idx = np.argsort(proba)[-top_k:]\n",
    "    weights = proba[top_idx]\n",
    "    wsum = weights.sum()\n",
    "    if wsum <= 0:\n",
    "        # 예외 처리: 확률합이 0이면 argmax로 fallback\n",
    "        idx = int(np.argmax(proba))\n",
    "        return idx * bin_size + bin_size / 2\n",
    "\n",
    "    weights = weights / wsum\n",
    "    coords = top_idx * bin_size + bin_size / 2\n",
    "    return float(np.sum(weights * coords))\n",
    "\n",
    "# ======================\n",
    "# 14. Test 예측 (SoftAvg)\n",
    "# ======================\n",
    "proba_x = predictor_x.predict_proba(X_test_feat).values  # (N, NX)\n",
    "proba_y = predictor_y.predict_proba(X_test_feat).values  # (N, NY)\n",
    "\n",
    "pred_x = np.array([proba_to_coord(p, BIN_X, TOP_K_SOFTAVG) for p in proba_x])\n",
    "pred_y = np.array([proba_to_coord(p, BIN_Y, TOP_K_SOFTAVG) for p in proba_y])\n",
    "\n",
    "pred_x = np.clip(pred_x, 0, 105)\n",
    "pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "# ======================\n",
    "# 15. 제출 파일 생성 (형식 충돌 방지: 덮어쓰기 방식)\n",
    "# ======================\n",
    "pred_df = X_test[[\"game_episode\"]].copy()\n",
    "pred_df[\"end_x\"] = pred_x\n",
    "pred_df[\"end_y\"] = pred_y\n",
    "\n",
    "# sample_submission이 end_x/end_y를 이미 갖고 있으니 \"merge suffix 충돌\" 피해서 덮어쓰기\n",
    "sub = sample_sub.copy().set_index(\"game_episode\")\n",
    "pred_df = pred_df.set_index(\"game_episode\")\n",
    "\n",
    "sub[\"end_x\"] = pred_df[\"end_x\"]\n",
    "sub[\"end_y\"] = pred_df[\"end_y\"]\n",
    "\n",
    "sub = sub.reset_index()\n",
    "\n",
    "# 컬럼 순서 강제\n",
    "sub = sub[[\"game_episode\", \"end_x\", \"end_y\"]]\n",
    "\n",
    "out_name = \"submission_bin_softavg_10min_full.csv\"\n",
    "sub.to_csv(out_name, index=False)\n",
    "\n",
    "print(f\"✅ Saved submission: {out_name}\")\n",
    "print(sub.head(10))\n",
    "print(\"Columns:\", sub.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11416c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 경로: ag_models_xbin_10min, ag_models_ybin_10min\n",
      "제출 파일: submission_bin_softavg_10min_full.csv\n",
      "✓ 모델 로드 완료\n",
      "======================================================================\n",
      "3단계 검증법 (BIN + SoftAvg)\n",
      "======================================================================\n",
      "\n",
      "1단계: Feature Importance 건너뜀\n",
      "\n",
      "======================================================================\n",
      "[2단계] Leaderboard (log_loss 기준)\n",
      "======================================================================\n",
      "\n",
      "[X-bin 모델]\n",
      "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0  WeightedEnsemble_L2  -2.104453    log_loss       0.034077  391.328376   \n",
      "1             CatBoost  -2.119763    log_loss       0.007326  314.799729   \n",
      "2             LightGBM  -2.155898    log_loss       0.025724   76.478824   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.001027           0.049823            2       True   \n",
      "1                0.007326         314.799729            1       True   \n",
      "2                0.025724          76.478824            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          3  \n",
      "1          2  \n",
      "2          1  \n",
      "✓ Best log_loss (X): -2.10445\n",
      "\n",
      "[Y-bin 모델]\n",
      "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0  WeightedEnsemble_L2  -1.777630    log_loss       0.045781  377.135492   \n",
      "1             CatBoost  -1.792829    log_loss       0.009611  320.104677   \n",
      "2             LightGBM  -1.808585    log_loss       0.035008   56.987811   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.001162           0.043004            2       True   \n",
      "1                0.009611         320.104677            1       True   \n",
      "2                0.035008          56.987811            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          3  \n",
      "1          2  \n",
      "2          1  \n",
      "✓ Best log_loss (Y): -1.77763\n",
      "\n",
      "[Train Accuracy 참고용]\n",
      "Train Accuracy X-bin: 0.7811\n",
      "Train Accuracy Y-bin: 0.8007\n",
      "\n",
      "======================================================================\n",
      "[3단계] 상황별 예측 분포 분석\n",
      "======================================================================\n",
      "\n",
      "[전체 예측 좌표 통계]\n",
      "             end_x        end_y\n",
      "count  2414.000000  2414.000000\n",
      "mean     65.800328    33.780747\n",
      "std      21.675982    24.549730\n",
      "min       4.665375     2.668219\n",
      "25%      48.307574     6.683084\n",
      "50%      68.200808    33.914304\n",
      "75%      84.929168    60.943495\n",
      "max     101.748283    67.379120\n",
      "\n",
      "[공격 방향 sanity check]\n",
      "end_x 평균: 65.80 (값이 클수록 전진 패스 성향)\n",
      "\n",
      "[Y축 분산 sanity check]\n",
      "end_y 표준편차: 24.55 (너무 작으면 중앙 쏠림 의심)\n",
      "\n",
      "======================================================================\n",
      "검증 완료 (BIN + SoftAvg)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# 결과 확인 및 3단계 검증법\n",
    "# (BIN Classification + Soft Average 버전)\n",
    "# =======================\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =======================\n",
    "# 실행 플래그\n",
    "# =======================\n",
    "RUN_STEP1_FEATURE_IMPORTANCE = False   # ⚠️ 매우 느림\n",
    "RUN_STEP2_LEADERBOARD = True\n",
    "RUN_STEP3_SITUATIONAL_ANALYSIS = True\n",
    "\n",
    "# =======================\n",
    "# 모델 경로 & 제출 파일 (⭐ 중요: 새 경로)\n",
    "# =======================\n",
    "model_path_x = \"ag_models_xbin_10min\"\n",
    "model_path_y = \"ag_models_ybin_10min\"\n",
    "submission_filename = \"submission_bin_softavg_10min_full.csv\"\n",
    "\n",
    "print(f\"모델 경로: {model_path_x}, {model_path_y}\")\n",
    "print(f\"제출 파일: {submission_filename}\")\n",
    "\n",
    "# =======================\n",
    "# 모델 로드\n",
    "# =======================\n",
    "predictor_x = TabularPredictor.load(model_path_x)\n",
    "predictor_y = TabularPredictor.load(model_path_y)\n",
    "\n",
    "print(\"✓ 모델 로드 완료\")\n",
    "print(\"=\" * 70)\n",
    "print(\"3단계 검증법 (BIN + SoftAvg)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# 1단계: Feature Importance (참고용)\n",
    "# ============================================================\n",
    "if RUN_STEP1_FEATURE_IMPORTANCE:\n",
    "    print(\"\\n[1단계] Feature Importance 확인\")\n",
    "\n",
    "    print(\"\\n[X-bin 모델]\")\n",
    "    fi_x = predictor_x.feature_importance(data=X_train_feat)\n",
    "    print(fi_x.iloc[:, 0].sort_values(ascending=False).head(20))\n",
    "\n",
    "    print(\"\\n[Y-bin 모델]\")\n",
    "    fi_y = predictor_y.feature_importance(data=X_train_feat)\n",
    "    print(fi_y.iloc[:, 0].sort_values(ascending=False).head(20))\n",
    "\n",
    "else:\n",
    "    print(\"\\n1단계: Feature Importance 건너뜀\")\n",
    "\n",
    "# ============================================================\n",
    "# 2단계: Leaderboard (log_loss 기준)\n",
    "# ============================================================\n",
    "if RUN_STEP2_LEADERBOARD:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[2단계] Leaderboard (log_loss 기준)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\n[X-bin 모델]\")\n",
    "    lb_x = predictor_x.leaderboard(silent=True)\n",
    "    print(lb_x.head(5))\n",
    "    print(f\"✓ Best log_loss (X): {lb_x.iloc[0]['score_val']:.5f}\")\n",
    "\n",
    "    print(\"\\n[Y-bin 모델]\")\n",
    "    lb_y = predictor_y.leaderboard(silent=True)\n",
    "    print(lb_y.head(5))\n",
    "    print(f\"✓ Best log_loss (Y): {lb_y.iloc[0]['score_val']:.5f}\")\n",
    "\n",
    "    # ⚠️ Train accuracy는 참고용 (RMSE 대회)\n",
    "    print(\"\\n[Train Accuracy 참고용]\")\n",
    "    pred_x_train = predictor_x.predict(X_train_feat)\n",
    "    pred_y_train = predictor_y.predict(X_train_feat)\n",
    "\n",
    "    acc_x = (pred_x_train == y_train_x).mean()\n",
    "    acc_y = (pred_y_train == y_train_y).mean()\n",
    "\n",
    "    print(f\"Train Accuracy X-bin: {acc_x:.4f}\")\n",
    "    print(f\"Train Accuracy Y-bin: {acc_y:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n2단계: Leaderboard 건너뜀\")\n",
    "\n",
    "# ============================================================\n",
    "# 3단계: 상황별 예측 분포 분석 (⭐ 핵심)\n",
    "# ============================================================\n",
    "if RUN_STEP3_SITUATIONAL_ANALYSIS:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[3단계] 상황별 예측 분포 분석\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # 제출 파일 로드\n",
    "    sub = pd.read_csv(submission_filename)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # X_test에서 존재하는 컬럼만 안전하게 선택\n",
    "    # --------------------------------------------------------\n",
    "    merge_cols = [\"game_episode\"]\n",
    "\n",
    "    if \"period_id\" in X_test.columns:\n",
    "        merge_cols.append(\"period_id\")\n",
    "\n",
    "    if \"game_clock_min\" in X_test.columns:\n",
    "        merge_cols.append(\"game_clock_min\")\n",
    "\n",
    "    # 병합\n",
    "    merged = X_test[merge_cols].merge(\n",
    "        sub, on=\"game_episode\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 컬럼 존재 sanity check\n",
    "    # --------------------------------------------------------\n",
    "    if (\"end_x\" not in merged.columns) or (\"end_y\" not in merged.columns):\n",
    "        raise ValueError(\n",
    "            f\"❌ 제출 파일 컬럼 오류\\n\"\n",
    "            f\"merged columns: {merged.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 전체 분포\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n[전체 예측 좌표 통계]\")\n",
    "    print(merged[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 전 / 후반 비교\n",
    "    # --------------------------------------------------------\n",
    "    if \"period_id\" in merged.columns:\n",
    "        first_half = merged[merged[\"period_id\"] == 1]\n",
    "        second_half = merged[merged[\"period_id\"] == 2]\n",
    "\n",
    "        if len(first_half) > 0:\n",
    "            print(\"\\n[전반전 예측 분포]\")\n",
    "            print(first_half[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "        if len(second_half) > 0:\n",
    "            print(\"\\n[후반전 예측 분포]\")\n",
    "            print(second_half[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 공격 방향 sanity check (X축)\n",
    "    # --------------------------------------------------------\n",
    "    mean_x = merged[\"end_x\"].mean()\n",
    "    print(\"\\n[공격 방향 sanity check]\")\n",
    "    print(\n",
    "        f\"end_x 평균: {mean_x:.2f} \"\n",
    "        \"(값이 클수록 전진 패스 성향)\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Y축 중앙 쏠림 sanity check\n",
    "    # --------------------------------------------------------\n",
    "    std_y = merged[\"end_y\"].std()\n",
    "    print(\"\\n[Y축 분산 sanity check]\")\n",
    "    print(\n",
    "        f\"end_y 표준편차: {std_y:.2f} \"\n",
    "        \"(너무 작으면 중앙 쏠림 의심)\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 시간대별 (선택)\n",
    "    # --------------------------------------------------------\n",
    "    if \"game_clock_min\" in merged.columns:\n",
    "        early = merged[merged[\"game_clock_min\"] < 30]\n",
    "        late = merged[merged[\"game_clock_min\"] > 75]\n",
    "\n",
    "        if len(early) > 0:\n",
    "            print(\"\\n[초반 (0~30분)]\")\n",
    "            print(early[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "        if len(late) > 0:\n",
    "            print(\"\\n[후반 막판 (75분~)]\")\n",
    "            print(late[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "else:\n",
    "    print(\"\\n3단계: 상황 분석 건너뜀\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"검증 완료 (BIN + SoftAvg)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
