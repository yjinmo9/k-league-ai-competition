{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8477ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/867682853.py:162: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/867682853.py:242: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train_feat.fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/867682853.py:243: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test_feat.fillna(0)\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models_x\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.32 GB / 16.00 GB (20.7%)\n",
      "Disk Space Avail:   10.39 GB / 460.43 GB (2.3%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "빠른 테스트 모드: CatBoost + LightGBM만 사용 (10분)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "X 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 378\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3358.20 MB\n",
      "\tTrain Data (Original)  Memory Usage: 46.35 MB (1.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 334 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 312 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('int', ['bool']) :  42 | ['ep_idx_norm_19', 'is_final_team_0', 'is_final_team_1', 'is_final_team_2', 'is_final_team_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t354 features in original data used to generate 354 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.21 MB (1.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 99.70s of the 149.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.73%)\n",
      "\t-11.9344\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.03s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 86.74s of the 136.63s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.60% memory usage per fold, 78.38%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=19.60%)\n",
      "\t-11.8782\t = Validation score   (-root_mean_squared_error)\n",
      "\t61.79s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.59s of the 72.82s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.2 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.583, 'LightGBM_BAG_L1': 0.417}\n",
      "\t-11.8211\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 72.80s of the 72.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.77%)\n",
      "\t-11.9073\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.56s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 64.41s of the 64.40s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.47% memory usage per fold, 52.94%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=26.47%)\n",
      "\t-11.8811\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.55s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.59s of the 26.23s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.0 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.5, 'LightGBM_BAG_L1': 0.357, 'LightGBM_BAG_L2': 0.143}\n",
      "\t-11.8189\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 123.81s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 3433.8 rows/s (1715 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.2/4.0 GB\n",
      "\t2.32s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0\n",
      "\t6.95s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.583, 'LightGBM_BAG_L1': 0.417}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.2/4.1 GB\n",
      "\t1.16s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0\n",
      "\t1.87s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.5, 'LightGBM_BAG_L1': 0.357, 'LightGBM_BAG_L2': 0.143}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 12.63s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val   fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3_FULL     -11.677812 -11.818869  root_mean_squared_error        0.023416          None  10.425614                 0.000992                   None           0.005845            3       True          6\n",
      "1  WeightedEnsemble_L2_FULL     -11.687529 -11.821073  root_mean_squared_error        0.018212          None   9.273455                 0.001018                   None           0.010068            2       True          3\n",
      "2      LightGBM_BAG_L2_FULL     -11.714382 -11.907331  root_mean_squared_error        0.022424          None  10.419769                 0.005230                   None           1.156382            2       True          4\n",
      "3      CatBoost_BAG_L1_FULL     -11.723868 -11.878230  root_mean_squared_error        0.008533          None   6.948031                 0.008533                   None           6.948031            1       True          2\n",
      "4      CatBoost_BAG_L2_FULL     -11.771259 -11.881136  root_mean_squared_error        0.024205          None  11.136596                 0.007011                   None           1.873209            2       True          5\n",
      "5      LightGBM_BAG_L1_FULL     -11.834300 -11.934395  root_mean_squared_error        0.008661          None   2.315356                 0.008661                   None           2.315356            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t137s\t = DyStack   runtime |\t463s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 463s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 378\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4153.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.14 MB (1.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 333 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 312 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('int', ['bool']) :  41 | ['ep_idx_norm_19', 'is_final_team_0', 'is_final_team_1', 'is_final_team_2', 'is_final_team_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t353 features in original data used to generate 353 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.34 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 308.31s of the 462.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.64%)\n",
      "\t-11.8744\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.42s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 296.41s of the 450.68s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 25.62% memory usage per fold, 51.24%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=25.62%)\n",
      "\t-11.8548\t = Validation score   (-root_mean_squared_error)\n",
      "\t67.43s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 381.62s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.8 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.533, 'LightGBM_BAG_L1': 0.467}\n",
      "\t-11.7883\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 381.60s of the 381.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.11%)\n",
      "\t-11.8856\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.4s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 373.59s of the 373.57s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.88% memory usage per fold, 43.77%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=21.88%)\n",
      "\t-11.8357\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.1s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 328.79s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.8 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.5, 'LightGBM_BAG_L1': 0.417, 'LightGBM_BAG_L2': 0.042, 'CatBoost_BAG_L2': 0.042}\n",
      "\t-11.7884\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 134.28s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6378.4 rows/s (1930 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.8 GB\n",
      "\t2.82s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0\n",
      "\t6.44s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.533, 'LightGBM_BAG_L1': 0.467}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.8 GB\n",
      "\t1.42s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0\n",
      "\t2.9s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.5, 'LightGBM_BAG_L1': 0.417, 'LightGBM_BAG_L2': 0.042, 'CatBoost_BAG_L2': 0.042}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 13.91s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models_y\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.61 GB / 16.00 GB (22.6%)\n",
      "Disk Space Avail:   10.34 GB / 460.43 GB (2.2%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Y 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 378\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3345.70 MB\n",
      "\tTrain Data (Original)  Memory Usage: 46.35 MB (1.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 334 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 312 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('int', ['bool']) :  42 | ['ep_idx_norm_19', 'is_final_team_0', 'is_final_team_1', 'is_final_team_2', 'is_final_team_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t354 features in original data used to generate 354 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.21 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 99.68s of the 149.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.32%)\n",
      "\t-13.0851\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.36s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 87.19s of the 137.07s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.75% memory usage per fold, 49.50%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=24.75%)\n",
      "\t-13.1345\t = Validation score   (-root_mean_squared_error)\n",
      "\t73.89s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.55s of the 61.25s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.2 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.583, 'CatBoost_BAG_L1': 0.417}\n",
      "\t-13.0355\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 61.23s of the 61.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.68%)\n",
      "\t-13.1829\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.53s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 53.02s of the 53.01s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 25.52% memory usage per fold, 51.04%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=25.52%)\n",
      "\t-13.0827\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.06s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.55s of the 18.28s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.2 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.56, 'CatBoost_BAG_L1': 0.4, 'CatBoost_BAG_L2': 0.04}\n",
      "\t-13.0357\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 131.77s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5690.6 rows/s (1715 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.2/4.2 GB\n",
      "\t1.78s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0\n",
      "\t6.32s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.583, 'CatBoost_BAG_L1': 0.417}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.2/4.3 GB\n",
      "\t1.09s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0\n",
      "\t1.61s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.56, 'CatBoost_BAG_L1': 0.4, 'CatBoost_BAG_L2': 0.04}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 11.08s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val  fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3_FULL     -13.456364 -13.035701  root_mean_squared_error        0.023165          None  9.719826                 0.001107                   None           0.006416            3       True          6\n",
      "1  WeightedEnsemble_L2_FULL     -13.457688 -13.035543  root_mean_squared_error        0.015498          None  8.113163                 0.000974                   None           0.007760            2       True          3\n",
      "2      CatBoost_BAG_L2_FULL     -13.458418 -13.082717  root_mean_squared_error        0.022058          None  9.713410                 0.007534                   None           1.608007            2       True          5\n",
      "3      LightGBM_BAG_L2_FULL     -13.473877 -13.182931  root_mean_squared_error        0.019103          None  9.197505                 0.004579                   None           1.092102            2       True          4\n",
      "4      LightGBM_BAG_L1_FULL     -13.520720 -13.085120  root_mean_squared_error        0.007291          None  1.780905                 0.007291                   None           1.780905            1       True          1\n",
      "5      CatBoost_BAG_L1_FULL     -13.531714 -13.134495  root_mean_squared_error        0.007233          None  6.324498                 0.007233                   None           6.324498            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t143s\t = DyStack   runtime |\t457s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 457s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 378\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4343.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.14 MB (1.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 333 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 312 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('int', ['bool']) :  41 | ['ep_idx_norm_19', 'is_final_team_0', 'is_final_team_1', 'is_final_team_2', 'is_final_team_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t353 features in original data used to generate 353 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.34 MB (0.8% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 304.06s of the 456.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.22%)\n",
      "\t-13.109\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.21s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 295.42s of the 447.56s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.29% memory usage per fold, 40.58%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.29%)\n",
      "\t-13.1512\t = Validation score   (-root_mean_squared_error)\n",
      "\t78.37s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 367.61s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.1 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.571, 'CatBoost_BAG_L1': 0.429}\n",
      "\t-13.0553\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 367.59s of the 367.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.48%)\n",
      "\t-13.1682\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.71s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 359.32s of the 359.30s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.85% memory usage per fold, 49.69%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=24.85%)\n",
      "\t-13.096\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 320.68s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.1 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.5, 'CatBoost_BAG_L1': 0.375, 'LightGBM_BAG_L2': 0.083, 'CatBoost_BAG_L2': 0.042}\n",
      "\t-13.0546\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 136.02s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 5564.1 rows/s (1930 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/4.1 GB\n",
      "\t1.74s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0\n",
      "\t7.2s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.571, 'CatBoost_BAG_L1': 0.429}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/4.2 GB\n",
      "\t1.21s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0\n",
      "\t1.93s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.5, 'CatBoost_BAG_L1': 0.375, 'LightGBM_BAG_L2': 0.083, 'CatBoost_BAG_L2': 0.042}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 12.4s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test 데이터 예측 중...\n",
      "==================================================\n",
      "Saved submission_autogluon_lastK.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# ----------------------\n",
    "# 0. 설정\n",
    "# ----------------------\n",
    "BASE_PATH = \"open_track1/\"\n",
    "PATH_TRAIN = os.path.join(BASE_PATH, \"train.csv\")\n",
    "PATH_TEST = os.path.join(BASE_PATH, \"test.csv\")\n",
    "PATH_MATCH_INFO = os.path.join(BASE_PATH, \"match_info.csv\")\n",
    "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "K = 20   # 마지막 K 이벤트 사용 (20~32 사이 선택)\n",
    "\n",
    "# ----------------------\n",
    "# 1. 데이터 로드\n",
    "# ----------------------\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test_index = pd.read_csv(PATH_TEST)\n",
    "match_info = pd.read_csv(PATH_MATCH_INFO)\n",
    "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
    "\n",
    "test_events_list = []\n",
    "for _, row in test_index.iterrows():\n",
    "    # path가 \"./test/...\" 형식이므로 BASE_PATH와 결합\n",
    "    test_path = os.path.join(BASE_PATH, row[\"path\"].lstrip(\"./\"))\n",
    "    df_ep = pd.read_csv(test_path)\n",
    "    test_events_list.append(df_ep)\n",
    "\n",
    "test_events = pd.concat(test_events_list, ignore_index=True)\n",
    "\n",
    "train[\"is_train\"] = 1\n",
    "test_events[\"is_train\"] = 0\n",
    "\n",
    "events = pd.concat([train, test_events], ignore_index=True)\n",
    "\n",
    "# ----------------------\n",
    "# 2. 기본 정렬 + episode 내 인덱스\n",
    "# ----------------------\n",
    "events = events.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
    "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
    "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
    "\n",
    "# ----------------------\n",
    "# 3. 시간/공간 feature\n",
    "# ----------------------\n",
    "# Δt\n",
    "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
    "events[\"dt\"] = events[\"time_seconds\"] - events[\"prev_time\"]\n",
    "events[\"dt\"] = events[\"dt\"].fillna(0.0)\n",
    "\n",
    "# 이동량/거리\n",
    "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
    "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
    "events[\"dist\"] = np.sqrt(events[\"dx\"]**2 + events[\"dy\"]**2)\n",
    "\n",
    "# 속도 (dt=0 보호)\n",
    "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
    "\n",
    "# zone / lane (필요시 범위 조정)\n",
    "events[\"x_zone\"] = (events[\"start_x\"] / (105/7)).astype(int).clip(0, 6)\n",
    "events[\"lane\"] = pd.cut(\n",
    "    events[\"start_y\"],\n",
    "    bins=[0, 68/3, 2*68/3, 68],\n",
    "    labels=[0, 1, 2],\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 4. 라벨 및 episode-level 메타 (train 전용)\n",
    "# ----------------------\n",
    "train_events = events[events[\"is_train\"] == 1].copy()\n",
    "\n",
    "last_events = (\n",
    "    train_events\n",
    "    .groupby(\"game_episode\", as_index=False)\n",
    "    .tail(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "labels = last_events[[\"game_episode\", \"end_x\", \"end_y\"]].rename(\n",
    "    columns={\"end_x\": \"target_x\", \"end_y\": \"target_y\"}\n",
    ")\n",
    "\n",
    "# episode-level 메타 (마지막 이벤트 기준)\n",
    "ep_meta = last_events[[\"game_episode\", \"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"]].copy()\n",
    "ep_meta = ep_meta.rename(columns={\"team_id\": \"final_team_id\"})\n",
    "\n",
    "# game_clock (분 단위, 0~90+)\n",
    "ep_meta[\"game_clock_min\"] = np.where(\n",
    "    ep_meta[\"period_id\"] == 1,\n",
    "    ep_meta[\"time_seconds\"] / 60.0,\n",
    "    45.0 + ep_meta[\"time_seconds\"] / 60.0\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5. 공격 팀 플래그 (final_team vs 상대)\n",
    "# ----------------------\n",
    "# final_team_id를 전체 events에 붙임\n",
    "events = events.merge(\n",
    "    ep_meta[[\"game_episode\", \"final_team_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 6. 입력용 events에서 마지막 이벤트 타깃 정보 가리기\n",
    "# ----------------------\n",
    "# is_last 플래그\n",
    "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
    "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
    "\n",
    "# labels는 이미 뽑아놨으니, 입력쪽에서만 end_x, end_y, dx, dy, dist, speed 지움\n",
    "mask_last = events[\"is_last\"] == 1\n",
    "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
    "    events.loc[mask_last, col] = np.nan\n",
    "\n",
    "# ----------------------\n",
    "# 7. 카테고리 인코딩 (type_name, result_name, team_id 등)\n",
    "# ----------------------\n",
    "events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
    "events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA_RES__\")\n",
    "\n",
    "le_type = LabelEncoder()\n",
    "le_res = LabelEncoder()\n",
    "\n",
    "events[\"type_id\"] = le_type.fit_transform(events[\"type_name\"])\n",
    "events[\"res_id\"] = le_res.fit_transform(events[\"result_name\"])\n",
    "\n",
    "# team_id는 그대로 써도 되지만, 문자열이면 숫자로 매핑\n",
    "if events[\"team_id\"].dtype == \"object\":\n",
    "    le_team = LabelEncoder()\n",
    "    events[\"team_id_enc\"] = le_team.fit_transform(events[\"team_id\"])\n",
    "else:\n",
    "    events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 8. 마지막 K 이벤트만 사용 (lastK)\n",
    "# ----------------------\n",
    "# rev_idx: 0이 마지막 이벤트\n",
    "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\n",
    "    lambda s: s.max() - s\n",
    ")\n",
    "\n",
    "lastK = events[events[\"rev_idx\"] < K].copy()\n",
    "\n",
    "# pos_in_K: 0~(K-1), 앞쪽 패딩 고려해서 뒤에 실제 이벤트가 모이게\n",
    "def assign_pos_in_K(df):\n",
    "    df = df.sort_values(\"event_idx\")  # 오래된 → 최근\n",
    "    L = len(df)\n",
    "    df = df.copy()\n",
    "    df[\"pos_in_K\"] = np.arange(K - L, K)\n",
    "    return df\n",
    "\n",
    "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
    "\n",
    "# ----------------------\n",
    "# 9. wide feature pivot\n",
    "# ----------------------\n",
    "# 사용할 이벤트 피처 선택\n",
    "num_cols = [\n",
    "    \"start_x\", \"start_y\",\n",
    "    \"end_x\", \"end_y\",\n",
    "    \"dx\", \"dy\", \"dist\", \"speed\",\n",
    "    \"dt\",\n",
    "    \"ep_idx_norm\",\n",
    "    \"x_zone\", \"lane\",\n",
    "    \"is_final_team\",\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"type_id\",\n",
    "    \"res_id\",\n",
    "    \"team_id_enc\",\n",
    "    \"is_home\",\n",
    "    \"period_id\",\n",
    "    \"is_last\",\n",
    "]\n",
    "\n",
    "feature_cols = num_cols + cat_cols\n",
    "\n",
    "wide = lastK[[\"game_episode\", \"pos_in_K\"] + feature_cols].copy()\n",
    "\n",
    "# 숫자형 pivot\n",
    "wide_num = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=num_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 범주형 pivot\n",
    "wide_cat = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=cat_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 컬럼 이름 평탄화\n",
    "wide_num.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_num.columns]\n",
    "wide_cat.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_cat.columns]\n",
    "\n",
    "X = pd.concat([wide_num, wide_cat], axis=1).reset_index()  # game_episode 포함\n",
    "\n",
    "# episode-level 메타 붙이기\n",
    "X = X.merge(ep_meta[[\"game_episode\", \"game_id\", \"game_clock_min\", \"final_team_id\", \"is_home\", \"period_id\"]],\n",
    "            on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# train 라벨 붙이기\n",
    "X = X.merge(labels, on=\"game_episode\", how=\"left\")  # test는 NaN\n",
    "\n",
    "# ----------------------\n",
    "# 10. train/test 분리\n",
    "# ----------------------\n",
    "train_mask = X[\"game_episode\"].isin(labels[\"game_episode\"])\n",
    "X_train = X[train_mask].copy()\n",
    "X_test = X[~train_mask].copy()\n",
    "\n",
    "y_train_x = X_train[\"target_x\"].astype(float)\n",
    "y_train_y = X_train[\"target_y\"].astype(float)\n",
    "\n",
    "# 모델 입력에서 빼야 할 컬럼들\n",
    "drop_cols = [\n",
    "    \"game_episode\",\n",
    "    \"game_id\",\n",
    "    \"target_x\",\n",
    "    \"target_y\",\n",
    "]\n",
    "\n",
    "X_train_feat = X_train.drop(columns=drop_cols)\n",
    "X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns])\n",
    "\n",
    "# NaN 채우기 (LGBM은 NaN 다루긴 하지만, 깔끔하게)\n",
    "X_train_feat = X_train_feat.fillna(0)\n",
    "X_test_feat = X_test_feat.fillna(0)\n",
    "\n",
    "# ----------------------\n",
    "# 11. AutoGluon 학습\n",
    "# ----------------------\n",
    "# 빠른 테스트 모드: CatBoost와 LightGBM만 사용 (10~20분)\n",
    "# 새로운 피처 추가/변경 시 빠르게 테스트하기 위한 설정\n",
    "FAST_TEST_MODE = True  # False로 변경하면 모든 모델 사용 (best_quality)\n",
    "\n",
    "if FAST_TEST_MODE:\n",
    "    # CatBoost와 LightGBM만 선택\n",
    "    hyperparameters = {\n",
    "        'CAT': {},\n",
    "        'GBM': {},  # LightGBM\n",
    "    }\n",
    "    time_limit = 600  # 10분 (10분: 600, 15분: 900, 20분: 1200)\n",
    "    presets = \"good_quality\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"빠른 테스트 모드: CatBoost + LightGBM만 사용 (10분)\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    # 모든 모델 사용 (최종 제출용)\n",
    "    hyperparameters = None\n",
    "    time_limit = 1800  # 30분\n",
    "    presets = \"best_quality\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"전체 모델 학습 모드: 모든 모델 사용 (30분)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# X 좌표 예측 모델\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"X 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_x = X_train_feat.copy()\n",
    "train_data_x[\"target_x\"] = y_train_x\n",
    "\n",
    "predictor_x = TabularPredictor(\n",
    "    label=\"target_x\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=\"ag_models_x\"  # 모델 저장 경로\n",
    ").fit(\n",
    "    train_data=train_data_x,\n",
    "    time_limit=time_limit,\n",
    "    presets=presets,\n",
    "    hyperparameters=hyperparameters,  # 특정 모델만 선택\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Y 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_y = X_train_feat.copy()\n",
    "train_data_y[\"target_y\"] = y_train_y\n",
    "\n",
    "predictor_y = TabularPredictor(\n",
    "    label=\"target_y\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=\"ag_models_y\"  # 모델 저장 경로\n",
    ").fit(\n",
    "    train_data=train_data_y,\n",
    "    time_limit=time_limit,\n",
    "    presets=presets,\n",
    "    hyperparameters=hyperparameters,  # 특정 모델만 선택\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 12. test 예측\n",
    "# ----------------------\n",
    "print(\"=\" * 50)\n",
    "print(\"Test 데이터 예측 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pred_x = predictor_x.predict(X_test_feat)\n",
    "pred_y = predictor_y.predict(X_test_feat)\n",
    "\n",
    "# 필드 범위로 클립\n",
    "pred_x = np.clip(pred_x, 0, 105)\n",
    "pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "# ----------------------\n",
    "# 13. submission 생성\n",
    "# ----------------------\n",
    "sub = sample_sub.copy()\n",
    "\n",
    "# X_test에는 game_episode가 있으니, test_index와 align\n",
    "pred_df = X_test[[\"game_episode\"]].copy()\n",
    "pred_df[\"end_x\"] = pred_x\n",
    "pred_df[\"end_y\"] = pred_y\n",
    "\n",
    "sub = sub.drop(columns=[\"end_x\", \"end_y\"], errors=\"ignore\")\n",
    "sub = sub.merge(pred_df, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "sub.to_csv(\"submission_autogluon_lastK.csv\", index=False)\n",
    "print(\"Saved submission_autogluon_lastK.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11416c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "모델 성능 확인\n",
      "==================================================\n",
      "\n",
      "[X 좌표 모델 - 리더보드]\n",
      "                      model  score_val              eval_metric  \\\n",
      "0       WeightedEnsemble_L2 -11.788316  root_mean_squared_error   \n",
      "1       WeightedEnsemble_L3 -11.788395  root_mean_squared_error   \n",
      "2           CatBoost_BAG_L2 -11.835713  root_mean_squared_error   \n",
      "3           CatBoost_BAG_L1 -11.854799  root_mean_squared_error   \n",
      "4           LightGBM_BAG_L1 -11.874412  root_mean_squared_error   \n",
      "5           LightGBM_BAG_L2 -11.885579  root_mean_squared_error   \n",
      "6  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
      "7  WeightedEnsemble_L2_FULL        NaN  root_mean_squared_error   \n",
      "8      LightGBM_BAG_L2_FULL        NaN  root_mean_squared_error   \n",
      "9      LightGBM_BAG_L1_FULL        NaN  root_mean_squared_error   \n",
      "\n",
      "   pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
      "0       0.302857   76.862985                0.000310           0.009036   \n",
      "1       0.439774  125.364687                0.000295           0.010340   \n",
      "2       0.335542  119.953451                0.032995          43.099502   \n",
      "3       0.050181   67.429266                0.050181          67.429266   \n",
      "4       0.252366    9.424683                0.252366           9.424683   \n",
      "5       0.406484   82.254845                0.103936           5.400896   \n",
      "6            NaN   13.586618                     NaN           0.010340   \n",
      "7            NaN    9.263703                     NaN           0.009036   \n",
      "8            NaN   10.671368                     NaN           1.416701   \n",
      "9            NaN    2.817377                     NaN           2.817377   \n",
      "\n",
      "   stack_level  can_infer  fit_order  \n",
      "0            2      False          3  \n",
      "1            3      False          6  \n",
      "2            2      False          5  \n",
      "3            1      False          2  \n",
      "4            1      False          1  \n",
      "5            2      False          4  \n",
      "6            3       True         12  \n",
      "7            2       True          9  \n",
      "8            2       True         10  \n",
      "9            1       True          7  \n",
      "\n",
      "[Y 좌표 모델 - 리더보드]\n",
      "                      model  score_val              eval_metric  \\\n",
      "0       WeightedEnsemble_L3 -13.054562  root_mean_squared_error   \n",
      "1       WeightedEnsemble_L2 -13.055272  root_mean_squared_error   \n",
      "2           CatBoost_BAG_L2 -13.096042  root_mean_squared_error   \n",
      "3           LightGBM_BAG_L1 -13.108974  root_mean_squared_error   \n",
      "4           CatBoost_BAG_L1 -13.151211  root_mean_squared_error   \n",
      "5           LightGBM_BAG_L2 -13.168191  root_mean_squared_error   \n",
      "6  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
      "7  WeightedEnsemble_L2_FULL        NaN  root_mean_squared_error   \n",
      "8      LightGBM_BAG_L2_FULL        NaN  root_mean_squared_error   \n",
      "9      LightGBM_BAG_L1_FULL        NaN  root_mean_squared_error   \n",
      "\n",
      "   pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
      "0       0.347186  127.317010                0.000364           0.006953   \n",
      "1       0.224222   84.596237                0.000303           0.008019   \n",
      "2       0.248204  121.598788                0.024285          37.010570   \n",
      "3       0.158083    6.214154                0.158083           6.214154   \n",
      "4       0.065835   78.374064                0.065835          78.374064   \n",
      "5       0.322537   90.299487                0.098618           5.711269   \n",
      "6            NaN   12.095992                     NaN           0.006953   \n",
      "7            NaN    8.952632                     NaN           0.008019   \n",
      "8            NaN   10.158551                     NaN           1.213938   \n",
      "9            NaN    1.743996                     NaN           1.743996   \n",
      "\n",
      "   stack_level  can_infer  fit_order  \n",
      "0            3      False          6  \n",
      "1            2      False          3  \n",
      "2            2      False          5  \n",
      "3            1      False          1  \n",
      "4            1      False          2  \n",
      "5            2      False          4  \n",
      "6            3       True         12  \n",
      "7            2       True          9  \n",
      "8            2       True         10  \n",
      "9            1       True          7  \n",
      "\n",
      "==================================================\n",
      "Train 데이터 성능 평가\n",
      "==================================================\n",
      "\n",
      "X 좌표 RMSE (Train): 9.8172\n",
      "Y 좌표 RMSE (Train): 10.9279\n",
      "\n",
      "==================================================\n",
      "제출 파일 확인\n",
      "==================================================\n",
      "\n",
      "제출 파일 행 수: 2414\n",
      "\n",
      "제출 파일 샘플:\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  63.058628  13.852748\n",
      "1     153363_2  32.675780  51.616188\n",
      "2     153363_6  38.105050  61.992630\n",
      "3     153363_7  53.824370  10.032447\n",
      "4     153363_8  80.236030  12.526071\n",
      "5     153363_9  74.733840  65.887070\n",
      "6    153363_10  63.249874  17.194273\n",
      "7    153363_12  72.009790  11.105665\n",
      "8    153363_13  31.736425  62.437840\n",
      "9    153363_15  73.550300  13.247793\n",
      "\n",
      "제출 파일 통계:\n",
      "             end_x        end_y\n",
      "count  2414.000000  2414.000000\n",
      "mean     67.109863    33.791077\n",
      "std      20.614104    20.227241\n",
      "min       8.577427     1.049518\n",
      "25%      52.162208    13.275247\n",
      "50%      71.731615    33.930807\n",
      "75%      84.654348    53.997015\n",
      "max     101.567260    68.000000\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 결과 확인 (학습 완료 후 실행)\n",
    "# ----------------------\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 저장된 모델 로드\n",
    "predictor_x = TabularPredictor.load(\"ag_models_x\")\n",
    "predictor_y = TabularPredictor.load(\"ag_models_y\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"모델 성능 확인\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# X 좌표 모델 리더보드\n",
    "print(\"\\n[X 좌표 모델 - 리더보드]\")\n",
    "leaderboard_x = predictor_x.leaderboard(silent=True)\n",
    "print(leaderboard_x.head(10))\n",
    "\n",
    "# Y 좌표 모델 리더보드\n",
    "print(\"\\n[Y 좌표 모델 - 리더보드]\")\n",
    "leaderboard_y = predictor_y.leaderboard(silent=True)\n",
    "print(leaderboard_y.head(10))\n",
    "\n",
    "# Train 데이터로 성능 평가 (첫 번째 셀 실행 후 사용 가능)\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Train 데이터 성능 평가\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # X 좌표 평가\n",
    "    y_pred_train_x = predictor_x.predict(X_train_feat)\n",
    "    rmse_x = np.sqrt(np.mean((y_train_x - y_pred_train_x) ** 2))\n",
    "    print(f\"\\nX 좌표 RMSE (Train): {rmse_x:.4f}\")\n",
    "    \n",
    "    # Y 좌표 평가\n",
    "    y_pred_train_y = predictor_y.predict(X_train_feat)\n",
    "    rmse_y = np.sqrt(np.mean((y_train_y - y_pred_train_y) ** 2))\n",
    "    print(f\"Y 좌표 RMSE (Train): {rmse_y:.4f}\")\n",
    "except NameError:\n",
    "    print(\"\\n(첫 번째 셀을 먼저 실행해야 Train 데이터 평가가 가능합니다)\")\n",
    "\n",
    "# 제출 파일 확인\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"제출 파일 확인\")\n",
    "print(\"=\" * 50)\n",
    "sub = pd.read_csv(\"submission_autogluon_lastK.csv\")\n",
    "print(f\"\\n제출 파일 행 수: {len(sub)}\")\n",
    "print(f\"\\n제출 파일 샘플:\")\n",
    "print(sub.head(10))\n",
    "print(f\"\\n제출 파일 통계:\")\n",
    "print(sub.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
