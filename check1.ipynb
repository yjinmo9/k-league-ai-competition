{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82f6a2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Experiment setup\n",
      " - BIN_Y_LIST: [5.0, 6.0]\n",
      " - TOPK_LIST : [3]\n",
      " - Total combinations: 2\n",
      "‚úÖ Path OK\n",
      " - open_track1/train.csv\n",
      " - open_track1/test.csv\n",
      " - open_track1/sample_submission.csv\n",
      "‚úÖ Loaded index files\n",
      " - train shape: (356721, 15)\n",
      " - test_index shape: (2414, 3)\n",
      " - sample_sub shape: (2414, 3)\n",
      "‚úÖ Loaded events total=409,831 | train=356,721 | test=53,110\n",
      "‚úÖ Basic preprocessing done\n",
      "‚úÖ Spatial features done\n",
      "‚úÖ Episode meta built\n",
      "‚úÖ Leakage blocked (last event end_x/end_y masked)\n",
      "‚úÖ Categorical encoding done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/4259314962.py:307: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/4259314962.py:340: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_base = X_base.fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/4259314962.py:412: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/4259314962.py:413: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.14 GB / 16.00 GB (19.6%)\n",
      "Disk Space Avail:   100.09 GB / 460.43 GB (21.7%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wide features built\n",
      " - X_base shape: (17849, 540)\n",
      " - Train episodes: 15435 | Test episodes: 2414\n",
      "\n",
      "==========================================================================================\n",
      "üöÄ TRAIN BIN_Y=5.0 (NX=21, NY=14) | time_limit=600s/model\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_xbin_BY5p0\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_x_bin\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3291.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (2.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 13891, Val Rows: 1544\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 599.41s of the 599.40s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.9/3.1 GB\n",
      "\t-2.1559\t = Validation score   (-log_loss)\n",
      "\t79.59s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 519.73s of the 519.73s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.3/3.3 GB\n",
      "\t-2.1198\t = Validation score   (-log_loss)\n",
      "\t302.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 217.60s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.4 GB\n",
      "\tEnsemble Weights: {'CatBoost': 0.636, 'LightGBM': 0.364}\n",
      "\t-2.1045\t = Validation score   (-log_loss)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 382.53s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 33902.6 rows/s (1544 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_xbin_BY5p0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.46 GB / 16.00 GB (21.6%)\n",
      "Disk Space Avail:   98.96 GB / 460.43 GB (21.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_ybin_BY5p0\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_y_bin\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 14\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3599.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (2.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 13891, Val Rows: 1544\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 599.40s of the 599.40s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.7/3.4 GB\n",
      "\t-1.8086\t = Validation score   (-log_loss)\n",
      "\t54.34s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 544.98s of the 544.98s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.2/3.6 GB\n",
      "\t-1.7928\t = Validation score   (-log_loss)\n",
      "\t317.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 227.14s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.2 GB\n",
      "\tEnsemble Weights: {'CatBoost': 0.583, 'LightGBM': 0.417}\n",
      "\t-1.7776\t = Validation score   (-log_loss)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 372.99s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 38234.5 rows/s (1544 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_ybin_BY5p0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[X-bin leaderboard head]\n",
      "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0  WeightedEnsemble_L2  -2.104453    log_loss       0.045542  381.736680   \n",
      "1             CatBoost  -2.119763    log_loss       0.012725  302.091879   \n",
      "2             LightGBM  -2.155898    log_loss       0.031681   79.590763   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.001136           0.054038            2       True   \n",
      "1                0.012725         302.091879            1       True   \n",
      "2                0.031681          79.590763            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          3  \n",
      "1          2  \n",
      "2          1  \n",
      "\n",
      "[Y-bin leaderboard head]\n",
      "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0  WeightedEnsemble_L2  -1.777630    log_loss       0.040382  372.208516   \n",
      "1             CatBoost  -1.792829    log_loss       0.011853  317.810023   \n",
      "2             LightGBM  -1.808585    log_loss       0.026599   54.341741   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.001930           0.056752            2       True   \n",
      "1                0.011853         317.810023            1       True   \n",
      "2                0.026599          54.341741            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          3  \n",
      "1          2  \n",
      "2          1  \n",
      "\n",
      "‚úÖ Proxy score = score_x + score_y = -2.104453 + -1.777630 = -3.882083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/4259314962.py:412: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/4259314962.py:413: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.43 GB / 16.00 GB (21.5%)\n",
      "Disk Space Avail:   98.89 GB / 460.43 GB (21.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_xbin_BY6p0\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_x_bin\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3512.70 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (2.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "üöÄ TRAIN BIN_Y=6.0 (NX=21, NY=12) | time_limit=600s/model\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 13891, Val Rows: 1544\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 599.41s of the 599.41s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.9/3.4 GB\n",
      "\t-2.1559\t = Validation score   (-log_loss)\n",
      "\t75.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 523.87s of the 523.87s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.3/3.4 GB\n",
      "\t-2.1198\t = Validation score   (-log_loss)\n",
      "\t298.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 225.52s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.5 GB\n",
      "\tEnsemble Weights: {'CatBoost': 0.636, 'LightGBM': 0.364}\n",
      "\t-2.1045\t = Validation score   (-log_loss)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 374.6s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 41491.6 rows/s (1544 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_xbin_BY6p0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.57 GB / 16.00 GB (22.3%)\n",
      "Disk Space Avail:   98.81 GB / 460.43 GB (21.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_ybin_BY6p0\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_y_bin\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 12\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3761.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (1.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 13891, Val Rows: 1544\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 599.37s of the 599.37s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.6/3.5 GB\n",
      "\t-1.6852\t = Validation score   (-log_loss)\n",
      "\t58.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 540.88s of the 540.87s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.2/3.3 GB\n",
      "\t-1.6676\t = Validation score   (-log_loss)\n",
      "\t196.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 344.22s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.3 GB\n",
      "\tEnsemble Weights: {'CatBoost': 0.632, 'LightGBM': 0.368}\n",
      "\t-1.6584\t = Validation score   (-log_loss)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 255.89s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 46535.1 rows/s (1544 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_ybin_BY6p0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[X-bin leaderboard head]\n",
      "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0  WeightedEnsemble_L2  -2.104453    log_loss       0.037212  373.839991   \n",
      "1             CatBoost  -2.119763    log_loss       0.011246  298.322105   \n",
      "2             LightGBM  -2.155898    log_loss       0.024983   75.468402   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.000984           0.049483            2       True   \n",
      "1                0.011246         298.322105            1       True   \n",
      "2                0.024983          75.468402            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          3  \n",
      "1          2  \n",
      "2          1  \n",
      "\n",
      "[Y-bin leaderboard head]\n",
      "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0  WeightedEnsemble_L2  -1.658445    log_loss       0.033179  255.109275   \n",
      "1             CatBoost  -1.667581    log_loss       0.008859  196.630645   \n",
      "2             LightGBM  -1.685180    log_loss       0.023209   58.436837   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.001111           0.041793            2       True   \n",
      "1                0.008859         196.630645            1       True   \n",
      "2                0.023209          58.436837            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          3  \n",
      "1          2  \n",
      "2          1  \n",
      "\n",
      "‚úÖ Proxy score = score_x + score_y = -2.104453 + -1.658445 = -3.762898\n",
      "\n",
      "====================================================================================================\n",
      "üèÜ BEST CONFIG (Flagged Grid)\n",
      "====================================================================================================\n",
      "{\n",
      "  \"BIN_X\": 5.0,\n",
      "  \"BIN_Y\": 6.0,\n",
      "  \"NX\": 21,\n",
      "  \"NY\": 12,\n",
      "  \"TOPK\": 3,\n",
      "  \"score_x\": -2.1044533402792585,\n",
      "  \"score_y\": -1.6584450840092235,\n",
      "  \"proxy_score\": -3.762898424288482,\n",
      "  \"pred_end_x_mean\": 65.8003276611793,\n",
      "  \"pred_end_y_mean\": 34.065300536133634,\n",
      "  \"model_path_x\": \"ag_models_xbin_BY6p0\",\n",
      "  \"model_path_y\": \"ag_models_ybin_BY6p0\",\n",
      "  \"submission_name\": null\n",
      "}\n",
      "\n",
      "==========================================================================================\n",
      "üîÅ Re-train BEST for final submission: BIN_Y=6.0, TOPK=3\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/4259314962.py:563: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_49695/4259314962.py:564: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.51 GB / 16.00 GB (21.9%)\n",
      "Disk Space Avail:   98.72 GB / 460.43 GB (21.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_xbin_BEST_BY6p0\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_x_bin\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3652.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (1.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 13891, Val Rows: 1544\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 599.43s of the 599.43s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.9/3.4 GB\n",
      "\t-2.1559\t = Validation score   (-log_loss)\n",
      "\t77.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 522.34s of the 522.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.3/3.4 GB\n",
      "\t-2.1198\t = Validation score   (-log_loss)\n",
      "\t299.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 222.60s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.2 GB\n",
      "\tEnsemble Weights: {'CatBoost': 0.636, 'LightGBM': 0.364}\n",
      "\t-2.1045\t = Validation score   (-log_loss)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 377.52s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 37450.4 rows/s (1544 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_xbin_BEST_BY6p0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.27 GB / 16.00 GB (20.4%)\n",
      "Disk Space Avail:   98.66 GB / 460.43 GB (21.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_ybin_BEST_BY6p0\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_y_bin\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 12\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3431.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (2.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 13891, Val Rows: 1544\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 599.38s of the 599.38s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.6/3.3 GB\n",
      "\t-1.6852\t = Validation score   (-log_loss)\n",
      "\t53.8s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 545.54s of the 545.54s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.2/3.6 GB\n",
      "\t-1.6676\t = Validation score   (-log_loss)\n",
      "\t195.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 350.12s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.4 GB\n",
      "\tEnsemble Weights: {'CatBoost': 0.632, 'LightGBM': 0.368}\n",
      "\t-1.6584\t = Validation score   (-log_loss)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 249.99s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 48477.8 rows/s (1544 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_ybin_BEST_BY6p0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FINAL submission saved: submission_FINAL_flagged.csv\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  66.803431   5.939561\n",
      "1     153363_2  37.273295  62.813239\n",
      "2     153363_6  35.577678  64.887259\n",
      "3     153363_7  52.682555   5.903442\n",
      "4     153363_8  82.271261   8.201394\n",
      "5     153363_9  76.906888  68.000000\n",
      "6    153363_10  57.111227   6.912072\n",
      "7    153363_12  66.878253   4.290965\n",
      "8    153363_13  32.663202  65.978741\n",
      "9    153363_15  59.366657   5.604907\n",
      "‚úÖ FINAL columns: ['game_episode', 'end_x', 'end_y']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# K-League Track1 - BIN Classification + SoftAvg (Flag-Controlled Grid)\n",
    "# ------------------------------------------------------------\n",
    "# ‚úÖ You can toggle BIN_Y search and TOPK search independently:\n",
    "#   - USE_BIN_Y_SEARCH: True -> compare multiple BIN_Y, False -> fixed BIN_Y\n",
    "#   - USE_TOPK_SEARCH : True -> compare multiple TOPK,  False -> fixed TOPK\n",
    "#\n",
    "# ‚úÖ Models: AutoGluon Tabular (LightGBM + CatBoost + Ensemble)\n",
    "# ‚úÖ Output: final submission csv in required format:\n",
    "#           game_episode,end_x,end_y\n",
    "#\n",
    "# ‚úÖ Selection criterion (proxy):\n",
    "#   - AutoGluon leaderboard score_val for log_loss is typically NEGATIVE log_loss\n",
    "#   - Higher (closer to 0) is better\n",
    "#   - proxy_score = best_score_x + best_score_y (maximize)\n",
    "#\n",
    "# ‚ö†Ô∏è You must set BASE_PATH + PATH_COL correctly.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 0) USER SETTINGS\n",
    "# ======================\n",
    "BASE_PATH = \"open_track1/\"   # ‚úÖ change to your dataset folder\n",
    "PATH_COL = \"path\"            # ‚úÖ test.csv column name containing episode file path\n",
    "\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_INDEX_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\"\n",
    "MATCH_INFO_FILE = \"match_info.csv\"  # optional\n",
    "\n",
    "PATH_TRAIN = os.path.join(BASE_PATH, TRAIN_FILE)\n",
    "PATH_TEST_INDEX = os.path.join(BASE_PATH, TEST_INDEX_FILE)\n",
    "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, SAMPLE_SUB_FILE)\n",
    "PATH_MATCH_INFO = os.path.join(BASE_PATH, MATCH_INFO_FILE)\n",
    "\n",
    "# ======================\n",
    "# EXPERIMENT CONTROL FLAGS (‚≠ê ÌïµÏã¨)\n",
    "# ======================\n",
    "# --- BIN_Y search ---\n",
    "USE_BIN_Y_SEARCH = True\n",
    "BIN_Y_CANDIDATES = [5.0, 6.0]\n",
    "FIXED_BIN_Y = 5.0\n",
    "\n",
    "# --- TOP-K soft average search ---\n",
    "USE_TOPK_SEARCH = False\n",
    "TOPK_CANDIDATES = [1, 3, 5]\n",
    "FIXED_TOPK = 3\n",
    "\n",
    "# --- BIN_X fixed (you can extend similarly if you want) ---\n",
    "BIN_X = 5.0\n",
    "\n",
    "# --- last K events ---\n",
    "K = 20\n",
    "\n",
    "# --- AutoGluon training budget (per model) ---\n",
    "TIME_LIMIT = 600                # seconds (10min)\n",
    "PRESETS = \"medium_quality\"\n",
    "HYPERPARAMETERS = {\"GBM\": {}, \"CAT\": {}}  # LightGBM + CatBoost\n",
    "\n",
    "# --- Logging / saving ---\n",
    "PRINT_LEADERBOARD_TOPN = 10\n",
    "SAVE_ALL_SUBMISSIONS = False    # True -> save every config submission\n",
    "\n",
    "# --- Final output name ---\n",
    "FINAL_SUBMISSION_NAME = \"submission_FINAL_flagged.csv\"\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1) Resolve experiment grid from flags\n",
    "# ======================\n",
    "if USE_BIN_Y_SEARCH:\n",
    "    BIN_Y_LIST = BIN_Y_CANDIDATES\n",
    "else:\n",
    "    BIN_Y_LIST = [FIXED_BIN_Y]\n",
    "\n",
    "if USE_TOPK_SEARCH:\n",
    "    TOPK_LIST = TOPK_CANDIDATES\n",
    "else:\n",
    "    TOPK_LIST = [FIXED_TOPK]\n",
    "\n",
    "print(\"üîß Experiment setup\")\n",
    "print(\" - BIN_Y_LIST:\", BIN_Y_LIST)\n",
    "print(\" - TOPK_LIST :\", TOPK_LIST)\n",
    "print(\" - Total combinations:\", len(BIN_Y_LIST) * len(TOPK_LIST))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2) Path checks\n",
    "# ======================\n",
    "for p in [PATH_TRAIN, PATH_TEST_INDEX, PATH_SAMPLE_SUB]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {p}\")\n",
    "\n",
    "print(\"‚úÖ Path OK\")\n",
    "print(\" -\", PATH_TRAIN)\n",
    "print(\" -\", PATH_TEST_INDEX)\n",
    "print(\" -\", PATH_SAMPLE_SUB)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3) Load base CSVs\n",
    "# ======================\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test_index = pd.read_csv(PATH_TEST_INDEX)\n",
    "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
    "\n",
    "match_info = None\n",
    "if os.path.exists(PATH_MATCH_INFO):\n",
    "    match_info = pd.read_csv(PATH_MATCH_INFO)\n",
    "\n",
    "if PATH_COL not in test_index.columns:\n",
    "    raise KeyError(f\"‚ùå test.csv missing '{PATH_COL}'. Available: {test_index.columns.tolist()}\")\n",
    "\n",
    "print(\"‚úÖ Loaded index files\")\n",
    "print(\" - train shape:\", train.shape)\n",
    "print(\" - test_index shape:\", test_index.shape)\n",
    "print(\" - sample_sub shape:\", sample_sub.shape)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 4) Load all test episode event CSVs\n",
    "# ======================\n",
    "test_events_list = []\n",
    "for _, row in test_index.iterrows():\n",
    "    rel = str(row[PATH_COL])\n",
    "    ep_path = os.path.join(BASE_PATH, rel.lstrip(\"./\"))\n",
    "    if not os.path.exists(ep_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Missing test episode file: {ep_path}\")\n",
    "    df_ep = pd.read_csv(ep_path)\n",
    "    test_events_list.append(df_ep)\n",
    "\n",
    "test_events = pd.concat(test_events_list, ignore_index=True)\n",
    "\n",
    "# concat train + test\n",
    "train = train.copy()\n",
    "test_events = test_events.copy()\n",
    "train[\"is_train\"] = 1\n",
    "test_events[\"is_train\"] = 0\n",
    "events = pd.concat([train, test_events], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Loaded events total={len(events):,} | train={len(train):,} | test={len(test_events):,}\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 5) Basic required columns & sorting\n",
    "# ======================\n",
    "REQ_COLS = [\"game_episode\", \"time_seconds\", \"action_id\", \"start_x\", \"start_y\", \"end_x\", \"end_y\"]\n",
    "for c in REQ_COLS:\n",
    "    if c not in events.columns:\n",
    "        raise KeyError(f\"‚ùå Required column missing: {c}\")\n",
    "\n",
    "events = events.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
    "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
    "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
    "\n",
    "# time delta\n",
    "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
    "events[\"dt\"] = (events[\"time_seconds\"] - events[\"prev_time\"]).fillna(0)\n",
    "\n",
    "# movement\n",
    "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
    "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
    "events[\"dist\"] = np.sqrt(events[\"dx\"] ** 2 + events[\"dy\"] ** 2)\n",
    "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
    "\n",
    "# zones / lanes\n",
    "events[\"x_zone\"] = (events[\"start_x\"] / (105 / 7)).astype(int).clip(0, 6)\n",
    "events[\"lane\"] = pd.cut(\n",
    "    events[\"start_y\"],\n",
    "    bins=[0, 68 / 3, 2 * 68 / 3, 68],\n",
    "    labels=[0, 1, 2],\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "print(\"‚úÖ Basic preprocessing done\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 6) Spatial engineered features\n",
    "# ======================\n",
    "events[\"angle_to_goal\"] = np.arctan2(34 - events[\"start_y\"], 105 - events[\"start_x\"]) * 180 / np.pi\n",
    "\n",
    "events[\"dist_corner_top\"] = np.sqrt((105 - events[\"start_x\"]) ** 2 + (0 - events[\"start_y\"]) ** 2)\n",
    "events[\"dist_corner_bottom\"] = np.sqrt((105 - events[\"start_x\"]) ** 2 + (68 - events[\"start_y\"]) ** 2)\n",
    "events[\"dist_to_nearest_corner\"] = events[[\"dist_corner_top\", \"dist_corner_bottom\"]].min(axis=1)\n",
    "\n",
    "events[\"is_corner_area\"] = (\n",
    "    (events[\"start_x\"] > 100) &\n",
    "    ((events[\"start_y\"] < 5) | (events[\"start_y\"] > 63))\n",
    ").astype(int)\n",
    "\n",
    "events[\"angle_goal_x_corner\"] = events[\"angle_to_goal\"] * events[\"is_corner_area\"]\n",
    "events[\"dist_corner_x_angle\"] = events[\"dist_to_nearest_corner\"] * events[\"angle_to_goal\"]\n",
    "\n",
    "events[\"dist_to_sideline\"] = events[\"start_y\"].apply(lambda y: min(y, 68 - y))\n",
    "events[\"angle_to_goal_center\"] = np.arctan2(34 - events[\"start_y\"], 105 - events[\"start_x\"])  # radians\n",
    "events[\"time_pos_inter\"] = events[\"ep_idx_norm\"] * events[\"start_x\"]\n",
    "\n",
    "print(\"‚úÖ Spatial features done\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 7) Episode meta from TRAIN last event\n",
    "# ======================\n",
    "train_events_only = events[events[\"is_train\"] == 1].copy()\n",
    "last_events_train = train_events_only.groupby(\"game_episode\", as_index=False).tail(1).copy()\n",
    "\n",
    "meta_cols = [\"game_episode\"]\n",
    "for cand in [\"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"]:\n",
    "    if cand in last_events_train.columns:\n",
    "        meta_cols.append(cand)\n",
    "\n",
    "ep_meta = last_events_train[meta_cols].copy()\n",
    "\n",
    "# derive game_clock_min if possible\n",
    "if (\"period_id\" in ep_meta.columns) and (\"time_seconds\" in ep_meta.columns):\n",
    "    ep_meta[\"game_clock_min\"] = np.where(\n",
    "        ep_meta[\"period_id\"] == 1,\n",
    "        ep_meta[\"time_seconds\"] / 60,\n",
    "        45 + ep_meta[\"time_seconds\"] / 60\n",
    "    )\n",
    "else:\n",
    "    ep_meta[\"game_clock_min\"] = np.nan\n",
    "\n",
    "# mark final_team_id if possible\n",
    "if \"team_id\" in ep_meta.columns:\n",
    "    ep_meta = ep_meta.rename(columns={\"team_id\": \"final_team_id\"})\n",
    "    events = events.merge(ep_meta[[\"game_episode\", \"final_team_id\"]], on=\"game_episode\", how=\"left\")\n",
    "    if \"team_id\" in events.columns:\n",
    "        events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
    "    else:\n",
    "        events[\"is_final_team\"] = 0\n",
    "else:\n",
    "    events[\"is_final_team\"] = 0\n",
    "\n",
    "print(\"‚úÖ Episode meta built\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 8) Leakage block (mask last event end_x/end_y and derivatives)\n",
    "# ======================\n",
    "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
    "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
    "\n",
    "mask_last = events[\"is_last\"] == 1\n",
    "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
    "    if col in events.columns:\n",
    "        events.loc[mask_last, col] = np.nan\n",
    "\n",
    "print(\"‚úÖ Leakage blocked (last event end_x/end_y masked)\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 9) Categorical encoding\n",
    "# ======================\n",
    "if \"type_name\" in events.columns:\n",
    "    events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA__\")\n",
    "else:\n",
    "    events[\"type_name\"] = \"__NA__\"\n",
    "\n",
    "if \"result_name\" in events.columns:\n",
    "    events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA__\")\n",
    "else:\n",
    "    events[\"result_name\"] = \"__NA__\"\n",
    "\n",
    "events[\"type_id\"] = LabelEncoder().fit_transform(events[\"type_name\"])\n",
    "events[\"res_id\"] = LabelEncoder().fit_transform(events[\"result_name\"])\n",
    "\n",
    "if \"team_id\" in events.columns:\n",
    "    if events[\"team_id\"].dtype == \"object\":\n",
    "        events[\"team_id_enc\"] = LabelEncoder().fit_transform(events[\"team_id\"].astype(str))\n",
    "    else:\n",
    "        events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
    "else:\n",
    "    events[\"team_id_enc\"] = 0\n",
    "\n",
    "if \"is_home\" not in events.columns:\n",
    "    events[\"is_home\"] = 0\n",
    "if \"period_id\" not in events.columns:\n",
    "    events[\"period_id\"] = 1\n",
    "\n",
    "print(\"‚úÖ Categorical encoding done\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 10) Build lastK sequence and pivot to wide table\n",
    "# ======================\n",
    "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(lambda s: s.max() - s)\n",
    "lastK = events[events[\"rev_idx\"] < K].copy()\n",
    "\n",
    "def assign_pos(df):\n",
    "    df = df.sort_values(\"event_idx\")\n",
    "    L = len(df)\n",
    "    df[\"pos_in_K\"] = np.arange(K - L, K)  # left-pad\n",
    "    return df\n",
    "\n",
    "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos)\n",
    "\n",
    "num_cols = [\n",
    "    \"start_x\", \"start_y\", \"end_x\", \"end_y\",\n",
    "    \"dx\", \"dy\", \"dist\", \"speed\", \"dt\",\n",
    "    \"ep_idx_norm\", \"x_zone\", \"lane\",\n",
    "    \"is_final_team\",\n",
    "    \"angle_to_goal\", \"dist_to_nearest_corner\", \"is_corner_area\",\n",
    "    \"angle_goal_x_corner\", \"dist_corner_x_angle\",\n",
    "    \"dist_to_sideline\", \"angle_to_goal_center\", \"time_pos_inter\"\n",
    "]\n",
    "cat_cols = [\n",
    "    \"type_id\", \"res_id\", \"team_id_enc\",\n",
    "    \"is_home\", \"period_id\", \"is_last\"\n",
    "]\n",
    "\n",
    "wide = lastK[[\"game_episode\", \"pos_in_K\"] + num_cols + cat_cols].copy()\n",
    "\n",
    "wide_num = wide.pivot_table(index=\"game_episode\", columns=\"pos_in_K\", values=num_cols, aggfunc=\"first\")\n",
    "wide_cat = wide.pivot_table(index=\"game_episode\", columns=\"pos_in_K\", values=cat_cols, aggfunc=\"first\")\n",
    "\n",
    "wide_num.columns = [f\"{c}_{int(p)}\" for c, p in wide_num.columns]\n",
    "wide_cat.columns = [f\"{c}_{int(p)}\" for c, p in wide_cat.columns]\n",
    "\n",
    "X_base = pd.concat([wide_num, wide_cat], axis=1).reset_index()\n",
    "\n",
    "# attach ep_meta\n",
    "meta_use = [\"game_episode\", \"game_clock_min\"]\n",
    "for c in [\"game_id\", \"final_team_id\", \"is_home\", \"period_id\"]:\n",
    "    if c in ep_meta.columns:\n",
    "        meta_use.append(c)\n",
    "\n",
    "X_base = X_base.merge(ep_meta[meta_use], on=\"game_episode\", how=\"left\")\n",
    "X_base = X_base.fillna(0)\n",
    "\n",
    "# train mask by episodes that exist in train labels (train last events)\n",
    "train_eps = last_events_train[\"game_episode\"].unique()\n",
    "train_mask = X_base[\"game_episode\"].isin(train_eps)\n",
    "\n",
    "print(\"‚úÖ Wide features built\")\n",
    "print(\" - X_base shape:\", X_base.shape)\n",
    "print(\" - Train episodes:\", train_mask.sum(), \"| Test episodes:\", (~train_mask).sum())\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 11) Utility functions\n",
    "# ======================\n",
    "def proba_to_coord_topk_softavg(proba: np.ndarray, bin_size: float, top_k: int) -> float:\n",
    "    \"\"\"\n",
    "    Convert class probability vector into a continuous coordinate:\n",
    "      - top_k=1: argmax bin center\n",
    "      - top_k>1: weighted average of top-k bin centers by probability\n",
    "    \"\"\"\n",
    "    if top_k <= 1:\n",
    "        idx = int(np.argmax(proba))\n",
    "        return idx * bin_size + bin_size / 2\n",
    "\n",
    "    top_idx = np.argsort(proba)[-top_k:]\n",
    "    weights = proba[top_idx]\n",
    "    wsum = weights.sum()\n",
    "    if wsum <= 0:\n",
    "        idx = int(np.argmax(proba))\n",
    "        return idx * bin_size + bin_size / 2\n",
    "\n",
    "    weights = weights / wsum\n",
    "    coords = top_idx * bin_size + bin_size / 2\n",
    "    return float(np.sum(weights * coords))\n",
    "\n",
    "def best_score_val(predictor: TabularPredictor) -> float:\n",
    "    \"\"\"\n",
    "    Return top row score_val from leaderboard.\n",
    "    For log_loss, AutoGluon often reports negative log_loss, so higher is better.\n",
    "    \"\"\"\n",
    "    lb = predictor.leaderboard(silent=True)\n",
    "    return float(lb.iloc[0][\"score_val\"])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 12) Main experiment loop (flag-controlled)\n",
    "# ======================\n",
    "results = []\n",
    "best_rec = None\n",
    "\n",
    "for BIN_Y in BIN_Y_LIST:\n",
    "    NX = int(np.ceil(105 / BIN_X))\n",
    "    NY = int(np.ceil(68 / BIN_Y))\n",
    "\n",
    "    # labels for this BIN_Y\n",
    "    last_local = last_events_train.copy()\n",
    "    last_local[\"target_x_bin\"] = (last_local[\"end_x\"] / BIN_X).astype(int).clip(0, NX - 1)\n",
    "    last_local[\"target_y_bin\"] = (last_local[\"end_y\"] / BIN_Y).astype(int).clip(0, NY - 1)\n",
    "    labels_local = last_local[[\"game_episode\", \"target_x_bin\", \"target_y_bin\"]].copy()\n",
    "\n",
    "    X_all = X_base.merge(labels_local, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "    X_train = X_all[train_mask].copy()\n",
    "    X_test = X_all[~train_mask].copy()\n",
    "\n",
    "    y_train_x = X_train[\"target_x_bin\"].astype(int)\n",
    "    y_train_y = X_train[\"target_y_bin\"].astype(int)\n",
    "\n",
    "    drop_cols = [\"game_episode\", \"target_x_bin\", \"target_y_bin\"]\n",
    "    if \"game_id\" in X_train.columns:\n",
    "        drop_cols.append(\"game_id\")\n",
    "\n",
    "    X_train_feat = X_train.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
    "    X_test_feat = X_test.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
    "\n",
    "    # train frames\n",
    "    train_x = X_train_feat.copy()\n",
    "    train_x[\"target_x_bin\"] = y_train_x\n",
    "\n",
    "    train_y = X_train_feat.copy()\n",
    "    train_y[\"target_y_bin\"] = y_train_y\n",
    "\n",
    "    # unique model path per BIN_Y config\n",
    "    by_tag = str(BIN_Y).replace(\".\", \"p\")\n",
    "    model_path_x = f\"ag_models_xbin_BY{by_tag}\"\n",
    "    model_path_y = f\"ag_models_ybin_BY{by_tag}\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"üöÄ TRAIN BIN_Y={BIN_Y} (NX={NX}, NY={NY}) | time_limit={TIME_LIMIT}s/model\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    predictor_x = TabularPredictor(\n",
    "        label=\"target_x_bin\",\n",
    "        problem_type=\"multiclass\",\n",
    "        eval_metric=\"log_loss\",\n",
    "        path=model_path_x\n",
    "    ).fit(\n",
    "        train_data=train_x,\n",
    "        time_limit=TIME_LIMIT,\n",
    "        presets=PRESETS,\n",
    "        hyperparameters=HYPERPARAMETERS,\n",
    "        verbosity=2\n",
    "    )\n",
    "\n",
    "    predictor_y = TabularPredictor(\n",
    "        label=\"target_y_bin\",\n",
    "        problem_type=\"multiclass\",\n",
    "        eval_metric=\"log_loss\",\n",
    "        path=model_path_y\n",
    "    ).fit(\n",
    "        train_data=train_y,\n",
    "        time_limit=TIME_LIMIT,\n",
    "        presets=PRESETS,\n",
    "        hyperparameters=HYPERPARAMETERS,\n",
    "        verbosity=2\n",
    "    )\n",
    "\n",
    "    # leaderboard\n",
    "    lbx = predictor_x.leaderboard(silent=True)\n",
    "    lby = predictor_y.leaderboard(silent=True)\n",
    "\n",
    "    print(\"\\n[X-bin leaderboard head]\")\n",
    "    print(lbx.head(PRINT_LEADERBOARD_TOPN))\n",
    "    print(\"\\n[Y-bin leaderboard head]\")\n",
    "    print(lby.head(PRINT_LEADERBOARD_TOPN))\n",
    "\n",
    "    score_x = best_score_val(predictor_x)\n",
    "    score_y = best_score_val(predictor_y)\n",
    "    proxy_score = score_x + score_y  # maximize (higher is better)\n",
    "\n",
    "    print(f\"\\n‚úÖ Proxy score = score_x + score_y = {score_x:.6f} + {score_y:.6f} = {proxy_score:.6f}\")\n",
    "\n",
    "    # predict proba once per BIN_Y\n",
    "    proba_x = predictor_x.predict_proba(X_test_feat).values\n",
    "    proba_y = predictor_y.predict_proba(X_test_feat).values\n",
    "\n",
    "    # TOPK loop (flag-controlled list)\n",
    "    for TOPK in TOPK_LIST:\n",
    "        pred_x = np.array([proba_to_coord_topk_softavg(p, BIN_X, TOPK) for p in proba_x])\n",
    "        pred_y = np.array([proba_to_coord_topk_softavg(p, BIN_Y, TOPK) for p in proba_y])\n",
    "\n",
    "        pred_x = np.clip(pred_x, 0, 105)\n",
    "        pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "        # build submission in correct format without merge suffix\n",
    "        pred_df = X_test[[\"game_episode\"]].copy()\n",
    "        pred_df[\"end_x\"] = pred_x\n",
    "        pred_df[\"end_y\"] = pred_y\n",
    "\n",
    "        sub = sample_sub.copy().set_index(\"game_episode\")\n",
    "        pred_df = pred_df.set_index(\"game_episode\")\n",
    "\n",
    "        sub[\"end_x\"] = pred_df[\"end_x\"]\n",
    "        sub[\"end_y\"] = pred_df[\"end_y\"]\n",
    "        sub = sub.reset_index()[[\"game_episode\", \"end_x\", \"end_y\"]]\n",
    "\n",
    "        out_name = f\"submission_BY{by_tag}_TOPK{TOPK}.csv\"\n",
    "        if SAVE_ALL_SUBMISSIONS:\n",
    "            sub.to_csv(out_name, index=False)\n",
    "\n",
    "        rec = {\n",
    "            \"BIN_X\": BIN_X,\n",
    "            \"BIN_Y\": BIN_Y,\n",
    "            \"NX\": NX,\n",
    "            \"NY\": NY,\n",
    "            \"TOPK\": TOPK,\n",
    "            \"score_x\": score_x,\n",
    "            \"score_y\": score_y,\n",
    "            \"proxy_score\": proxy_score,\n",
    "            \"pred_end_x_mean\": float(np.mean(pred_x)),\n",
    "            \"pred_end_y_mean\": float(np.mean(pred_y)),\n",
    "            \"model_path_x\": model_path_x,\n",
    "            \"model_path_y\": model_path_y,\n",
    "            \"submission_name\": out_name if SAVE_ALL_SUBMISSIONS else None,\n",
    "        }\n",
    "        results.append(rec)\n",
    "\n",
    "        # choose best: maximize proxy_score; tie -> prefer TOPK=3 if present\n",
    "        if best_rec is None:\n",
    "            best_rec = rec\n",
    "        else:\n",
    "            if rec[\"proxy_score\"] > best_rec[\"proxy_score\"] + 1e-12:\n",
    "                best_rec = rec\n",
    "            elif abs(rec[\"proxy_score\"] - best_rec[\"proxy_score\"]) <= 1e-12:\n",
    "                if (best_rec[\"TOPK\"] != 3) and (rec[\"TOPK\"] == 3):\n",
    "                    best_rec = rec\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üèÜ BEST CONFIG (Flagged Grid)\")\n",
    "print(\"=\" * 100)\n",
    "print(json.dumps(best_rec, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 13) Train best config again & save FINAL submission\n",
    "# ======================\n",
    "BEST_BIN_Y = best_rec[\"BIN_Y\"]\n",
    "BEST_TOPK = best_rec[\"TOPK\"]\n",
    "\n",
    "NX = int(np.ceil(105 / BIN_X))\n",
    "NY = int(np.ceil(68 / BEST_BIN_Y))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"üîÅ Re-train BEST for final submission: BIN_Y={BEST_BIN_Y}, TOPK={BEST_TOPK}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# rebuild labels for best BIN_Y\n",
    "last_local = last_events_train.copy()\n",
    "last_local[\"target_x_bin\"] = (last_local[\"end_x\"] / BIN_X).astype(int).clip(0, NX - 1)\n",
    "last_local[\"target_y_bin\"] = (last_local[\"end_y\"] / BEST_BIN_Y).astype(int).clip(0, NY - 1)\n",
    "labels_local = last_local[[\"game_episode\", \"target_x_bin\", \"target_y_bin\"]].copy()\n",
    "\n",
    "X_all = X_base.merge(labels_local, on=\"game_episode\", how=\"left\")\n",
    "X_train = X_all[train_mask].copy()\n",
    "X_test = X_all[~train_mask].copy()\n",
    "\n",
    "y_train_x = X_train[\"target_x_bin\"].astype(int)\n",
    "y_train_y = X_train[\"target_y_bin\"].astype(int)\n",
    "\n",
    "drop_cols = [\"game_episode\", \"target_x_bin\", \"target_y_bin\"]\n",
    "if \"game_id\" in X_train.columns:\n",
    "    drop_cols.append(\"game_id\")\n",
    "\n",
    "X_train_feat = X_train.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
    "X_test_feat = X_test.drop(columns=drop_cols, errors=\"ignore\").fillna(0)\n",
    "\n",
    "train_x = X_train_feat.copy()\n",
    "train_x[\"target_x_bin\"] = y_train_x\n",
    "train_y = X_train_feat.copy()\n",
    "train_y[\"target_y_bin\"] = y_train_y\n",
    "\n",
    "best_tag = str(BEST_BIN_Y).replace(\".\", \"p\")\n",
    "model_path_x = f\"ag_models_xbin_BEST_BY{best_tag}\"\n",
    "model_path_y = f\"ag_models_ybin_BEST_BY{best_tag}\"\n",
    "\n",
    "predictor_x = TabularPredictor(\n",
    "    label=\"target_x_bin\",\n",
    "    problem_type=\"multiclass\",\n",
    "    eval_metric=\"log_loss\",\n",
    "    path=model_path_x\n",
    ").fit(\n",
    "    train_data=train_x,\n",
    "    time_limit=TIME_LIMIT,\n",
    "    presets=PRESETS,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "predictor_y = TabularPredictor(\n",
    "    label=\"target_y_bin\",\n",
    "    problem_type=\"multiclass\",\n",
    "    eval_metric=\"log_loss\",\n",
    "    path=model_path_y\n",
    ").fit(\n",
    "    train_data=train_y,\n",
    "    time_limit=TIME_LIMIT,\n",
    "    presets=PRESETS,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# proba -> softavg coords\n",
    "proba_x = predictor_x.predict_proba(X_test_feat).values\n",
    "proba_y = predictor_y.predict_proba(X_test_feat).values\n",
    "\n",
    "pred_x = np.array([proba_to_coord_topk_softavg(p, BIN_X, BEST_TOPK) for p in proba_x])\n",
    "pred_y = np.array([proba_to_coord_topk_softavg(p, BEST_BIN_Y, BEST_TOPK) for p in proba_y])\n",
    "\n",
    "pred_x = np.clip(pred_x, 0, 105)\n",
    "pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "pred_df = X_test[[\"game_episode\"]].copy()\n",
    "pred_df[\"end_x\"] = pred_x\n",
    "pred_df[\"end_y\"] = pred_y\n",
    "\n",
    "sub = sample_sub.copy().set_index(\"game_episode\")\n",
    "pred_df = pred_df.set_index(\"game_episode\")\n",
    "\n",
    "sub[\"end_x\"] = pred_df[\"end_x\"]\n",
    "sub[\"end_y\"] = pred_df[\"end_y\"]\n",
    "sub = sub.reset_index()[[\"game_episode\", \"end_x\", \"end_y\"]]\n",
    "\n",
    "sub.to_csv(FINAL_SUBMISSION_NAME, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ FINAL submission saved:\", FINAL_SUBMISSION_NAME)\n",
    "print(sub.head(10))\n",
    "print(\"‚úÖ FINAL columns:\", sub.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11416c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î™®Îç∏ Í≤ΩÎ°ú: ag_models_xbin_BEST_BY6p0, ag_models_ybin_BEST_BY6p0\n",
      "Ï†úÏ∂ú ÌååÏùº: submission_FINAL_flagged.csv\n",
      "‚úì Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "======================================================================\n",
      "3Îã®Í≥Ñ Í≤ÄÏ¶ùÎ≤ï (BIN + SoftAvg)\n",
      "======================================================================\n",
      "\n",
      "1Îã®Í≥Ñ: Feature Importance Í±¥ÎÑàÎúÄ\n",
      "\n",
      "======================================================================\n",
      "[2Îã®Í≥Ñ] Leaderboard (log_loss Í∏∞Ï§Ä)\n",
      "======================================================================\n",
      "\n",
      "[X-bin Î™®Îç∏]\n",
      "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0  WeightedEnsemble_L2  -2.104453    log_loss       0.041228  376.767408   \n",
      "1             CatBoost  -2.119763    log_loss       0.010085  299.704512   \n",
      "2             LightGBM  -2.155898    log_loss       0.030132   77.010774   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.001011           0.052122            2       True   \n",
      "1                0.010085         299.704512            1       True   \n",
      "2                0.030132          77.010774            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          3  \n",
      "1          2  \n",
      "2          1  \n",
      "‚úì Best log_loss (X): -2.10445\n",
      "\n",
      "[Y-bin Î™®Îç∏]\n",
      "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0  WeightedEnsemble_L2  -1.658445    log_loss       0.031850  249.222392   \n",
      "1             CatBoost  -1.667581    log_loss       0.008246  195.382505   \n",
      "2             LightGBM  -1.685180    log_loss       0.022702   53.795528   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.000902           0.044359            2       True   \n",
      "1                0.008246         195.382505            1       True   \n",
      "2                0.022702          53.795528            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          3  \n",
      "1          2  \n",
      "2          1  \n",
      "‚úì Best log_loss (Y): -1.65845\n",
      "\n",
      "[Train Accuracy Ï∞∏Í≥†Ïö©]\n",
      "Train Accuracy X-bin: 0.7811\n",
      "Train Accuracy Y-bin: 0.7239\n",
      "\n",
      "======================================================================\n",
      "[3Îã®Í≥Ñ] ÏÉÅÌô©Î≥Ñ ÏòàÏ∏° Î∂ÑÌè¨ Î∂ÑÏÑù\n",
      "======================================================================\n",
      "\n",
      "[Ï†ÑÏ≤¥ ÏòàÏ∏° Ï¢åÌëú ÌÜµÍ≥Ñ]\n",
      "             end_x        end_y\n",
      "count  2414.000000  2414.000000\n",
      "mean     65.800328    34.065301\n",
      "std      21.675982    24.125256\n",
      "min       4.665375     3.197103\n",
      "25%      48.307574     7.528308\n",
      "50%      68.200808    33.966287\n",
      "75%      84.929168    58.674702\n",
      "max     101.748283    68.000000\n",
      "\n",
      "[Í≥µÍ≤© Î∞©Ìñ• sanity check]\n",
      "end_x ÌèâÍ∑†: 65.80 (Í∞íÏù¥ ÌÅ¥ÏàòÎ°ù Ï†ÑÏßÑ Ìå®Ïä§ ÏÑ±Ìñ•)\n",
      "\n",
      "[YÏ∂ï Î∂ÑÏÇ∞ sanity check]\n",
      "end_y ÌëúÏ§ÄÌé∏Ï∞®: 24.13 (ÎÑàÎ¨¥ ÏûëÏúºÎ©¥ Ï§ëÏïô Ïè†Î¶º ÏùòÏã¨)\n",
      "\n",
      "[Ï¥àÎ∞ò (0~30Î∂Ñ)]\n",
      "             end_x        end_y\n",
      "count  2414.000000  2414.000000\n",
      "mean     65.800328    34.065301\n",
      "std      21.675982    24.125256\n",
      "min       4.665375     3.197103\n",
      "25%      48.307574     7.528308\n",
      "50%      68.200808    33.966287\n",
      "75%      84.929168    58.674702\n",
      "max     101.748283    68.000000\n",
      "\n",
      "======================================================================\n",
      "Í≤ÄÏ¶ù ÏôÑÎ£å (BIN + SoftAvg)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Í≤∞Í≥º ÌôïÏù∏ Î∞è 3Îã®Í≥Ñ Í≤ÄÏ¶ùÎ≤ï\n",
    "# (BIN Classification + Soft Average Î≤ÑÏ†Ñ)\n",
    "# ÏÖÄ 0Ïùò Î≤†Ïä§Ìä∏ ÏÑ§Ï†ïÏóê ÎßûÏ∂∞ÏÑú Í≤ÄÏ¶ù\n",
    "# =======================\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# =======================\n",
    "# Ïã§Ìñâ ÌîåÎûòÍ∑∏\n",
    "# =======================\n",
    "RUN_STEP1_FEATURE_IMPORTANCE = False   # ‚ö†Ô∏è Îß§Ïö∞ ÎäêÎ¶º\n",
    "RUN_STEP2_LEADERBOARD = True\n",
    "RUN_STEP3_SITUATIONAL_ANALYSIS = True\n",
    "\n",
    "# =======================\n",
    "# Î™®Îç∏ Í≤ΩÎ°ú & Ï†úÏ∂ú ÌååÏùº (ÏÖÄ 0Ïùò Î≤†Ïä§Ìä∏ ÏÑ§Ï†ï ÏÇ¨Ïö©)\n",
    "# =======================\n",
    "# ÏÖÄ 0ÏóêÏÑú ÏÉùÏÑ±Îêú Î≤†Ïä§Ìä∏ Î™®Îç∏ Í≤ΩÎ°ú ÏÇ¨Ïö©\n",
    "# Î≤†Ïä§Ìä∏ ÏÑ§Ï†ïÏù¥ Ïù¥ÎØ∏ ÌïôÏäµÎêòÏñ¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï\n",
    "# ÎßåÏïΩ Î≤†Ïä§Ìä∏ Î™®Îç∏Ïù¥ ÏóÜÎã§Î©¥, ÏÖÄ 0Ïùò resultsÏóêÏÑú ÏµúÏã† Î™®Îç∏ Í≤ΩÎ°ú ÏÇ¨Ïö©\n",
    "\n",
    "# Î≤†Ïä§Ìä∏ Î™®Îç∏ Í≤ΩÎ°ú (ÏÖÄ 0Ïùò best_recÏóêÏÑú Í∞ÄÏ†∏Ïò§Í±∞ÎÇò ÏßÅÏ†ë ÏßÄÏ†ï)\n",
    "# ÏÖÄ 0 Ïã§Ìñâ ÌõÑ best_rec Î≥ÄÏàòÏóêÏÑú Í∞ÄÏ†∏Ïò¨ Ïàò ÏûàÏùå\n",
    "try:\n",
    "    # ÏÖÄ 0ÏóêÏÑú best_recÏù¥ Ï†ïÏùòÎêòÏñ¥ ÏûàÎã§Î©¥ ÏÇ¨Ïö©\n",
    "    best_tag = str(best_rec[\"BIN_Y\"]).replace(\".\", \"p\")\n",
    "    model_path_x = f\"ag_models_xbin_BEST_BY{best_tag}\"\n",
    "    model_path_y = f\"ag_models_ybin_BEST_BY{best_tag}\"\n",
    "    submission_filename = FINAL_SUBMISSION_NAME\n",
    "except NameError:\n",
    "    # ÏÖÄ 0Ïù¥ Ïã§ÌñâÎêòÏßÄ ÏïäÏïòÎã§Î©¥ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö© (ÏÇ¨Ïö©ÏûêÍ∞Ä ÏàòÎèôÏúºÎ°ú ÏßÄÏ†ï)\n",
    "    print(\"‚ö†Ô∏è ÏÖÄ 0Ïùò best_recÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Í∏∞Î≥∏ Í≤ΩÎ°úÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "    print(\"   ÏÖÄ 0ÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÍ±∞ÎÇò, ÏïÑÎûò Í≤ΩÎ°úÎ•º ÏàòÎèôÏúºÎ°ú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.\")\n",
    "    # ÏòàÏãú: BIN_Y=5.0Ïù∏ Í≤ΩÏö∞\n",
    "    model_path_x = \"ag_models_xbin_BEST_BY5p\"\n",
    "    model_path_y = \"ag_models_ybin_BEST_BY5p\"\n",
    "    submission_filename = \"submission_FINAL_flagged.csv\"\n",
    "\n",
    "print(f\"Î™®Îç∏ Í≤ΩÎ°ú: {model_path_x}, {model_path_y}\")\n",
    "print(f\"Ï†úÏ∂ú ÌååÏùº: {submission_filename}\")\n",
    "\n",
    "# =======================\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "# =======================\n",
    "try:\n",
    "    predictor_x = TabularPredictor.load(model_path_x)\n",
    "    predictor_y = TabularPredictor.load(model_path_y)\n",
    "    print(\"‚úì Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}\")\n",
    "    print(\"   ÏÖÄ 0ÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏó¨ Î™®Îç∏ÏùÑ ÌïôÏäµÌïòÏÑ∏Ïöî.\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"3Îã®Í≥Ñ Í≤ÄÏ¶ùÎ≤ï (BIN + SoftAvg)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# 1Îã®Í≥Ñ: Feature Importance (Ï∞∏Í≥†Ïö©)\n",
    "# ============================================================\n",
    "if RUN_STEP1_FEATURE_IMPORTANCE:\n",
    "    print(\"\\n[1Îã®Í≥Ñ] Feature Importance ÌôïÏù∏\")\n",
    "\n",
    "    print(\"\\n[X-bin Î™®Îç∏]\")\n",
    "    fi_x = predictor_x.feature_importance(data=X_train_feat)\n",
    "    print(fi_x.iloc[:, 0].sort_values(ascending=False).head(20))\n",
    "\n",
    "    print(\"\\n[Y-bin Î™®Îç∏]\")\n",
    "    fi_y = predictor_y.feature_importance(data=X_train_feat)\n",
    "    print(fi_y.iloc[:, 0].sort_values(ascending=False).head(20))\n",
    "\n",
    "else:\n",
    "    print(\"\\n1Îã®Í≥Ñ: Feature Importance Í±¥ÎÑàÎúÄ\")\n",
    "\n",
    "# ============================================================\n",
    "# 2Îã®Í≥Ñ: Leaderboard (log_loss Í∏∞Ï§Ä)\n",
    "# ============================================================\n",
    "if RUN_STEP2_LEADERBOARD:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[2Îã®Í≥Ñ] Leaderboard (log_loss Í∏∞Ï§Ä)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\n[X-bin Î™®Îç∏]\")\n",
    "    lb_x = predictor_x.leaderboard(silent=True)\n",
    "    print(lb_x.head(5))\n",
    "    print(f\"‚úì Best log_loss (X): {lb_x.iloc[0]['score_val']:.5f}\")\n",
    "\n",
    "    print(\"\\n[Y-bin Î™®Îç∏]\")\n",
    "    lb_y = predictor_y.leaderboard(silent=True)\n",
    "    print(lb_y.head(5))\n",
    "    print(f\"‚úì Best log_loss (Y): {lb_y.iloc[0]['score_val']:.5f}\")\n",
    "\n",
    "    # ‚ö†Ô∏è Train accuracyÎäî Ï∞∏Í≥†Ïö© (RMSE ÎåÄÌöå)\n",
    "    print(\"\\n[Train Accuracy Ï∞∏Í≥†Ïö©]\")\n",
    "    pred_x_train = predictor_x.predict(X_train_feat)\n",
    "    pred_y_train = predictor_y.predict(X_train_feat)\n",
    "\n",
    "    acc_x = (pred_x_train == y_train_x).mean()\n",
    "    acc_y = (pred_y_train == y_train_y).mean()\n",
    "\n",
    "    print(f\"Train Accuracy X-bin: {acc_x:.4f}\")\n",
    "    print(f\"Train Accuracy Y-bin: {acc_y:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n2Îã®Í≥Ñ: Leaderboard Í±¥ÎÑàÎúÄ\")\n",
    "\n",
    "# ============================================================\n",
    "# 3Îã®Í≥Ñ: ÏÉÅÌô©Î≥Ñ ÏòàÏ∏° Î∂ÑÌè¨ Î∂ÑÏÑù (‚≠ê ÌïµÏã¨)\n",
    "# ============================================================\n",
    "if RUN_STEP3_SITUATIONAL_ANALYSIS:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[3Îã®Í≥Ñ] ÏÉÅÌô©Î≥Ñ ÏòàÏ∏° Î∂ÑÌè¨ Î∂ÑÏÑù\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Ï†úÏ∂ú ÌååÏùº Î°úÎìú\n",
    "    try:\n",
    "        sub = pd.read_csv(submission_filename)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è Ï†úÏ∂ú ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {submission_filename}\")\n",
    "        print(\"   ÏÖÄ 0ÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏó¨ Ï†úÏ∂ú ÌååÏùºÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.\")\n",
    "        sub = None\n",
    "\n",
    "    if sub is not None:\n",
    "        # --------------------------------------------------------\n",
    "        # X_testÏóêÏÑú Ï°¥Ïû¨ÌïòÎäî Ïª¨ÎüºÎßå ÏïàÏ†ÑÌïòÍ≤å ÏÑ†ÌÉù\n",
    "        # --------------------------------------------------------\n",
    "        merge_cols = [\"game_episode\"]\n",
    "\n",
    "        if \"period_id\" in X_test.columns:\n",
    "            merge_cols.append(\"period_id\")\n",
    "\n",
    "        if \"game_clock_min\" in X_test.columns:\n",
    "            merge_cols.append(\"game_clock_min\")\n",
    "\n",
    "        # Î≥ëÌï©\n",
    "        merged = X_test[merge_cols].merge(\n",
    "            sub, on=\"game_episode\", how=\"left\"\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # Ïª¨Îüº Ï°¥Ïû¨ sanity check\n",
    "        # --------------------------------------------------------\n",
    "        if (\"end_x\" not in merged.columns) or (\"end_y\" not in merged.columns):\n",
    "            raise ValueError(\n",
    "                f\"‚ùå Ï†úÏ∂ú ÌååÏùº Ïª¨Îüº Ïò§Î•ò\\n\"\n",
    "                f\"merged columns: {merged.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # Ï†ÑÏ≤¥ Î∂ÑÌè¨\n",
    "        # --------------------------------------------------------\n",
    "        print(\"\\n[Ï†ÑÏ≤¥ ÏòàÏ∏° Ï¢åÌëú ÌÜµÍ≥Ñ]\")\n",
    "        print(merged[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # Ï†Ñ / ÌõÑÎ∞ò ÎπÑÍµê\n",
    "        # --------------------------------------------------------\n",
    "        if \"period_id\" in merged.columns:\n",
    "            first_half = merged[merged[\"period_id\"] == 1]\n",
    "            second_half = merged[merged[\"period_id\"] == 2]\n",
    "\n",
    "            if len(first_half) > 0:\n",
    "                print(\"\\n[Ï†ÑÎ∞òÏ†Ñ ÏòàÏ∏° Î∂ÑÌè¨]\")\n",
    "                print(first_half[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "            if len(second_half) > 0:\n",
    "                print(\"\\n[ÌõÑÎ∞òÏ†Ñ ÏòàÏ∏° Î∂ÑÌè¨]\")\n",
    "                print(second_half[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # Í≥µÍ≤© Î∞©Ìñ• sanity check (XÏ∂ï)\n",
    "        # --------------------------------------------------------\n",
    "        mean_x = merged[\"end_x\"].mean()\n",
    "        print(\"\\n[Í≥µÍ≤© Î∞©Ìñ• sanity check]\")\n",
    "        print(\n",
    "            f\"end_x ÌèâÍ∑†: {mean_x:.2f} \"\n",
    "            \"(Í∞íÏù¥ ÌÅ¥ÏàòÎ°ù Ï†ÑÏßÑ Ìå®Ïä§ ÏÑ±Ìñ•)\"\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # YÏ∂ï Ï§ëÏïô Ïè†Î¶º sanity check\n",
    "        # --------------------------------------------------------\n",
    "        std_y = merged[\"end_y\"].std()\n",
    "        print(\"\\n[YÏ∂ï Î∂ÑÏÇ∞ sanity check]\")\n",
    "        print(\n",
    "            f\"end_y ÌëúÏ§ÄÌé∏Ï∞®: {std_y:.2f} \"\n",
    "            \"(ÎÑàÎ¨¥ ÏûëÏúºÎ©¥ Ï§ëÏïô Ïè†Î¶º ÏùòÏã¨)\"\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # ÏãúÍ∞ÑÎåÄÎ≥Ñ (ÏÑ†ÌÉù)\n",
    "        # --------------------------------------------------------\n",
    "        if \"game_clock_min\" in merged.columns:\n",
    "            early = merged[merged[\"game_clock_min\"] < 30]\n",
    "            late = merged[merged[\"game_clock_min\"] > 75]\n",
    "\n",
    "            if len(early) > 0:\n",
    "                print(\"\\n[Ï¥àÎ∞ò (0~30Î∂Ñ)]\")\n",
    "                print(early[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "            if len(late) > 0:\n",
    "                print(\"\\n[ÌõÑÎ∞ò ÎßâÌåê (75Î∂Ñ~)]\")\n",
    "                print(late[[\"end_x\", \"end_y\"]].describe())\n",
    "\n",
    "else:\n",
    "    print(\"\\n3Îã®Í≥Ñ: ÏÉÅÌô© Î∂ÑÏÑù Í±¥ÎÑàÎúÄ\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Í≤ÄÏ¶ù ÏôÑÎ£å (BIN + SoftAvg)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
