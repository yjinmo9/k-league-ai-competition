{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6e05691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/miniconda3/lib/python3.13/site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/miniconda3/lib/python3.13/site-packages (from lightgbm) (2.1.3)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.13/site-packages (from lightgbm) (1.16.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75b56c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ í•µì‹¬ í”¼ì²˜ 3ê°œ ì¶”ê°€ ì™„ë£Œ (dist_to_sideline, angle_to_goal_center, time_pos_inter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_90174/3983062393.py:276: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ ìë™í™”ëœ í”¼ì²˜ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n",
      "================================================================================\n",
      "ì´ 6ê°œ ì‹¤í—˜ ì‹¤í–‰ ì˜ˆì •\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ ì‹¤í—˜ 1/6: baseline\n",
      "================================================================================\n",
      "ğŸ“Š Baseline ëª¨ë“œ (ì¶”ê°€ í”¼ì²˜ ì—†ìŒ)\n",
      "  ğŸ“¦ í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "  âœ“ í”¼ì²˜ ìƒì„± ì™„ë£Œ (0.0ì´ˆ) | Train: 808 features, Test: 808 features\n",
      "  ğŸ”„ CV í•™ìŠµ ì‹œì‘ (n_splits=5)...\n",
      "    â””â”€ Fold 1/5 í•™ìŠµ ì¤‘... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_90174/3983062393.py:347: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train_feat.fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_90174/3983062393.py:348: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test_feat.fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_dist = 14.662034 (285.6ì´ˆ)\n",
      "    â””â”€ Fold 2/5 í•™ìŠµ ì¤‘... mean_dist = 14.732737 (277.3ì´ˆ)\n",
      "    â””â”€ Fold 3/5 í•™ìŠµ ì¤‘... mean_dist = 14.567390 (322.2ì´ˆ)\n",
      "    â””â”€ Fold 4/5 í•™ìŠµ ì¤‘... mean_dist = 14.277168 (409.1ì´ˆ)\n",
      "    â””â”€ Fold 5/5 í•™ìŠµ ì¤‘... mean_dist = 14.650447 (345.1ì´ˆ)\n",
      "  âœ“ ì‹¤í—˜ ì™„ë£Œ (ì´ 1639.4ì´ˆ)\n",
      "  ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: feature_experiment_results.json\n",
      "âœ… baseline ì™„ë£Œ | mean_dist = 14.577955 Â± 0.159290\n",
      "   p90 = 29.223168, p95 = 36.793945\n",
      "  â±ï¸  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: ì•½ 136.6ë¶„ (5ê°œ ì‹¤í—˜ ë‚¨ìŒ)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ ì‹¤í—˜ 2/6: Recency\n",
      "================================================================================\n",
      "ğŸ”§ í™œì„±í™”ëœ í”¼ì²˜: Recency\n",
      "  ğŸ“¦ í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "  âœ“ Recency ê°€ì¤‘ í”¼ì²˜ ì¶”ê°€ ì¤‘...\n",
      "  âœ“ í”¼ì²˜ ìƒì„± ì™„ë£Œ (0.2ì´ˆ) | Train: 924 features, Test: 924 features\n",
      "  ğŸ”„ CV í•™ìŠµ ì‹œì‘ (n_splits=5)...\n",
      "    â””â”€ Fold 1/5 í•™ìŠµ ì¤‘... mean_dist = 14.771770 (444.8ì´ˆ)\n",
      "    â””â”€ Fold 2/5 í•™ìŠµ ì¤‘... mean_dist = 14.735575 (342.2ì´ˆ)\n",
      "    â””â”€ Fold 3/5 í•™ìŠµ ì¤‘... mean_dist = 14.616510 (338.9ì´ˆ)\n",
      "    â””â”€ Fold 4/5 í•™ìŠµ ì¤‘... mean_dist = 14.249585 (353.3ì´ˆ)\n",
      "    â””â”€ Fold 5/5 í•™ìŠµ ì¤‘... mean_dist = 14.693172 (350.9ì´ˆ)\n",
      "  âœ“ ì‹¤í—˜ ì™„ë£Œ (ì´ 1830.4ì´ˆ)\n",
      "  ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: feature_experiment_results.json\n",
      "âœ… Recency ì™„ë£Œ | mean_dist = 14.613322 Â± 0.189073\n",
      "   p90 = 29.092223, p95 = 36.594408\n",
      "  â±ï¸  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: ì•½ 115.7ë¶„ (4ê°œ ì‹¤í—˜ ë‚¨ìŒ)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ ì‹¤í—˜ 3/6: Direction\n",
      "================================================================================\n",
      "ğŸ”§ í™œì„±í™”ëœ í”¼ì²˜: Direction\n",
      "  ğŸ“¦ í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "  âœ“ ìµœê·¼ ì´ë™ ë°©í–¥ ìš”ì•½ í”¼ì²˜ ì¶”ê°€ ì¤‘...\n",
      "  âœ“ í”¼ì²˜ ìƒì„± ì™„ë£Œ (0.1ì´ˆ) | Train: 813 features, Test: 813 features\n",
      "  ğŸ”„ CV í•™ìŠµ ì‹œì‘ (n_splits=5)...\n",
      "    â””â”€ Fold 1/5 í•™ìŠµ ì¤‘... mean_dist = 14.650279 (304.3ì´ˆ)\n",
      "    â””â”€ Fold 2/5 í•™ìŠµ ì¤‘... mean_dist = 14.702945 (270.9ì´ˆ)\n",
      "    â””â”€ Fold 3/5 í•™ìŠµ ì¤‘... mean_dist = 14.553869 (255.8ì´ˆ)\n",
      "    â””â”€ Fold 4/5 í•™ìŠµ ì¤‘... mean_dist = 14.209149 (272.0ì´ˆ)\n",
      "    â””â”€ Fold 5/5 í•™ìŠµ ì¤‘... mean_dist = 14.637864 (271.4ì´ˆ)\n",
      "  âœ“ ì‹¤í—˜ ì™„ë£Œ (ì´ 1374.6ì´ˆ)\n",
      "  ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: feature_experiment_results.json\n",
      "âœ… Direction ì™„ë£Œ | mean_dist = 14.550821 Â± 0.177403\n",
      "   p90 = 29.212182, p95 = 36.499497\n",
      "  â±ï¸  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: ì•½ 80.7ë¶„ (3ê°œ ì‹¤í—˜ ë‚¨ìŒ)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ ì‹¤í—˜ 4/6: FieldBoundary\n",
      "================================================================================\n",
      "ğŸ”§ í™œì„±í™”ëœ í”¼ì²˜: FieldBoundary\n",
      "  ğŸ“¦ í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "  âœ“ í•„ë“œ ê²½ê³„ ì¸ì‹ ê°•í™” í”¼ì²˜ ì¶”ê°€ ì¤‘...\n",
      "  âœ“ í”¼ì²˜ ìƒì„± ì™„ë£Œ (0.1ì´ˆ) | Train: 811 features, Test: 811 features\n",
      "  ğŸ”„ CV í•™ìŠµ ì‹œì‘ (n_splits=5)...\n",
      "    â””â”€ Fold 1/5 í•™ìŠµ ì¤‘... mean_dist = 14.724675 (268.9ì´ˆ)\n",
      "    â””â”€ Fold 2/5 í•™ìŠµ ì¤‘... mean_dist = 14.656687 (281.7ì´ˆ)\n",
      "    â””â”€ Fold 3/5 í•™ìŠµ ì¤‘... mean_dist = 14.554035 (259.6ì´ˆ)\n",
      "    â””â”€ Fold 4/5 í•™ìŠµ ì¤‘... mean_dist = 14.254201 (262.9ì´ˆ)\n",
      "    â””â”€ Fold 5/5 í•™ìŠµ ì¤‘... mean_dist = 14.659609 (265.6ì´ˆ)\n",
      "  âœ“ ì‹¤í—˜ ì™„ë£Œ (ì´ 1338.8ì´ˆ)\n",
      "  ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: feature_experiment_results.json\n",
      "âœ… FieldBoundary ì™„ë£Œ | mean_dist = 14.569841 Â± 0.167005\n",
      "   p90 = 29.107168, p95 = 36.476508\n",
      "  â±ï¸  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: ì•½ 51.5ë¶„ (2ê°œ ì‹¤í—˜ ë‚¨ìŒ)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ ì‹¤í—˜ 5/6: SpeedConstraint\n",
      "================================================================================\n",
      "ğŸ”§ í™œì„±í™”ëœ í”¼ì²˜: SpeedConstraint\n",
      "  ğŸ“¦ í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "  âœ“ ìµœê·¼ ì†ë„ ê¸°ë°˜ ì´ë™ í•œê³„ í”¼ì²˜ ì¶”ê°€ ì¤‘...\n",
      "  âœ“ í”¼ì²˜ ìƒì„± ì™„ë£Œ (0.1ì´ˆ) | Train: 808 features, Test: 808 features\n",
      "  ğŸ”„ CV í•™ìŠµ ì‹œì‘ (n_splits=5)...\n",
      "    â””â”€ Fold 1/5 í•™ìŠµ ì¤‘... mean_dist = 14.662034 (263.4ì´ˆ)\n",
      "    â””â”€ Fold 2/5 í•™ìŠµ ì¤‘... mean_dist = 14.732737 (284.2ì´ˆ)\n",
      "    â””â”€ Fold 3/5 í•™ìŠµ ì¤‘... mean_dist = 14.567390 (263.7ì´ˆ)\n",
      "    â””â”€ Fold 4/5 í•™ìŠµ ì¤‘... mean_dist = 14.277168 (277.1ì´ˆ)\n",
      "    â””â”€ Fold 5/5 í•™ìŠµ ì¤‘... mean_dist = 14.650447 (277.7ì´ˆ)\n",
      "  âœ“ ì‹¤í—˜ ì™„ë£Œ (ì´ 1366.1ì´ˆ)\n",
      "  ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: feature_experiment_results.json\n",
      "âœ… SpeedConstraint ì™„ë£Œ | mean_dist = 14.577955 Â± 0.159290\n",
      "   p90 = 29.223168, p95 = 36.793945\n",
      "  â±ï¸  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: ì•½ 25.2ë¶„ (1ê°œ ì‹¤í—˜ ë‚¨ìŒ)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ ì‹¤í—˜ 6/6: CornerContext\n",
      "================================================================================\n",
      "ğŸ”§ í™œì„±í™”ëœ í”¼ì²˜: CornerContext\n",
      "  ğŸ“¦ í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "  âœ“ ì½”ë„ˆ/ì‚¬ì´ë“œ ìƒí™© ê°•ì¡° í”¼ì²˜ ì¶”ê°€ ì¤‘...\n",
      "  âœ“ í”¼ì²˜ ìƒì„± ì™„ë£Œ (0.1ì´ˆ) | Train: 811 features, Test: 811 features\n",
      "  ğŸ”„ CV í•™ìŠµ ì‹œì‘ (n_splits=5)...\n",
      "    â””â”€ Fold 1/5 í•™ìŠµ ì¤‘... mean_dist = 14.712639 (264.6ì´ˆ)\n",
      "    â””â”€ Fold 2/5 í•™ìŠµ ì¤‘... mean_dist = 14.728702 (261.7ì´ˆ)\n",
      "    â””â”€ Fold 3/5 í•™ìŠµ ì¤‘... mean_dist = 14.614302 (274.1ì´ˆ)\n",
      "    â””â”€ Fold 4/5 í•™ìŠµ ì¤‘... mean_dist = 14.313998 (290.2ì´ˆ)\n",
      "    â””â”€ Fold 5/5 í•™ìŠµ ì¤‘... mean_dist = 14.633368 (279.6ì´ˆ)\n",
      "  âœ“ ì‹¤í—˜ ì™„ë£Œ (ì´ 1370.4ì´ˆ)\n",
      "  ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: feature_experiment_results.json\n",
      "âœ… CornerContext ì™„ë£Œ | mean_dist = 14.600602 Â± 0.149910\n",
      "   p90 = 29.005147, p95 = 36.642515\n",
      "\n",
      "================================================================================\n",
      "â±ï¸  ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: 148.7ë¶„ (8919.9ì´ˆ)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š í”¼ì²˜ ì‹¤í—˜ ê²°ê³¼ ë¹„êµ\n",
      "================================================================================\n",
      "ì‹¤í—˜ëª…                  ì¶”ê°€ í”¼ì²˜                mean_dist       Î”mean        p90          p95          pred_std_x   pred_std_y  \n",
      "--------------------------------------------------------------------------------\n",
      "baseline             ì—†ìŒ                   14.577955       -            29.223168    36.793945    20.827691    20.668531   \n",
      "Recency              Recency              14.613322       +0.035367    29.092223    36.594408    20.808381    20.665530   \n",
      "Direction            Direction            14.550821       -0.027134    29.212182    36.499497    20.795611    20.669608   \n",
      "FieldBoundary        FieldBoundary        14.569841       -0.008114    29.107168    36.476508    20.807762    20.670965   \n",
      "SpeedConstraint      SpeedConstraint      14.577955       +0.000000    29.223168    36.793945    20.827691    20.668531   \n",
      "CornerContext        CornerContext        14.600602       +0.022647    29.005147    36.642515    20.821635    20.657491   \n",
      "================================================================================\n",
      "ğŸ’¡ ì°¸ê³ :\n",
      "   - Î”mean: baseline ëŒ€ë¹„ ë³€í™”ëŸ‰ (ìŒìˆ˜ = ê°œì„ , ì–‘ìˆ˜ = ì•…í™”)\n",
      "   - Î”mean_distê°€ -0.03 ì´í•˜ì´ë©´ ì˜ë¯¸ ìˆëŠ” ê°œì„ ìœ¼ë¡œ ë´…ë‹ˆë‹¤.\n",
      "================================================================================\n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥: Direction (mean_dist = 14.550821)\n",
      "   Baseline ëŒ€ë¹„ ê°œì„ : 0.027134 (0.19%)\n",
      "\n",
      "ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: feature_experiment_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ============================================================\n",
    "# K-League Track1 - LGBM Direct Regression (Wide Feature + CV)\n",
    "# ------------------------------------------------------------\n",
    "# âœ… ëª©í‘œ: end_x, end_yë¥¼ BIN ì—†ì´ \"ì§ì ‘ íšŒê·€\"ë¡œ ì˜ˆì¸¡\n",
    "# âœ… í•µì‹¬: ë§ˆì§€ë§‰ K ì´ë²¤íŠ¸ë¥¼ wide featureë¡œ í¼ì¹œ ë’¤,\n",
    "#         LGBMRegressorë¡œ target_x, target_y ê°ê° í•™ìŠµ\n",
    "# âœ… ì˜µì…˜: KFoldë¡œ CV(OOF) ì ìˆ˜ í™•ì¸ í›„, ì „ì²´ í•™ìŠµ â†’ Test ì˜ˆì¸¡\n",
    "#\n",
    "# âš ï¸ BASE_PATH ë° íŒŒì¼ ê²½ë¡œëŠ” ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ìœ ì§€\n",
    "# ============================================================\n",
    "\n",
    "# ----------------------\n",
    "# 0. ì„¤ì • (ê²½ë¡œ/í•˜ì´í¼íŒŒë¼ë¯¸í„°)\n",
    "# ----------------------\n",
    "BASE_PATH = \"open_track1/\"\n",
    "PATH_TRAIN = os.path.join(BASE_PATH, \"train.csv\")\n",
    "PATH_TEST = os.path.join(BASE_PATH, \"test.csv\")\n",
    "PATH_MATCH_INFO = os.path.join(BASE_PATH, \"match_info.csv\")\n",
    "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "K = 30   # ë§ˆì§€ë§‰ K ì´ë²¤íŠ¸ ì‚¬ìš© (20~32 ì‚¬ì´ ì„ íƒ)\n",
    "\n",
    "# CV ì„¤ì •\n",
    "USE_CV = True\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "\n",
    "# LGBM ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ë„ˆë¬´ ê³¼íŠœë‹í•˜ì§€ ë§ê³ , ì•ˆì •ì ìœ¼ë¡œ ì‹œì‘)\n",
    "LGB_PARAMS_BASE = dict(\n",
    "    n_estimators=5000,          # early stoppingìœ¼ë¡œ ì‹¤ì œ ì‚¬ìš© ê°œìˆ˜ ìë™ ê²°ì •\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=50,       # (=min_data_in_leaf ë¹„ìŠ·)\n",
    "    subsample=0.8,              # bagging_fraction\n",
    "    subsample_freq=1,           # bagging_freq\n",
    "    colsample_bytree=0.8,       # feature_fraction\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# OOF ì•™ìƒë¸”ì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©\n",
    "# seed: [42, 2023, 777]\n",
    "# num_leaves: [48, 64, 96]\n",
    "# min_child_samples: [30, 50]\n",
    "ENSEMBLE_SEEDS = [42, 2023, 777]\n",
    "ENSEMBLE_NUM_LEAVES = [48, 64, 96]\n",
    "ENSEMBLE_MIN_CHILD_SAMPLES = [30, 50]\n",
    "\n",
    "# ì•™ìƒë¸” ì‚¬ìš© ì—¬ë¶€\n",
    "USE_ENSEMBLE = False  # í”¼ì²˜ ì‹¤í—˜ ì‹œì—ëŠ” False, ìµœì¢… ì œì¶œ ì‹œ Trueë¡œ ë³€ê²½\n",
    "\n",
    "# ============================================================\n",
    "# ìë™í™”ëœ í”¼ì²˜ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸\n",
    "# ============================================================\n",
    "# ëª¨ë“  í”¼ì²˜ë¥¼ ìˆœì„œëŒ€ë¡œ ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "RUN_FEATURE_EXPERIMENTS = True  # True: ìë™ ì‹¤í—˜, False: ìˆ˜ë™ í”Œë˜ê·¸ ì œì–´\n",
    "\n",
    "# ============================================================\n",
    "# í”¼ì²˜ ê³ ë„í™” ì‹¤í—˜ í”Œë˜ê·¸ (One-by-One ê²€ì¦)\n",
    "# ============================================================\n",
    "# âš ï¸ RUN_FEATURE_EXPERIMENTS = Trueì¼ ë•ŒëŠ” ìë™ìœ¼ë¡œ ì œì–´ë¨\n",
    "# âš ï¸ RUN_FEATURE_EXPERIMENTS = Falseì¼ ë•Œë§Œ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •\n",
    "\n",
    "# 1ï¸âƒ£ Recency ê°€ì¤‘ í”¼ì²˜ (ìµœìš°ì„ , â˜…â˜…â˜…â˜…â˜…)\n",
    "USE_RECENCY_WEIGHT = False\n",
    "RECENCY_ALPHA = 0.1  # exp(-alpha * (K - pos_in_K))\n",
    "\n",
    "# 2ï¸âƒ£ ìµœê·¼ ì´ë™ ë°©í–¥ ìš”ì•½ í”¼ì²˜ (â˜…â˜…â˜…â˜…â˜†)\n",
    "USE_DIRECTION_SUMMARY = False\n",
    "\n",
    "# 3ï¸âƒ£ í•„ë“œ ê²½ê³„ ì¸ì‹ ê°•í™” í”¼ì²˜ (â˜…â˜…â˜…â˜…â˜†)\n",
    "USE_FIELD_BOUNDARY = False\n",
    "FIELD_BOUNDARY_THRESHOLD = 5.0  # T = 5m\n",
    "\n",
    "# 4ï¸âƒ£ ìµœê·¼ ì†ë„ ê¸°ë°˜ ì´ë™ í•œê³„ í”¼ì²˜ (â˜…â˜…â˜…â˜†â˜†)\n",
    "USE_SPEED_CONSTRAINT = False\n",
    "\n",
    "# 5ï¸âƒ£ ì½”ë„ˆ/ì‚¬ì´ë“œ ìƒí™© ê°•ì¡° í”¼ì²˜ (â˜…â˜…â˜…â˜†â˜†)\n",
    "USE_CORNER_CONTEXT = False\n",
    "\n",
    "# ì‹¤í—˜í•  í”¼ì²˜ ëª©ë¡ (ìˆœì„œëŒ€ë¡œ)\n",
    "FEATURE_EXPERIMENTS = [\n",
    "    {\"name\": \"baseline\", \"USE_RECENCY_WEIGHT\": False, \"USE_DIRECTION_SUMMARY\": False, \n",
    "     \"USE_FIELD_BOUNDARY\": False, \"USE_SPEED_CONSTRAINT\": False, \"USE_CORNER_CONTEXT\": False},\n",
    "    {\"name\": \"Recency\", \"USE_RECENCY_WEIGHT\": True, \"USE_DIRECTION_SUMMARY\": False, \n",
    "     \"USE_FIELD_BOUNDARY\": False, \"USE_SPEED_CONSTRAINT\": False, \"USE_CORNER_CONTEXT\": False},\n",
    "    {\"name\": \"Direction\", \"USE_RECENCY_WEIGHT\": False, \"USE_DIRECTION_SUMMARY\": True, \n",
    "     \"USE_FIELD_BOUNDARY\": False, \"USE_SPEED_CONSTRAINT\": False, \"USE_CORNER_CONTEXT\": False},\n",
    "    {\"name\": \"FieldBoundary\", \"USE_RECENCY_WEIGHT\": False, \"USE_DIRECTION_SUMMARY\": False, \n",
    "     \"USE_FIELD_BOUNDARY\": True, \"USE_SPEED_CONSTRAINT\": False, \"USE_CORNER_CONTEXT\": False},\n",
    "    {\"name\": \"SpeedConstraint\", \"USE_RECENCY_WEIGHT\": False, \"USE_DIRECTION_SUMMARY\": False, \n",
    "     \"USE_FIELD_BOUNDARY\": False, \"USE_SPEED_CONSTRAINT\": True, \"USE_CORNER_CONTEXT\": False},\n",
    "    {\"name\": \"CornerContext\", \"USE_RECENCY_WEIGHT\": False, \"USE_DIRECTION_SUMMARY\": False, \n",
    "     \"USE_FIELD_BOUNDARY\": False, \"USE_SPEED_CONSTRAINT\": False, \"USE_CORNER_CONTEXT\": True},\n",
    "]\n",
    "\n",
    "# ----------------------\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "# ----------------------\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test_index = pd.read_csv(PATH_TEST)\n",
    "match_info = pd.read_csv(PATH_MATCH_INFO)  # í˜„ì¬ ì½”ë“œì—ì„œëŠ” ì§ì ‘ ì‚¬ìš© ì•ˆ í•¨ (ì›ë³¸ ìœ ì§€)\n",
    "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
    "\n",
    "# testëŠ” episodeë³„ csvë¡œ ë”°ë¡œ ì¡´ì¬ â†’ í•©ì³ì„œ eventsë¡œ ë§Œë“¤ê¸°\n",
    "test_events_list = []\n",
    "for _, row in test_index.iterrows():\n",
    "    test_path = os.path.join(BASE_PATH, row[\"path\"].lstrip(\"./\"))\n",
    "    df_ep = pd.read_csv(test_path)\n",
    "    test_events_list.append(df_ep)\n",
    "\n",
    "test_events = pd.concat(test_events_list, ignore_index=True)\n",
    "\n",
    "train[\"is_train\"] = 1\n",
    "test_events[\"is_train\"] = 0\n",
    "\n",
    "events = pd.concat([train, test_events], ignore_index=True)\n",
    "\n",
    "# ----------------------\n",
    "# 2. ê¸°ë³¸ ì •ë ¬ + episode ë‚´ ì¸ë±ìŠ¤\n",
    "# ----------------------\n",
    "events = events.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
    "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
    "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
    "\n",
    "# ----------------------\n",
    "# 3. ì‹œê°„/ê³µê°„ feature\n",
    "# ----------------------\n",
    "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
    "events[\"dt\"] = (events[\"time_seconds\"] - events[\"prev_time\"]).fillna(0.0)\n",
    "\n",
    "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
    "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
    "events[\"dist\"] = np.sqrt(events[\"dx\"]**2 + events[\"dy\"]**2)\n",
    "\n",
    "# dt=0 ë³´í˜¸\n",
    "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
    "\n",
    "# zone / lane\n",
    "events[\"x_zone\"] = (events[\"start_x\"] / (105/7)).astype(int).clip(0, 6)\n",
    "events[\"lane\"] = pd.cut(\n",
    "    events[\"start_y\"],\n",
    "    bins=[0, 68/3, 2*68/3, 68],\n",
    "    labels=[0, 1, 2],\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 3-1. ì½”ë„ˆ ê´€ë ¨ í”¼ì²˜ (Corner Features)\n",
    "# ----------------------\n",
    "events[\"angle_to_goal\"] = np.arctan2(\n",
    "    34 - events[\"start_y\"],\n",
    "    105 - events[\"start_x\"]\n",
    ") * 180 / np.pi\n",
    "\n",
    "events[\"dist_corner_top\"] = np.sqrt((105 - events[\"start_x\"])**2 + (0 - events[\"start_y\"])**2)\n",
    "events[\"dist_corner_bottom\"] = np.sqrt((105 - events[\"start_x\"])**2 + (68 - events[\"start_y\"])**2)\n",
    "events[\"dist_to_nearest_corner\"] = events[[\"dist_corner_top\", \"dist_corner_bottom\"]].min(axis=1)\n",
    "\n",
    "events[\"is_corner_area\"] = ((events[\"start_x\"] > 100) &\n",
    "                            ((events[\"start_y\"] < 5) | (events[\"start_y\"] > 63))).astype(int)\n",
    "\n",
    "events[\"angle_goal_x_corner\"] = events[\"angle_to_goal\"] * events[\"is_corner_area\"]\n",
    "events[\"dist_corner_x_angle\"] = events[\"dist_to_nearest_corner\"] * events[\"angle_to_goal\"]\n",
    "\n",
    "# ----------------------\n",
    "# 3-2. ìƒˆë¡œìš´ í•µì‹¬ í”¼ì²˜ 3ê°œ (ê³ ë“ì ìš©)\n",
    "# ----------------------\n",
    "events[\"dist_to_sideline\"] = events[\"start_y\"].apply(lambda y: min(y, 68-y))\n",
    "\n",
    "# ë¼ë””ì•ˆ ë‹¨ìœ„\n",
    "events[\"angle_to_goal_center\"] = np.arctan2(\n",
    "    34 - events[\"start_y\"],\n",
    "    105 - events[\"start_x\"]\n",
    ")\n",
    "\n",
    "events[\"time_pos_inter\"] = events[\"ep_idx_norm\"] * events[\"start_x\"]\n",
    "\n",
    "print(\"âœ“ í•µì‹¬ í”¼ì²˜ 3ê°œ ì¶”ê°€ ì™„ë£Œ (dist_to_sideline, angle_to_goal_center, time_pos_inter)\")\n",
    "\n",
    "# ----------------------\n",
    "# 4. ë¼ë²¨ ë° episode-level ë©”íƒ€ (train ì „ìš©)\n",
    "# ----------------------\n",
    "train_events = events[events[\"is_train\"] == 1].copy()\n",
    "\n",
    "last_events = (\n",
    "    train_events\n",
    "    .groupby(\"game_episode\", as_index=False)\n",
    "    .tail(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "labels = last_events[[\"game_episode\", \"end_x\", \"end_y\"]].rename(\n",
    "    columns={\"end_x\": \"target_x\", \"end_y\": \"target_y\"}\n",
    ")\n",
    "\n",
    "ep_meta = last_events[[\"game_episode\", \"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"]].copy()\n",
    "ep_meta = ep_meta.rename(columns={\"team_id\": \"final_team_id\"})\n",
    "\n",
    "ep_meta[\"game_clock_min\"] = np.where(\n",
    "    ep_meta[\"period_id\"] == 1,\n",
    "    ep_meta[\"time_seconds\"] / 60.0,\n",
    "    45.0 + ep_meta[\"time_seconds\"] / 60.0\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5. ê³µê²© íŒ€ í”Œë˜ê·¸ (final_team vs ìƒëŒ€)\n",
    "# ----------------------\n",
    "events = events.merge(\n",
    "    ep_meta[[\"game_episode\", \"final_team_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 6. ì…ë ¥ìš© eventsì—ì„œ ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ íƒ€ê¹ƒ ì •ë³´ ê°€ë¦¬ê¸°\n",
    "# ----------------------\n",
    "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
    "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
    "\n",
    "mask_last = events[\"is_last\"] == 1\n",
    "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
    "    events.loc[mask_last, col] = np.nan\n",
    "\n",
    "# ----------------------\n",
    "# 7. ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”© (type_name, result_name, team_id ë“±)\n",
    "# ----------------------\n",
    "events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
    "events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA_RES__\")\n",
    "\n",
    "le_type = LabelEncoder()\n",
    "le_res = LabelEncoder()\n",
    "\n",
    "events[\"type_id\"] = le_type.fit_transform(events[\"type_name\"])\n",
    "events[\"res_id\"] = le_res.fit_transform(events[\"result_name\"])\n",
    "\n",
    "if events[\"team_id\"].dtype == \"object\":\n",
    "    le_team = LabelEncoder()\n",
    "    events[\"team_id_enc\"] = le_team.fit_transform(events[\"team_id\"])\n",
    "else:\n",
    "    events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 8. ë§ˆì§€ë§‰ K ì´ë²¤íŠ¸ë§Œ ì‚¬ìš© (lastK)\n",
    "# ----------------------\n",
    "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\n",
    "    lambda s: s.max() - s\n",
    ")\n",
    "lastK = events[events[\"rev_idx\"] < K].copy()\n",
    "\n",
    "def assign_pos_in_K(df):\n",
    "    df = df.sort_values(\"event_idx\")  # ì˜¤ë˜ëœ â†’ ìµœê·¼\n",
    "    L = len(df)\n",
    "    df = df.copy()\n",
    "    df[\"pos_in_K\"] = np.arange(K - L, K)\n",
    "    return df\n",
    "\n",
    "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
    "\n",
    "# ----------------------\n",
    "# 9. wide feature pivot\n",
    "# ----------------------\n",
    "num_cols = [\n",
    "    \"start_x\", \"start_y\",\n",
    "    \"end_x\", \"end_y\",\n",
    "    \"dx\", \"dy\", \"dist\", \"speed\",\n",
    "    \"dt\",\n",
    "    \"ep_idx_norm\",\n",
    "    \"x_zone\", \"lane\",\n",
    "    \"is_final_team\",\n",
    "    # ì½”ë„ˆ ê´€ë ¨\n",
    "    \"angle_to_goal\",\n",
    "    \"dist_to_nearest_corner\",\n",
    "    \"is_corner_area\",\n",
    "    \"angle_goal_x_corner\",\n",
    "    \"dist_corner_x_angle\",\n",
    "    # í•µì‹¬ 3ê°œ\n",
    "    \"dist_to_sideline\",\n",
    "    \"angle_to_goal_center\",\n",
    "    \"time_pos_inter\",\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"type_id\",\n",
    "    \"res_id\",\n",
    "    \"team_id_enc\",\n",
    "    \"is_home\",\n",
    "    \"period_id\",\n",
    "    \"is_last\",\n",
    "]\n",
    "\n",
    "feature_cols = num_cols + cat_cols\n",
    "wide = lastK[[\"game_episode\", \"pos_in_K\"] + feature_cols].copy()\n",
    "\n",
    "wide_num = wide.pivot_table(index=\"game_episode\", columns=\"pos_in_K\", values=num_cols, aggfunc=\"first\")\n",
    "wide_cat = wide.pivot_table(index=\"game_episode\", columns=\"pos_in_K\", values=cat_cols, aggfunc=\"first\")\n",
    "\n",
    "wide_num.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_num.columns]\n",
    "wide_cat.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_cat.columns]\n",
    "\n",
    "X = pd.concat([wide_num, wide_cat], axis=1).reset_index()\n",
    "\n",
    "# episode-level ë©”íƒ€ ë¶™ì´ê¸°\n",
    "X = X.merge(\n",
    "    ep_meta[[\"game_episode\", \"game_id\", \"game_clock_min\", \"final_team_id\", \"is_home\", \"period_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# train ë¼ë²¨ ë¶™ì´ê¸°\n",
    "X = X.merge(labels, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# ----------------------\n",
    "# 10. train/test ë¶„ë¦¬\n",
    "# ----------------------\n",
    "train_mask = X[\"game_episode\"].isin(labels[\"game_episode\"])\n",
    "X_train = X[train_mask].copy()\n",
    "X_test = X[~train_mask].copy()\n",
    "\n",
    "y_train_x = X_train[\"target_x\"].astype(float).values\n",
    "y_train_y = X_train[\"target_y\"].astype(float).values\n",
    "\n",
    "drop_cols = [\"game_episode\", \"game_id\", \"target_x\", \"target_y\"]\n",
    "\n",
    "X_train_feat = X_train.drop(columns=drop_cols)\n",
    "X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns])\n",
    "\n",
    "# ê²°ì¸¡ ì²˜ë¦¬\n",
    "X_train_feat = X_train_feat.fillna(0)\n",
    "X_test_feat = X_test_feat.fillna(0)\n",
    "\n",
    "# is_home ì»¬ëŸ¼ì„ ìˆ«ìë¡œ ë³€í™˜ (LightGBM í˜¸í™˜ì„±)\n",
    "for col in X_train_feat.columns:\n",
    "    if \"is_home\" in col and X_train_feat[col].dtype == \"object\":\n",
    "        X_train_feat[col] = pd.to_numeric(X_train_feat[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "        if col in X_test_feat.columns:\n",
    "            X_test_feat[col] = pd.to_numeric(X_test_feat[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "    \n",
    "    # period_id ì»¬ëŸ¼ë„ í™•ì¸\n",
    "    if \"period_id\" in col and X_train_feat[col].dtype == \"object\":\n",
    "        X_train_feat[col] = pd.to_numeric(X_train_feat[col], errors=\"coerce\").fillna(1).astype(int)\n",
    "        if col in X_test_feat.columns:\n",
    "            X_test_feat[col] = pd.to_numeric(X_test_feat[col], errors=\"coerce\").fillna(1).astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 9-1. í”¼ì²˜ ê³ ë„í™” (One-by-One ê²€ì¦)\n",
    "# ----------------------\n",
    "def add_advanced_features(X_train_feat, X_test_feat, X_train, X_test, events_df, K_val):\n",
    "    \"\"\"\n",
    "    í”¼ì²˜ ê³ ë„í™” ì‹¤í—˜ìš© í”¼ì²˜ ì¶”ê°€\n",
    "    ê° í”Œë˜ê·¸ì— ë”°ë¼ í•˜ë‚˜ì”©ë§Œ ì¶”ê°€\n",
    "    \"\"\"\n",
    "    X_train_feat = X_train_feat.copy()\n",
    "    X_test_feat = X_test_feat.copy()\n",
    "    \n",
    "    # 1ï¸âƒ£ Recency ê°€ì¤‘ í”¼ì²˜\n",
    "    if USE_RECENCY_WEIGHT:\n",
    "        print(\"  âœ“ Recency ê°€ì¤‘ í”¼ì²˜ ì¶”ê°€ ì¤‘...\")\n",
    "        # pd.concatì„ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì— ë³‘í•© (ì„±ëŠ¥ ìµœì í™”)\n",
    "        weighted_train_cols = {}\n",
    "        weighted_test_cols = {}\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ì˜ pos_in_K = K-1\n",
    "        for pos in range(K_val):\n",
    "            recency_weight = np.exp(-RECENCY_ALPHA * (K_val - pos))\n",
    "            \n",
    "            # dx, dy, speed, distì— ê°€ì¤‘ì¹˜ ì ìš©\n",
    "            for feat_base in [\"dx\", \"dy\", \"speed\", \"dist\"]:\n",
    "                col_name = f\"{feat_base}_{pos}\"\n",
    "                if col_name in X_train_feat.columns:\n",
    "                    weighted_train_cols[f\"{feat_base}_weighted_{pos}\"] = X_train_feat[col_name] * recency_weight\n",
    "                if col_name in X_test_feat.columns:\n",
    "                    weighted_test_cols[f\"{feat_base}_weighted_{pos}\"] = X_test_feat[col_name] * recency_weight\n",
    "        \n",
    "        # í•œ ë²ˆì— ë³‘í•©\n",
    "        if weighted_train_cols:\n",
    "            weighted_train_df = pd.DataFrame(weighted_train_cols, index=X_train_feat.index)\n",
    "            X_train_feat = pd.concat([X_train_feat, weighted_train_df], axis=1)\n",
    "        if weighted_test_cols:\n",
    "            weighted_test_df = pd.DataFrame(weighted_test_cols, index=X_test_feat.index)\n",
    "            X_test_feat = pd.concat([X_test_feat, weighted_test_df], axis=1)\n",
    "    \n",
    "    # 2ï¸âƒ£ ìµœê·¼ ì´ë™ ë°©í–¥ ìš”ì•½ í”¼ì²˜\n",
    "    if USE_DIRECTION_SUMMARY:\n",
    "        print(\"  âœ“ ìµœê·¼ ì´ë™ ë°©í–¥ ìš”ì•½ í”¼ì²˜ ì¶”ê°€ ì¤‘...\")\n",
    "        # ë§ˆì§€ë§‰ 3ê°œ, 5ê°œ ì´ë²¤íŠ¸ì˜ í‰ê· \n",
    "        for last_n in [3, 5]:\n",
    "            dx_cols = [f\"dx_{K_val-1-i}\" for i in range(last_n) if f\"dx_{K_val-1-i}\" in X_train_feat.columns]\n",
    "            dy_cols = [f\"dy_{K_val-1-i}\" for i in range(last_n) if f\"dy_{K_val-1-i}\" in X_train_feat.columns]\n",
    "            \n",
    "            if dx_cols:\n",
    "                X_train_feat[f\"mean_dx_last{last_n}\"] = X_train_feat[dx_cols].mean(axis=1)\n",
    "                X_test_feat[f\"mean_dx_last{last_n}\"] = X_test_feat[dx_cols].mean(axis=1) if all(c in X_test_feat.columns for c in dx_cols) else 0\n",
    "            \n",
    "            if dy_cols:\n",
    "                X_train_feat[f\"mean_dy_last{last_n}\"] = X_train_feat[dy_cols].mean(axis=1)\n",
    "                X_test_feat[f\"mean_dy_last{last_n}\"] = X_test_feat[dy_cols].mean(axis=1) if all(c in X_test_feat.columns for c in dy_cols) else 0\n",
    "        \n",
    "        # angle_mean_last5 (angle_to_goal í‰ê· )\n",
    "        angle_cols = [f\"angle_to_goal_{K_val-1-i}\" for i in range(5) if f\"angle_to_goal_{K_val-1-i}\" in X_train_feat.columns]\n",
    "        if angle_cols:\n",
    "            X_train_feat[\"angle_mean_last5\"] = X_train_feat[angle_cols].mean(axis=1)\n",
    "            X_test_feat[\"angle_mean_last5\"] = X_test_feat[angle_cols].mean(axis=1) if all(c in X_test_feat.columns for c in angle_cols) else 0\n",
    "    \n",
    "    # 3ï¸âƒ£ í•„ë“œ ê²½ê³„ ì¸ì‹ ê°•í™” í”¼ì²˜\n",
    "    if USE_FIELD_BOUNDARY:\n",
    "        print(\"  âœ“ í•„ë“œ ê²½ê³„ ì¸ì‹ ê°•í™” í”¼ì²˜ ì¶”ê°€ ì¤‘...\")\n",
    "        # ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ì˜ start_x, start_y ì‚¬ìš©\n",
    "        last_start_x_col = f\"start_x_{K_val-1}\"\n",
    "        last_start_y_col = f\"start_y_{K_val-1}\"\n",
    "        \n",
    "        if last_start_x_col in X_train_feat.columns:\n",
    "            X_train_feat[\"dist_to_goal_line\"] = 105 - X_train_feat[last_start_x_col]\n",
    "            X_test_feat[\"dist_to_goal_line\"] = 105 - X_test_feat[last_start_x_col] if last_start_x_col in X_test_feat.columns else 0\n",
    "            \n",
    "            X_train_feat[\"is_near_goal_line\"] = (X_train_feat[\"dist_to_goal_line\"] < FIELD_BOUNDARY_THRESHOLD).astype(int)\n",
    "            X_test_feat[\"is_near_goal_line\"] = (X_test_feat[\"dist_to_goal_line\"] < FIELD_BOUNDARY_THRESHOLD).astype(int)\n",
    "        \n",
    "        # dist_to_sidelineì€ ì´ë¯¸ ìˆì„ ìˆ˜ ìˆìŒ\n",
    "        last_dist_sideline_col = f\"dist_to_sideline_{K_val-1}\"\n",
    "        if last_dist_sideline_col in X_train_feat.columns:\n",
    "            X_train_feat[\"is_near_sideline\"] = (X_train_feat[last_dist_sideline_col] < FIELD_BOUNDARY_THRESHOLD).astype(int)\n",
    "            X_test_feat[\"is_near_sideline\"] = (X_test_feat[last_dist_sideline_col] < FIELD_BOUNDARY_THRESHOLD).astype(int) if last_dist_sideline_col in X_test_feat.columns else 0\n",
    "    \n",
    "    # 4ï¸âƒ£ ìµœê·¼ ì†ë„ ê¸°ë°˜ ì´ë™ í•œê³„ í”¼ì²˜\n",
    "    if USE_SPEED_CONSTRAINT:\n",
    "        print(\"  âœ“ ìµœê·¼ ì†ë„ ê¸°ë°˜ ì´ë™ í•œê³„ í”¼ì²˜ ì¶”ê°€ ì¤‘...\")\n",
    "        # ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ì˜ speed, dt\n",
    "        last_speed_col = f\"speed_{K_val-1}\"\n",
    "        last_dt_col = f\"dt_{K_val-1}\"\n",
    "        \n",
    "        if last_speed_col in X_train_feat.columns and last_dt_col in X_train_feat.columns:\n",
    "            X_train_feat[\"last_speed\"] = X_train_feat[last_speed_col]\n",
    "            X_train_feat[\"last_dt\"] = X_train_feat[last_dt_col]\n",
    "            X_train_feat[\"max_possible_move\"] = X_train_feat[\"last_speed\"] * X_train_feat[\"last_dt\"]\n",
    "            \n",
    "            if last_speed_col in X_test_feat.columns and last_dt_col in X_test_feat.columns:\n",
    "                X_test_feat[\"last_speed\"] = X_test_feat[last_speed_col]\n",
    "                X_test_feat[\"last_dt\"] = X_test_feat[last_dt_col]\n",
    "                X_test_feat[\"max_possible_move\"] = X_test_feat[\"last_speed\"] * X_test_feat[\"last_dt\"]\n",
    "    \n",
    "    # 5ï¸âƒ£ ì½”ë„ˆ/ì‚¬ì´ë“œ ìƒí™© ê°•ì¡° í”¼ì²˜\n",
    "    if USE_CORNER_CONTEXT:\n",
    "        print(\"  âœ“ ì½”ë„ˆ/ì‚¬ì´ë“œ ìƒí™© ê°•ì¡° í”¼ì²˜ ì¶”ê°€ ì¤‘...\")\n",
    "        # ì½”ë„ˆ ì˜ì—­ íŒë‹¨ (x > 100 and (y < 10 or y > 58))\n",
    "        last_start_x_col = f\"start_x_{K_val-1}\"\n",
    "        last_start_y_col = f\"start_y_{K_val-1}\"\n",
    "        \n",
    "        if last_start_x_col in X_train_feat.columns and last_start_y_col in X_train_feat.columns:\n",
    "            X_train_feat[\"is_corner_area\"] = (\n",
    "                (X_train_feat[last_start_x_col] > 100) & \n",
    "                ((X_train_feat[last_start_y_col] < 10) | (X_train_feat[last_start_y_col] > 58))\n",
    "            ).astype(int)\n",
    "            \n",
    "            if last_start_x_col in X_test_feat.columns and last_start_y_col in X_test_feat.columns:\n",
    "                X_test_feat[\"is_corner_area\"] = (\n",
    "                    (X_test_feat[last_start_x_col] > 100) & \n",
    "                    ((X_test_feat[last_start_y_col] < 10) | (X_test_feat[last_start_y_col] > 58))\n",
    "                ).astype(int)\n",
    "            \n",
    "            # ìƒí˜¸ì‘ìš© í”¼ì²˜\n",
    "            last_angle_col = f\"angle_to_goal_{K_val-1}\"\n",
    "            last_dist_sideline_col = f\"dist_to_sideline_{K_val-1}\"\n",
    "            \n",
    "            if last_angle_col in X_train_feat.columns:\n",
    "                X_train_feat[\"angle_to_goal_x_corner\"] = X_train_feat[last_angle_col] * X_train_feat[\"is_corner_area\"]\n",
    "                if last_angle_col in X_test_feat.columns:\n",
    "                    X_test_feat[\"angle_to_goal_x_corner\"] = X_test_feat[last_angle_col] * X_test_feat[\"is_corner_area\"]\n",
    "            \n",
    "            if last_dist_sideline_col in X_train_feat.columns:\n",
    "                X_train_feat[\"dist_to_sideline_x_corner\"] = X_train_feat[last_dist_sideline_col] * X_train_feat[\"is_corner_area\"]\n",
    "                if last_dist_sideline_col in X_test_feat.columns:\n",
    "                    X_test_feat[\"dist_to_sideline_x_corner\"] = X_test_feat[last_dist_sideline_col] * X_test_feat[\"is_corner_area\"]\n",
    "    \n",
    "    return X_train_feat, X_test_feat\n",
    "\n",
    "# ----------------------\n",
    "# 11. ìë™í™”ëœ í”¼ì²˜ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸\n",
    "# ----------------------\n",
    "def euclidean_mean(y_true_x, y_true_y, y_pred_x, y_pred_y):\n",
    "    dx = y_true_x - y_pred_x\n",
    "    dy = y_true_y - y_pred_y\n",
    "    return np.mean(np.sqrt(dx*dx + dy*dy))\n",
    "\n",
    "def run_single_experiment(exp_config, X_train_feat_base, X_test_feat_base, X_train, X_test, \n",
    "                         y_train_x, y_train_y, events, K_val):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ í”¼ì²˜ ì‹¤í—˜ ì‹¤í–‰ (ìƒì„¸ ì§„í–‰ ìƒí™© í‘œì‹œ í¬í•¨)\n",
    "    \"\"\"\n",
    "    exp_start_time = time.time()\n",
    "    \n",
    "    # í”Œë˜ê·¸ ì„¤ì •\n",
    "    global USE_RECENCY_WEIGHT, USE_DIRECTION_SUMMARY, USE_FIELD_BOUNDARY\n",
    "    global USE_SPEED_CONSTRAINT, USE_CORNER_CONTEXT\n",
    "    \n",
    "    USE_RECENCY_WEIGHT = exp_config[\"USE_RECENCY_WEIGHT\"]\n",
    "    USE_DIRECTION_SUMMARY = exp_config[\"USE_DIRECTION_SUMMARY\"]\n",
    "    USE_FIELD_BOUNDARY = exp_config[\"USE_FIELD_BOUNDARY\"]\n",
    "    USE_SPEED_CONSTRAINT = exp_config[\"USE_SPEED_CONSTRAINT\"]\n",
    "    USE_CORNER_CONTEXT = exp_config[\"USE_CORNER_CONTEXT\"]\n",
    "    \n",
    "    # í”¼ì²˜ ì¶”ê°€\n",
    "    print(\"  ğŸ“¦ í”¼ì²˜ ìƒì„± ì¤‘...\")\n",
    "    feat_start = time.time()\n",
    "    X_train_feat = X_train_feat_base.copy()\n",
    "    X_test_feat = X_test_feat_base.copy()\n",
    "    \n",
    "    active_features = []\n",
    "    if USE_RECENCY_WEIGHT:\n",
    "        active_features.append(\"Recency\")\n",
    "    if USE_DIRECTION_SUMMARY:\n",
    "        active_features.append(\"Direction\")\n",
    "    if USE_FIELD_BOUNDARY:\n",
    "        active_features.append(\"FieldBoundary\")\n",
    "    if USE_SPEED_CONSTRAINT:\n",
    "        active_features.append(\"SpeedConstraint\")\n",
    "    if USE_CORNER_CONTEXT:\n",
    "        active_features.append(\"CornerContext\")\n",
    "    \n",
    "    if active_features:\n",
    "        X_train_feat, X_test_feat = add_advanced_features(X_train_feat, X_test_feat, X_train, X_test, events, K_val)\n",
    "    feat_time = time.time() - feat_start\n",
    "    print(f\"  âœ“ í”¼ì²˜ ìƒì„± ì™„ë£Œ ({feat_time:.1f}ì´ˆ) | Train: {X_train_feat.shape[1]} features, Test: {X_test_feat.shape[1]} features\")\n",
    "    \n",
    "    # CV ì‹¤í–‰ (ë‹¨ì¼ ëª¨ë¸)\n",
    "    print(f\"  ğŸ”„ CV í•™ìŠµ ì‹œì‘ (n_splits={N_SPLITS})...\")\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    oof_pred_x = np.zeros(len(X_train_feat), dtype=float)\n",
    "    oof_pred_y = np.zeros(len(X_train_feat), dtype=float)\n",
    "    fold_scores = []\n",
    "    \n",
    "    LGB_PARAMS = LGB_PARAMS_BASE.copy()\n",
    "    LGB_PARAMS[\"random_state\"] = SEED\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train_feat), 1):\n",
    "        fold_start = time.time()\n",
    "        print(f\"    â””â”€ Fold {fold}/{N_SPLITS} í•™ìŠµ ì¤‘...\", end=\" \", flush=True)\n",
    "        \n",
    "        X_tr, X_va = X_train_feat.iloc[tr_idx], X_train_feat.iloc[va_idx]\n",
    "        y_tr_x, y_va_x = y_train_x[tr_idx], y_train_x[va_idx]\n",
    "        y_tr_y, y_va_y = y_train_y[tr_idx], y_train_y[va_idx]\n",
    "        \n",
    "        model_x = LGBMRegressor(**LGB_PARAMS)\n",
    "        model_x.fit(X_tr, y_tr_x, eval_set=[(X_va, y_va_x)], eval_metric=\"rmse\", callbacks=[])\n",
    "        \n",
    "        model_y = LGBMRegressor(**LGB_PARAMS)\n",
    "        model_y.fit(X_tr, y_tr_y, eval_set=[(X_va, y_va_y)], eval_metric=\"rmse\", callbacks=[])\n",
    "        \n",
    "        pred_va_x = np.clip(model_x.predict(X_va), 0, 105)\n",
    "        pred_va_y = np.clip(model_y.predict(X_va), 0, 68)\n",
    "        \n",
    "        oof_pred_x[va_idx] = pred_va_x\n",
    "        oof_pred_y[va_idx] = pred_va_y\n",
    "        \n",
    "        fold_euc = euclidean_mean(y_va_x, y_va_y, pred_va_x, pred_va_y)\n",
    "        fold_scores.append(fold_euc)\n",
    "        fold_time = time.time() - fold_start\n",
    "        print(f\"mean_dist = {fold_euc:.6f} ({fold_time:.1f}ì´ˆ)\")\n",
    "    \n",
    "    cv_mean = float(np.mean(fold_scores))\n",
    "    cv_std = float(np.std(fold_scores))\n",
    "    \n",
    "    # ìƒì„¸ ì§€í‘œ ê³„ì‚°\n",
    "    dist_all = np.sqrt((oof_pred_x - y_train_x)**2 + (oof_pred_y - y_train_y)**2)\n",
    "    p90 = np.percentile(dist_all, 90)\n",
    "    p95 = np.percentile(dist_all, 95)\n",
    "    pred_std_x = np.std(oof_pred_x)\n",
    "    pred_std_y = np.std(oof_pred_y)\n",
    "    \n",
    "    exp_time = time.time() - exp_start_time\n",
    "    \n",
    "    result = {\n",
    "        \"name\": exp_config[\"name\"],\n",
    "        \"features\": \", \".join(active_features) if active_features else \"ì—†ìŒ\",\n",
    "        \"cv_mean\": cv_mean,\n",
    "        \"cv_std\": cv_std,\n",
    "        \"p90\": p90,\n",
    "        \"p95\": p95,\n",
    "        \"pred_std_x\": pred_std_x,\n",
    "        \"pred_std_y\": pred_std_y,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"elapsed_time\": exp_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  âœ“ ì‹¤í—˜ ì™„ë£Œ (ì´ {exp_time:.1f}ì´ˆ)\")\n",
    "    return result\n",
    "\n",
    "# í”¼ì²˜ ì¶”ê°€ ì‹¤í–‰ (ê¸°ë³¸ í”¼ì²˜ë§Œ, ì‹¤í—˜ ì „ ì¤€ë¹„)\n",
    "X_train_feat_base = X_train_feat.copy()\n",
    "X_test_feat_base = X_test_feat.copy()\n",
    "\n",
    "# ìë™í™”ëœ ì‹¤í—˜ ì‹¤í–‰\n",
    "if RUN_FEATURE_EXPERIMENTS:\n",
    "    # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ\n",
    "    EXPERIMENT_RESULTS_FILE = \"feature_experiment_results.json\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸš€ ìë™í™”ëœ í”¼ì²˜ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ì´ {len(FEATURE_EXPERIMENTS)}ê°œ ì‹¤í—˜ ì‹¤í–‰ ì˜ˆì •\")\n",
    "    \n",
    "    # ì´ì „ ê²°ê³¼ ë¡œë“œ\n",
    "    saved_results = {}\n",
    "    if os.path.exists(EXPERIMENT_RESULTS_FILE):\n",
    "        try:\n",
    "            with open(EXPERIMENT_RESULTS_FILE, 'r', encoding='utf-8') as f:\n",
    "                saved_data = json.load(f)\n",
    "                saved_results = {exp['name']: exp for exp in saved_data.get('results', [])}\n",
    "            print(f\"ğŸ“‚ ì´ì „ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(saved_results)}ê°œ ì™„ë£Œëœ ì‹¤í—˜ ë°œê²¬\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  ì´ì „ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            saved_results = {}\n",
    "    \n",
    "    experiment_results = []\n",
    "    pipeline_start_time = time.time()\n",
    "    \n",
    "    for exp_idx, exp_config in enumerate(FEATURE_EXPERIMENTS, 1):\n",
    "        exp_name = exp_config['name']\n",
    "        \n",
    "        # ì´ë¯¸ ì™„ë£Œëœ ì‹¤í—˜ì¸ì§€ í™•ì¸\n",
    "        if exp_name in saved_results:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"â­ï¸  ì‹¤í—˜ {exp_idx}/{len(FEATURE_EXPERIMENTS)}: {exp_name} (ì´ë¯¸ ì™„ë£Œ - ìŠ¤í‚µ)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"  ğŸ“Š ì´ì „ ê²°ê³¼: mean_dist = {saved_results[exp_name]['cv_mean']:.6f} Â± {saved_results[exp_name]['cv_std']:.6f}\")\n",
    "            experiment_results.append(saved_results[exp_name])\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“Œ ì‹¤í—˜ {exp_idx}/{len(FEATURE_EXPERIMENTS)}: {exp_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if exp_name == 'baseline':\n",
    "            print(\"ğŸ“Š Baseline ëª¨ë“œ (ì¶”ê°€ í”¼ì²˜ ì—†ìŒ)\")\n",
    "        else:\n",
    "            print(f\"ğŸ”§ í™œì„±í™”ëœ í”¼ì²˜: {exp_name}\")\n",
    "        \n",
    "        # ê°œë³„ ì‹¤í—˜ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "        try:\n",
    "            result = run_single_experiment(\n",
    "                exp_config, X_train_feat_base, X_test_feat_base, \n",
    "                X_train, X_test, y_train_x, y_train_y, events, K\n",
    "            )\n",
    "            experiment_results.append(result)\n",
    "            \n",
    "            # ê²°ê³¼ ì¦‰ì‹œ ì €ì¥ (ì¤‘ê°„ ì €ì¥)\n",
    "            with open(EXPERIMENT_RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'results': experiment_results,\n",
    "                    'last_updated': datetime.now().isoformat()\n",
    "                }, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"  ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {EXPERIMENT_RESULTS_FILE}\")\n",
    "            print(f\"âœ… {exp_name} ì™„ë£Œ | mean_dist = {result['cv_mean']:.6f} Â± {result['cv_std']:.6f}\")\n",
    "            print(f\"   p90 = {result['p90']:.6f}, p95 = {result['p95']:.6f}\")\n",
    "            \n",
    "            # ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ ê³„ì‚°\n",
    "            if exp_idx < len(FEATURE_EXPERIMENTS):\n",
    "                elapsed = time.time() - pipeline_start_time\n",
    "                avg_time_per_exp = elapsed / exp_idx\n",
    "                remaining_exps = len(FEATURE_EXPERIMENTS) - exp_idx\n",
    "                estimated_remaining = avg_time_per_exp * remaining_exps\n",
    "                print(f\"  â±ï¸  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: ì•½ {estimated_remaining/60:.1f}ë¶„ ({remaining_exps}ê°œ ì‹¤í—˜ ë‚¨ìŒ)\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            # ì‚¬ìš©ìê°€ ì˜ë„ì ìœ¼ë¡œ ì¤‘ë‹¨í•œ ê²½ìš°\n",
    "            print(f\"\\n\\n{'='*80}\")\n",
    "            print(\"âš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì‹¤í—˜ ì¤‘ë‹¨ë¨ (KeyboardInterrupt)\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # ì™„ë£Œëœ ì‹¤í—˜ ê²°ê³¼ ì €ì¥\n",
    "            if experiment_results:\n",
    "                with open(EXPERIMENT_RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\n",
    "                        'results': experiment_results,\n",
    "                        'last_updated': datetime.now().isoformat(),\n",
    "                        'interrupted': True,\n",
    "                        'interrupted_at': exp_name\n",
    "                    }, f, indent=2, ensure_ascii=False)\n",
    "                print(f\"ğŸ’¾ ì™„ë£Œëœ {len(experiment_results)}ê°œ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {EXPERIMENT_RESULTS_FILE}\")\n",
    "                print(f\"ğŸ“Œ ì¤‘ë‹¨ëœ ì‹¤í—˜: {exp_name}\")\n",
    "                print(f\"ğŸ’¡ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì™„ë£Œëœ ì‹¤í—˜ì€ ìŠ¤í‚µë˜ê³  ì¤‘ë‹¨ëœ ì‹¤í—˜ë¶€í„° ì¬ê°œë©ë‹ˆë‹¤.\")\n",
    "            else:\n",
    "                print(\"âš ï¸  ì™„ë£Œëœ ì‹¤í—˜ì´ ì—†ì–´ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "            print(\"=\" * 80)\n",
    "            raise  # KeyboardInterruptë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ì •ìƒ ì¢…ë£Œ\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ì‹¤í—˜ ì‹¤íŒ¨: {exp_name}\")\n",
    "            print(f\"  ì˜¤ë¥˜ ë‚´ìš©: {str(e)}\")\n",
    "            print(f\"  âš ï¸  ì´ ì‹¤í—˜ì€ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ì‹¤í—˜ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # ì™„ë£Œëœ ì‹¤í—˜ ê²°ê³¼ëŠ” ì €ì¥ (ì˜¤ë¥˜ê°€ ë°œìƒí•œ ì‹¤í—˜ ì œì™¸)\n",
    "            if experiment_results:\n",
    "                with open(EXPERIMENT_RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\n",
    "                        'results': experiment_results,\n",
    "                        'last_updated': datetime.now().isoformat(),\n",
    "                        'failed_experiment': exp_name\n",
    "                    }, f, indent=2, ensure_ascii=False)\n",
    "            continue\n",
    "    \n",
    "    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œê°„\n",
    "    total_time = time.time() - pipeline_start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"â±ï¸  ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {total_time/60:.1f}ë¶„ ({total_time:.1f}ì´ˆ)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµ í…Œì´ë¸” ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š í”¼ì²˜ ì‹¤í—˜ ê²°ê³¼ ë¹„êµ\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'ì‹¤í—˜ëª…':<20} {'ì¶”ê°€ í”¼ì²˜':<20} {'mean_dist':<15} {'Î”mean':<12} {'p90':<12} {'p95':<12} {'pred_std_x':<12} {'pred_std_y':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if len(experiment_results) > 0:\n",
    "        baseline_mean = experiment_results[0]['cv_mean']\n",
    "        \n",
    "        for result in experiment_results:\n",
    "            delta_mean = result['cv_mean'] - baseline_mean\n",
    "            delta_str = f\"{delta_mean:+.6f}\" if result['name'] != 'baseline' else \"-\"\n",
    "            \n",
    "            print(f\"{result['name']:<20} {result['features']:<20} {result['cv_mean']:<15.6f} {delta_str:<12} {result['p90']:<12.6f} {result['p95']:<12.6f} {result['pred_std_x']:<12.6f} {result['pred_std_y']:<12.6f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"ğŸ’¡ ì°¸ê³ :\")\n",
    "        print(\"   - Î”mean: baseline ëŒ€ë¹„ ë³€í™”ëŸ‰ (ìŒìˆ˜ = ê°œì„ , ì–‘ìˆ˜ = ì•…í™”)\")\n",
    "        print(\"   - Î”mean_distê°€ -0.03 ì´í•˜ì´ë©´ ì˜ë¯¸ ìˆëŠ” ê°œì„ ìœ¼ë¡œ ë´…ë‹ˆë‹¤.\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ í”¼ì²˜ ì°¾ê¸°\n",
    "        best_exp = min(experiment_results, key=lambda x: x['cv_mean'])\n",
    "        print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥: {best_exp['name']} (mean_dist = {best_exp['cv_mean']:.6f})\")\n",
    "        if best_exp['name'] != 'baseline':\n",
    "            improvement = baseline_mean - best_exp['cv_mean']\n",
    "            print(f\"   Baseline ëŒ€ë¹„ ê°œì„ : {improvement:.6f} ({improvement/baseline_mean*100:.2f}%)\")\n",
    "        \n",
    "        # ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "        with open(EXPERIMENT_RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'results': experiment_results,\n",
    "                'last_updated': datetime.now().isoformat(),\n",
    "                'total_time_seconds': total_time\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {EXPERIMENT_RESULTS_FILE}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  ì™„ë£Œëœ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "else:\n",
    "    # ìˆ˜ë™ ëª¨ë“œ: ê¸°ì¡´ ë¡œì§ ì‚¬ìš©\n",
    "    active_features = []\n",
    "    if USE_RECENCY_WEIGHT:\n",
    "        active_features.append(\"Recency\")\n",
    "    if USE_DIRECTION_SUMMARY:\n",
    "        active_features.append(\"Direction\")\n",
    "    if USE_FIELD_BOUNDARY:\n",
    "        active_features.append(\"FieldBoundary\")\n",
    "    if USE_SPEED_CONSTRAINT:\n",
    "        active_features.append(\"SpeedConstraint\")\n",
    "    if USE_CORNER_CONTEXT:\n",
    "        active_features.append(\"CornerContext\")\n",
    "    \n",
    "    if active_features:\n",
    "        print(f\"\\nğŸ”§ í”¼ì²˜ ê³ ë„í™” í™œì„±í™”: {', '.join(active_features)}\")\n",
    "        X_train_feat, X_test_feat = add_advanced_features(X_train_feat, X_test_feat, X_train, X_test, events, K)\n",
    "    else:\n",
    "        print(\"\\nğŸ“Š Baseline ëª¨ë“œ (ì¶”ê°€ í”¼ì²˜ ì—†ìŒ)\")\n",
    "\n",
    "# ----------------------\n",
    "# 11-1. (ì˜µì…˜) CVë¡œ OOF ì ìˆ˜ í™•ì¸ (ìˆ˜ë™ ëª¨ë“œìš©)\n",
    "# ----------------------\n",
    "if USE_CV and not RUN_FEATURE_EXPERIMENTS:\n",
    "    if USE_ENSEMBLE:\n",
    "        # OOF ì•™ìƒë¸” CV\n",
    "        kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        ensemble_results = []\n",
    "        oof_pred_x_ensemble = np.zeros(len(X_train_feat), dtype=float)\n",
    "        oof_pred_y_ensemble = np.zeros(len(X_train_feat), dtype=float)\n",
    "        \n",
    "        model_idx = 0\n",
    "        for seed in ENSEMBLE_SEEDS:\n",
    "            for num_leaves in ENSEMBLE_NUM_LEAVES:\n",
    "                for min_child_samples in ENSEMBLE_MIN_CHILD_SAMPLES:\n",
    "                    model_idx += 1\n",
    "                    LGB_PARAMS = LGB_PARAMS_BASE.copy()\n",
    "                    LGB_PARAMS[\"random_state\"] = seed\n",
    "                    LGB_PARAMS[\"num_leaves\"] = num_leaves\n",
    "                    LGB_PARAMS[\"min_child_samples\"] = min_child_samples\n",
    "                    \n",
    "                    oof_pred_x_local = np.zeros(len(X_train_feat), dtype=float)\n",
    "                    oof_pred_y_local = np.zeros(len(X_train_feat), dtype=float)\n",
    "                    fold_scores = []\n",
    "                    \n",
    "                    print(f\"\\n{'='*80}\")\n",
    "                    print(f\"ğŸ“Œ ì•™ìƒë¸” ëª¨ë¸ {model_idx}: seed={seed}, num_leaves={num_leaves}, min_child_samples={min_child_samples}\")\n",
    "                    print(f\"{'='*80}\")\n",
    "                    \n",
    "                    for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train_feat), 1):\n",
    "                        X_tr, X_va = X_train_feat.iloc[tr_idx], X_train_feat.iloc[va_idx]\n",
    "                        y_tr_x, y_va_x = y_train_x[tr_idx], y_train_x[va_idx]\n",
    "                        y_tr_y, y_va_y = y_train_y[tr_idx], y_train_y[va_idx]\n",
    "                        \n",
    "                        model_x = LGBMRegressor(**LGB_PARAMS)\n",
    "                        model_x.fit(X_tr, y_tr_x, eval_set=[(X_va, y_va_x)], eval_metric=\"rmse\", callbacks=[])\n",
    "                        \n",
    "                        model_y = LGBMRegressor(**LGB_PARAMS)\n",
    "                        model_y.fit(X_tr, y_tr_y, eval_set=[(X_va, y_va_y)], eval_metric=\"rmse\", callbacks=[])\n",
    "                        \n",
    "                        pred_va_x = np.clip(model_x.predict(X_va), 0, 105)\n",
    "                        pred_va_y = np.clip(model_y.predict(X_va), 0, 68)\n",
    "                        \n",
    "                        oof_pred_x_local[va_idx] = pred_va_x\n",
    "                        oof_pred_y_local[va_idx] = pred_va_y\n",
    "                        \n",
    "                        fold_euc = euclidean_mean(y_va_x, y_va_y, pred_va_x, pred_va_y)\n",
    "                        fold_scores.append(fold_euc)\n",
    "                        print(f\"[Fold {fold}] mean Euclidean = {fold_euc:.6f}\")\n",
    "                    \n",
    "                    cv_mean = float(np.mean(fold_scores))\n",
    "                    cv_std = float(np.std(fold_scores))\n",
    "                    \n",
    "                    ensemble_results.append({\n",
    "                        \"seed\": seed,\n",
    "                        \"num_leaves\": num_leaves,\n",
    "                        \"min_child_samples\": min_child_samples,\n",
    "                        \"cv_mean\": cv_mean,\n",
    "                        \"cv_std\": cv_std\n",
    "                    })\n",
    "                    \n",
    "                    oof_pred_x_ensemble += oof_pred_x_local\n",
    "                    oof_pred_y_ensemble += oof_pred_y_local\n",
    "                    \n",
    "                    print(f\"âœ… ëª¨ë¸ {model_idx} CV ì™„ë£Œ | mean Euclidean = {cv_mean:.6f} Â± {cv_std:.6f}\")\n",
    "        \n",
    "        # ì•™ìƒë¸” í‰ê· \n",
    "        oof_pred_x_ensemble /= len(ensemble_results)\n",
    "        oof_pred_y_ensemble /= len(ensemble_results)\n",
    "        \n",
    "        # ì•™ìƒë¸” OOF ì ìˆ˜ ê³„ì‚° (ìƒì„¸ ì§€í‘œ)\n",
    "        ensemble_cv_mean = euclidean_mean(y_train_x, y_train_y, oof_pred_x_ensemble, oof_pred_y_ensemble)\n",
    "        \n",
    "        # ìƒì„¸ ì§€í‘œ ê³„ì‚°\n",
    "        dist_ensemble = np.sqrt((oof_pred_x_ensemble - y_train_x)**2 + (oof_pred_y_ensemble - y_train_y)**2)\n",
    "        p90_ensemble = np.percentile(dist_ensemble, 90)\n",
    "        p95_ensemble = np.percentile(dist_ensemble, 95)\n",
    "        pred_std_x_ensemble = np.std(oof_pred_x_ensemble)\n",
    "        pred_std_y_ensemble = np.std(oof_pred_y_ensemble)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ğŸ“Š ì•™ìƒë¸” ê²°ê³¼ ìš”ì•½\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"{'Model':<5} {'Seed':<8} {'Num_Leaves':<12} {'Min_Child':<12} {'CV_Mean':<15} {'CV_Std':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, r in enumerate(ensemble_results, 1):\n",
    "            print(f\"{i:<5} {r['seed']:<8} {r['num_leaves']:<12} {r['min_child_samples']:<12} {r['cv_mean']:<15.6f} {r['cv_std']:<12.6f}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"ğŸ¯ ì•™ìƒë¸” OOF ì„±ëŠ¥:\")\n",
    "        print(f\"   - mean_dist: {ensemble_cv_mean:.6f}\")\n",
    "        print(f\"   - p90: {p90_ensemble:.6f}\")\n",
    "        print(f\"   - p95: {p95_ensemble:.6f}\")\n",
    "        print(f\"   - pred_std_x: {pred_std_x_ensemble:.6f}, pred_std_y: {pred_std_y_ensemble:.6f}\")\n",
    "        \n",
    "        # í™œì„±í™”ëœ í”¼ì²˜ ì •ë³´ ì¶œë ¥\n",
    "        if active_features:\n",
    "            print(f\"\\nğŸ“Œ í™œì„±í™”ëœ í”¼ì²˜: {', '.join(active_features)}\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ“Œ Baseline ëª¨ë“œ\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # ìµœì¢… í•™ìŠµ (ì „ì²´ train) - ì•™ìƒë¸”\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ“Œ ì „ì²´ Trainìœ¼ë¡œ ìµœì¢… ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ í›„ Test ì˜ˆì¸¡ ì‹œì‘...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        pred_x_ensemble = np.zeros(len(X_test_feat), dtype=float)\n",
    "        pred_y_ensemble = np.zeros(len(X_test_feat), dtype=float)\n",
    "        \n",
    "        for seed in ENSEMBLE_SEEDS:\n",
    "            for num_leaves in ENSEMBLE_NUM_LEAVES:\n",
    "                for min_child_samples in ENSEMBLE_MIN_CHILD_SAMPLES:\n",
    "                    LGB_PARAMS = LGB_PARAMS_BASE.copy()\n",
    "                    LGB_PARAMS[\"random_state\"] = seed\n",
    "                    LGB_PARAMS[\"num_leaves\"] = num_leaves\n",
    "                    LGB_PARAMS[\"min_child_samples\"] = min_child_samples\n",
    "                    \n",
    "                    final_model_x = LGBMRegressor(**LGB_PARAMS)\n",
    "                    final_model_x.fit(X_train_feat, y_train_x)\n",
    "                    \n",
    "                    final_model_y = LGBMRegressor(**LGB_PARAMS)\n",
    "                    final_model_y.fit(X_train_feat, y_train_y)\n",
    "                    \n",
    "                    pred_x_ensemble += np.clip(final_model_x.predict(X_test_feat), 0, 105)\n",
    "                    pred_y_ensemble += np.clip(final_model_y.predict(X_test_feat), 0, 68)\n",
    "        \n",
    "        pred_x = pred_x_ensemble / len(ensemble_results)\n",
    "        pred_y = pred_y_ensemble / len(ensemble_results)\n",
    "        \n",
    "    else:\n",
    "        # ë‹¨ì¼ ëª¨ë¸ CV\n",
    "        kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        oof_pred_x = np.zeros(len(X_train_feat), dtype=float)\n",
    "        oof_pred_y = np.zeros(len(X_train_feat), dtype=float)\n",
    "        fold_scores = []\n",
    "        \n",
    "        LGB_PARAMS = LGB_PARAMS_BASE.copy()\n",
    "        LGB_PARAMS[\"random_state\"] = SEED\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ğŸ“Œ LGBM CV ì‹œì‘: n_splits={N_SPLITS}, K={K}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train_feat), 1):\n",
    "            X_tr, X_va = X_train_feat.iloc[tr_idx], X_train_feat.iloc[va_idx]\n",
    "            y_tr_x, y_va_x = y_train_x[tr_idx], y_train_x[va_idx]\n",
    "            y_tr_y, y_va_y = y_train_y[tr_idx], y_train_y[va_idx]\n",
    "            \n",
    "            model_x = LGBMRegressor(**LGB_PARAMS)\n",
    "            model_x.fit(X_tr, y_tr_x, eval_set=[(X_va, y_va_x)], eval_metric=\"rmse\", callbacks=[])\n",
    "            \n",
    "            model_y = LGBMRegressor(**LGB_PARAMS)\n",
    "            model_y.fit(X_tr, y_tr_y, eval_set=[(X_va, y_va_y)], eval_metric=\"rmse\", callbacks=[])\n",
    "            \n",
    "            pred_va_x = np.clip(model_x.predict(X_va), 0, 105)\n",
    "            pred_va_y = np.clip(model_y.predict(X_va), 0, 68)\n",
    "            \n",
    "            oof_pred_x[va_idx] = pred_va_x\n",
    "            oof_pred_y[va_idx] = pred_va_y\n",
    "            \n",
    "            fold_euc = euclidean_mean(y_va_x, y_va_y, pred_va_x, pred_va_y)\n",
    "            fold_scores.append(fold_euc)\n",
    "            print(f\"[Fold {fold}] mean Euclidean = {fold_euc:.6f}\")\n",
    "        \n",
    "        cv_mean = float(np.mean(fold_scores))\n",
    "        cv_std = float(np.std(fold_scores))\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"âœ… CV ì™„ë£Œ | mean Euclidean = {cv_mean:.6f} Â± {cv_std:.6f}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # ìµœì¢… í•™ìŠµ (ì „ì²´ train) - ë‹¨ì¼ ëª¨ë¸\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ“Œ ì „ì²´ Trainìœ¼ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ í›„ Test ì˜ˆì¸¡ ì‹œì‘...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        final_model_x = LGBMRegressor(**LGB_PARAMS)\n",
    "        final_model_x.fit(X_train_feat, y_train_x)\n",
    "        \n",
    "        final_model_y = LGBMRegressor(**LGB_PARAMS)\n",
    "        final_model_y.fit(X_train_feat, y_train_y)\n",
    "        \n",
    "        pred_x = np.clip(final_model_x.predict(X_test_feat), 0, 105)\n",
    "        pred_y = np.clip(final_model_y.predict(X_test_feat), 0, 68)\n",
    "    \n",
    "    # ìë™ ì‹¤í—˜ ëª¨ë“œì—ì„œëŠ” ì œì¶œ íŒŒì¼ ìƒì„± ì•ˆ í•¨ (ë¹„êµë§Œ)\n",
    "    if RUN_FEATURE_EXPERIMENTS:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… ìë™ ì‹¤í—˜ ì™„ë£Œ - ì œì¶œ íŒŒì¼ì€ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ğŸ’¡ ìµœê³  ì„±ëŠ¥ í”¼ì²˜ë¥¼ í™•ì¸í•œ í›„, ìˆ˜ë™ìœ¼ë¡œ í”Œë˜ê·¸ë¥¼ ì„¤ì •í•˜ê³ \")\n",
    "        print(\"   RUN_FEATURE_EXPERIMENTS = Falseë¡œ ë³€ê²½í•˜ì—¬ ì œì¶œ íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        # ìˆ˜ë™ ëª¨ë“œ: ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "        # ----------------------\n",
    "        # 12. submission ìƒì„±\n",
    "        # ----------------------\n",
    "        # ì œì¶œ íŒŒì¼ëª… ìƒì„± (í”¼ì²˜ ì •ë³´ í¬í•¨)\n",
    "        feature_suffix = \"\"\n",
    "        if active_features:\n",
    "            feature_suffix = \"_\" + \"_\".join(active_features).lower()\n",
    "        \n",
    "        if USE_ENSEMBLE:\n",
    "            submission_filename = f\"submission_lgbm_ensemble_k{K}{feature_suffix}.csv\"\n",
    "        else:\n",
    "            submission_filename = f\"submission_lgbm_direct_k{K}{feature_suffix}.csv\"\n",
    "        \n",
    "        sub = sample_sub.copy()\n",
    "        pred_df = X_test[[\"game_episode\"]].copy()\n",
    "        pred_df[\"end_x\"] = pred_x\n",
    "        pred_df[\"end_y\"] = pred_y\n",
    "        \n",
    "        sub = sub.drop(columns=[\"end_x\", \"end_y\"], errors=\"ignore\")\n",
    "        sub = sub.merge(pred_df, on=\"game_episode\", how=\"left\")\n",
    "        sub.to_csv(submission_filename, index=False)\n",
    "        \n",
    "        print(f\"âœ… Saved: {submission_filename}\")\n",
    "        print(\"ğŸ‰ ì™„ë£Œ!\")\n",
    "\n",
    "# ----------------------\n",
    "# 13. ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (í”¼ì²˜ ê³ ë„í™” ì‹¤í—˜ìš©)\n",
    "# ----------------------\n",
    "if USE_CV and USE_ENSEMBLE:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (í”¼ì²˜ ê³ ë„í™” ì‹¤í—˜)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'ì‹¤í—˜ëª…':<20} {'ì¶”ê°€ í”¼ì²˜':<20} {'mean_dist':<15} {'p90':<12} {'p95':<12} {'pred_std_x':<12} {'pred_std_y':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    exp_name = \"baseline\" if not active_features else \"_\".join(active_features).lower()\n",
    "    features_str = \"ì—†ìŒ\" if not active_features else \", \".join(active_features)\n",
    "    \n",
    "    print(f\"{exp_name:<20} {features_str:<20} {ensemble_cv_mean:<15.6f} {p90_ensemble:<12.6f} {p95_ensemble:<12.6f} {pred_std_x_ensemble:<12.6f} {pred_std_y_ensemble:<12.6f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"ğŸ’¡ ì°¸ê³ : Î”mean_distê°€ 0.03 ì´ìƒì´ë©´ ì˜ë¯¸ ìˆëŠ” ê°œì„ ìœ¼ë¡œ ë´…ë‹ˆë‹¤.\")\n",
    "    print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d5a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281049fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
