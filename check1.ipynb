{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8477ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/2800053034.py:162: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/2800053034.py:242: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train_feat.fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/2800053034.py:243: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test_feat.fillna(0)\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models_x\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.57 GB / 16.00 GB (22.3%)\n",
      "Disk Space Avail:   15.38 GB / 460.43 GB (3.3%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 450s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 378\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "X 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3687.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 46.35 MB (1.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 334 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 312 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('int', ['bool']) :  42 | ['ep_idx_norm_19', 'is_final_team_0', 'is_final_team_1', 'is_final_team_2', 'is_final_team_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t354 features in original data used to generate 354 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.21 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 299.65s of the 449.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.77%)\n",
      "\t-11.9893\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.77s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 288.98s of the 438.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.35%)\n",
      "\t-11.9344\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.66s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 279.28s of the 429.22s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/3.2 GB\n",
      "\t-12.1327\t = Validation score   (-root_mean_squared_error)\n",
      "\t90.09s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 187.61s of the 337.55s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.81% memory usage per fold, 49.61%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=24.81%)\n",
      "\t-11.878\t = Validation score   (-root_mean_squared_error)\n",
      "\t68.2s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 117.70s of the 267.63s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/3.5 GB\n",
      "\t-12.3074\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.82s\t = Training   runtime\n",
      "\t0.97s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 86.23s of the 236.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.73%)\n",
      "\t-13.451\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.0s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 69.16s of the 219.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.21%)\n",
      "\t-12.0558\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.79s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 37.65s of the 187.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.40%)\n",
      "\t-13.328\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.2s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 17.71s of the 167.64s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.51% memory usage per fold, 42.02%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.51%)\n",
      "\t-11.9881\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.09s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 149.49s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.9 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.48, 'LightGBM_BAG_L1': 0.16, 'LightGBMLarge_BAG_L1': 0.16, 'RandomForestMSE_BAG_L1': 0.12, 'LightGBMXT_BAG_L1': 0.04, 'NeuralNetFastAI_BAG_L1': 0.04}\n",
      "\t-11.7971\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 149.46s of the 149.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.23%)\n",
      "\t-11.8966\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.99s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 141.40s of the 141.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.03%)\n",
      "\t-11.8901\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.42s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 132.51s of the 132.48s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/5.2 GB\n",
      "\t-11.9548\t = Validation score   (-root_mean_squared_error)\n",
      "\t102.84s\t = Training   runtime\n",
      "\t1.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 27.75s of the 27.73s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.40% memory usage per fold, 40.81%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.40%)\n",
      "\t-11.8564\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.39s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -0.82s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.5 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.4, 'LightGBM_BAG_L1': 0.12, 'LightGBMLarge_BAG_L1': 0.12, 'RandomForestMSE_BAG_L2': 0.12, 'RandomForestMSE_BAG_L1': 0.08, 'LightGBM_BAG_L2': 0.08, 'NeuralNetFastAI_BAG_L1': 0.04, 'CatBoost_BAG_L2': 0.04}\n",
      "\t-11.7917\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 450.9s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 732.8 rows/s (1715 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3     -11.680003 -11.791664  root_mean_squared_error        2.470022       5.050624  416.277319                 0.002438                0.000343           0.023526            3       True         15\n",
      "1      WeightedEnsemble_L2     -11.696792 -11.797104  root_mean_squared_error        1.243339       2.077940  203.820190                 0.003804                0.000320           0.011870            2       True         10\n",
      "2          LightGBM_BAG_L2     -11.709811 -11.890116  root_mean_squared_error        2.135355       3.822580  287.030462                 0.041491                0.140016           7.420421            2       True         12\n",
      "3          CatBoost_BAG_L2     -11.729555 -11.856441  root_mean_squared_error        2.138144       3.719572  305.996838                 0.044280                0.037007          26.386797            2       True         14\n",
      "4        LightGBMXT_BAG_L2     -11.730750 -11.896564  root_mean_squared_error        2.139985       3.817292  285.604647                 0.046121                0.134727           5.994606            2       True         11\n",
      "5          CatBoost_BAG_L1     -11.747484 -11.878006  root_mean_squared_error        0.063835       0.046991   68.200542                 0.063835                0.046991          68.200542            1       True          4\n",
      "6   RandomForestMSE_BAG_L2     -11.765211 -11.954783  root_mean_squared_error        2.381813       4.873258  382.446574                 0.287949                1.190694         102.836533            2       True         13\n",
      "7          LightGBM_BAG_L1     -11.765724 -11.934395  root_mean_squared_error        0.054067       0.224480    7.663696                 0.054067                0.224480           7.663696            1       True          2\n",
      "8           XGBoost_BAG_L1     -11.786010 -12.055756  root_mean_squared_error        0.177852       0.191482   28.786281                 0.177852                0.191482          28.786281            1       True          7\n",
      "9     LightGBMLarge_BAG_L1     -11.806776 -11.988126  root_mean_squared_error        0.097514       0.183909   16.090940                 0.097514                0.183909          16.090940            1       True          9\n",
      "10       LightGBMXT_BAG_L1     -11.836098 -11.989264  root_mean_squared_error        0.248088       0.313864    7.767693                 0.248088                0.313864           7.767693            1       True          1\n",
      "11  RandomForestMSE_BAG_L1     -12.068783 -12.132683  root_mean_squared_error        0.276251       0.933458   90.085473                 0.276251                0.933458          90.085473            1       True          3\n",
      "12    ExtraTreesMSE_BAG_L1     -12.312837 -12.307433  root_mean_squared_error        0.274028       0.972795   29.818374                 0.274028                0.972795          29.818374            1       True          5\n",
      "13  NeuralNetFastAI_BAG_L1     -12.499513 -13.451036  root_mean_squared_error        0.499779       0.374918   13.999976                 0.499779                0.374918          13.999976            1       True          6\n",
      "14   NeuralNetTorch_BAG_L1     -12.853147 -13.327962  root_mean_squared_error        0.402449       0.440668   17.197066                 0.402449                0.440668          17.197066            1       True          8\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t454s\t = DyStack   runtime |\t1346s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1346s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 378\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3914.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.14 MB (1.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 333 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 312 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('int', ['bool']) :  41 | ['ep_idx_norm_19', 'is_final_team_0', 'is_final_team_1', 'is_final_team_2', 'is_final_team_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t353 features in original data used to generate 353 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.34 MB (1.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 896.55s of the 1345.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.31%)\n",
      "\t-11.9496\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.07s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 884.64s of the 1333.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.44%)\n",
      "\t-11.8744\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.65s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 873.04s of the 1321.65s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/3.8 GB\n",
      "\t-12.0827\t = Validation score   (-root_mean_squared_error)\n",
      "\t104.85s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 766.33s of the 1214.94s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.09% memory usage per fold, 44.17%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=22.09%)\n",
      "\t-11.8548\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.47s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 708.84s of the 1157.45s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/4.3 GB\n",
      "\t-12.2553\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.59s\t = Training   runtime\n",
      "\t1.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 669.89s of the 1118.50s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.90% memory usage per fold, 43.62%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.90%)\n",
      "\t-13.3165\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.95s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 641.66s of the 1090.27s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.22% memory usage per fold, 44.87%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.22%)\n",
      "\t-11.9692\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.53s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 612.97s of the 1061.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.04%)\n",
      "\t-13.2153\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.26s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 595.74s of the 1044.35s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.46% memory usage per fold, 53.86%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=13.46%)\n",
      "\t-11.8945\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.4s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 541.15s of the 989.76s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.63% memory usage per fold, 45.27%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=22.63%)\n",
      "\t-11.851\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.87s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 495.35s of the 943.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.40%)\n",
      "\t-12.8689\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.39s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 465.59s of the 914.20s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.23% memory usage per fold, 40.91%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.23%)\n",
      "\t-11.8153\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.8s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 424.49s of the 873.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.96%)\n",
      "\t-13.3431\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.98s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 388.97s of the 837.58s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.06% memory usage per fold, 56.12%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=28.06%)\n",
      "\t-11.9028\t = Validation score   (-root_mean_squared_error)\n",
      "\t316.36s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 70.23s of the 518.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.12%)\n",
      "\t-11.9924\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.52s\t = Training   runtime\n",
      "\t1.48s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 10.68s of the 459.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.80%)\n",
      "\t-13.1504\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.53s\t = Training   runtime\n",
      "\t0.71s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 446.47s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.3 GB\n",
      "\tEnsemble Weights: {'CatBoost_r177_BAG_L1': 0.32, 'LightGBMLarge_BAG_L1': 0.24, 'CatBoost_BAG_L1': 0.12, 'LightGBM_r131_BAG_L1': 0.12, 'LightGBM_BAG_L1': 0.08, 'RandomForestMSE_BAG_L1': 0.08, 'LightGBMXT_BAG_L1': 0.04}\n",
      "\t-11.7429\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 446.43s of the 446.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.78%)\n",
      "\t-11.8097\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.11s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 438.88s of the 438.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.45%)\n",
      "\t-11.8422\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.06s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 430.74s of the 430.69s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/4.5 GB\n",
      "\t-11.8862\t = Validation score   (-root_mean_squared_error)\n",
      "\t115.28s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 313.42s of the 313.37s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.55% memory usage per fold, 45.10%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=22.55%)\n",
      "\t-11.793\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.76s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 277.14s of the 277.09s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/4.0 GB\n",
      "\t-11.8369\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.58s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 237.59s of the 237.54s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.31% memory usage per fold, 41.26%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.31%)\n",
      "\t-12.4151\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.63s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 211.76s of the 211.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.89%)\n",
      "\t-11.8962\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.84s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 188.92s of the 188.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.35%)\n",
      "\t-12.1542\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.58s\t = Training   runtime\n",
      "\t0.94s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 173.00s of the 172.96s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.88% memory usage per fold, 47.52%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.88%)\n",
      "\t-11.8979\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.88s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 135.70s of the 135.65s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.32% memory usage per fold, 42.64%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=21.32%)\n",
      "\t-11.8013\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.19s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 104.58s of the 104.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.46%)\n",
      "\t-11.9895\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.8s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 84.55s of the 84.50s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.62%)\n",
      "\t-11.8437\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.03s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 63.62s of the 63.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.46%)\n",
      "\t-12.4626\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.23s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 34.86s of the 34.81s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.52% memory usage per fold, 53.05%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=26.52%)\n",
      "\t-13.3149\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.08s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1.66s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.2 GB\n",
      "\tEnsemble Weights: {'CatBoost_r177_BAG_L1': 0.24, 'LightGBM_r131_BAG_L1': 0.16, 'ExtraTreesMSE_BAG_L2': 0.16, 'RandomForestMSE_BAG_L1': 0.08, 'CatBoost_BAG_L1': 0.08, 'LightGBMXT_BAG_L2': 0.08, 'RandomForestMSE_BAG_L2': 0.08, 'LightGBM_BAG_L1': 0.04, 'CatBoost_BAG_L2': 0.04, 'NeuralNetTorch_r79_BAG_L2': 0.04}\n",
      "\t-11.7384\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1344.04s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 258.4 rows/s (1930 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       4.19 GB / 16.00 GB (26.2%)\n",
      "Disk Space Avail:   11.28 GB / 460.43 GB (2.4%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 450s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 378\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4340.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 46.35 MB (1.1% of available memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Y 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 334 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 312 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('int', ['bool']) :  42 | ['ep_idx_norm_19', 'is_final_team_0', 'is_final_team_1', 'is_final_team_2', 'is_final_team_3', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t354 features in original data used to generate 354 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.21 MB (0.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 299.68s of the 449.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.06%)\n",
      "\t-13.1606\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.62s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 289.67s of the 439.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.62%)\n",
      "\t-13.0851\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.23s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 279.45s of the 429.40s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/5.1 GB\n",
      "\t-13.2968\t = Validation score   (-root_mean_squared_error)\n",
      "\t95.91s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 181.88s of the 331.83s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.73% memory usage per fold, 41.46%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.73%)\n",
      "\t-13.1339\t = Validation score   (-root_mean_squared_error)\n",
      "\t64.21s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 116.16s of the 266.11s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/4.4 GB\n",
      "\t-13.5021\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.21s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 83.12s of the 233.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.20%)\n",
      "\t-14.3657\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.42s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 66.61s of the 216.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.10%)\n",
      "\t-13.1628\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.79s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 39.89s of the 189.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.96%)\n",
      "\t-14.8101\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.12s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 19.71s of the 169.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.93%)\n",
      "\t-13.0984\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.03s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 0.32s of the 150.27s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.01% memory usage per fold, 56.03%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=14.01%)\n",
      "\tTime limit exceeded... Skipping CatBoost_r177_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 147.55s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.3 GB\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 0.353, 'CatBoost_BAG_L1': 0.235, 'LightGBM_BAG_L1': 0.176, 'LightGBMXT_BAG_L1': 0.059, 'RandomForestMSE_BAG_L1': 0.059, 'NeuralNetFastAI_BAG_L1': 0.059, 'XGBoost_BAG_L1': 0.059}\n",
      "\t-12.9836\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 147.53s of the 147.50s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.43%)\n",
      "\t-13.0822\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.85s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 140.25s of the 140.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.05%)\n",
      "\t-13.1543\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.15s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 132.04s of the 132.02s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/5.0 GB\n",
      "\t-13.2505\t = Validation score   (-root_mean_squared_error)\n",
      "\t139.78s\t = Training   runtime\n",
      "\t1.18s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -9.85s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.5 GB\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 0.318, 'CatBoost_BAG_L1': 0.273, 'LightGBM_BAG_L1': 0.182, 'RandomForestMSE_BAG_L1': 0.091, 'NeuralNetFastAI_BAG_L1': 0.045, 'XGBoost_BAG_L1': 0.045, 'LightGBMXT_BAG_L2': 0.045}\n",
      "\t-12.9846\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 459.89s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1077.9 rows/s (1715 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        LightGBMXT_BAG_L2     -13.338906 -13.082183  root_mean_squared_error        1.745131       4.717520  283.403934                 0.051695                0.125306           5.853836            2       True         11\n",
      "1     LightGBMLarge_BAG_L1     -13.376468 -13.098409  root_mean_squared_error        0.160901       0.403632   16.031445                 0.160901                0.403632          16.031445            1       True          9\n",
      "2      WeightedEnsemble_L3     -13.385994 -12.984644  root_mean_squared_error        1.746723       4.717743  283.419359                 0.001593                0.000223           0.015425            3       True         14\n",
      "3      WeightedEnsemble_L2     -13.387539 -12.983556  root_mean_squared_error        0.995546       2.555934  229.225922                 0.002505                0.000268           0.010586            2       True         10\n",
      "4          LightGBM_BAG_L2     -13.399548 -13.154305  root_mean_squared_error        1.737095       4.680099  283.702505                 0.043659                0.087884           6.152407            2       True         12\n",
      "5          LightGBM_BAG_L1     -13.465068 -13.085120  root_mean_squared_error        0.066167       0.152010    8.232480                 0.066167                0.152010           8.232480            1       True          2\n",
      "6          CatBoost_BAG_L1     -13.478719 -13.133924  root_mean_squared_error        0.047793       0.032634   64.209512                 0.047793                0.032634          64.209512            1       True          4\n",
      "7        LightGBMXT_BAG_L1     -13.494331 -13.160600  root_mean_squared_error        0.083324       0.204920    7.620079                 0.083324                0.204920           7.620079            1       True          1\n",
      "8           XGBoost_BAG_L1     -13.497349 -13.162807  root_mean_squared_error        0.150613       0.240670   23.794588                 0.150613                0.240670          23.794588            1       True          7\n",
      "9   RandomForestMSE_BAG_L2     -13.520356 -13.250505  root_mean_squared_error        1.887753       5.767706  417.327859                 0.194317                1.175491         139.777761            2       True         13\n",
      "10  RandomForestMSE_BAG_L1     -13.727828 -13.296755  root_mean_squared_error        0.286077       1.102480   95.908496                 0.286077                1.102480          95.908496            1       True          3\n",
      "11    ExtraTreesMSE_BAG_L1     -13.887235 -13.502127  root_mean_squared_error        0.273477       1.116324   31.211890                 0.273477                1.116324          31.211890            1       True          5\n",
      "12  NeuralNetFastAI_BAG_L1     -14.169636 -14.365660  root_mean_squared_error        0.198166       0.419321   13.418735                 0.198166                0.419321          13.418735            1       True          6\n",
      "13   NeuralNetTorch_BAG_L1     -15.141826 -14.810129  root_mean_squared_error        0.426918       0.920225   17.122872                 0.426918                0.920225          17.122872            1       True          8\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t463s\t = DyStack   runtime |\t1337s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1337s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 378\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4789.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.14 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 44 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 333 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 312 | ['dist_0', 'dist_1', 'dist_2', 'dist_3', 'dist_4', ...]\n",
      "\t\t('int', ['bool']) :  41 | ['ep_idx_norm_19', 'is_final_team_0', 'is_final_team_1', 'is_final_team_2', 'is_final_team_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t353 features in original data used to generate 353 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.34 MB (0.8% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 890.84s of the 1336.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.61%)\n",
      "\t-13.1954\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.57s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 881.56s of the 1327.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.70%)\n",
      "\t-13.109\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.83s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 866.39s of the 1312.14s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/4.6 GB\n",
      "\t-13.3083\t = Validation score   (-root_mean_squared_error)\n",
      "\t122.19s\t = Training   runtime\n",
      "\t1.24s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 742.36s of the 1188.11s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.20% memory usage per fold, 40.41%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.20%)\n",
      "\t-13.1512\t = Validation score   (-root_mean_squared_error)\n",
      "\t73.47s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 666.73s of the 1112.48s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/4.5 GB\n",
      "\t-13.4885\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.18s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 629.17s of the 1074.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.30%)\n",
      "\t-14.3748\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.25s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 611.76s of the 1057.52s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.20% memory usage per fold, 40.78%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.20%)\n",
      "\t-13.1771\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.45s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 586.70s of the 1032.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.27%)\n",
      "\t-14.7965\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.99s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 568.14s of the 1013.90s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.82% memory usage per fold, 47.29%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.82%)\n",
      "\t-13.0787\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.67s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 527.25s of the 973.01s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.18% memory usage per fold, 40.35%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.18%)\n",
      "\t-13.1681\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.95s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 468.35s of the 914.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.36%)\n",
      "\t-14.2234\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.39s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 448.33s of the 894.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.03%)\n",
      "\t-13.0313\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.97s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 428.66s of the 874.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.53%)\n",
      "\t-14.3829\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.91s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 393.25s of the 839.00s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.64% memory usage per fold, 59.29%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=29.64%)\n",
      "\t-13.2252\t = Validation score   (-root_mean_squared_error)\n",
      "\t279.39s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 111.74s of the 557.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.19%)\n",
      "\t-13.2925\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.56s\t = Training   runtime\n",
      "\t0.68s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 74.30s of the 520.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.90%)\n",
      "\t-14.6872\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.9s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 44.62s of the 490.37s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.92% memory usage per fold, 59.84%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=29.92%)\n",
      "\t-15.1846\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.35s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 2.21s of the 447.97s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/4.5 GB\n",
      "\t-13.5046\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.76s\t = Training   runtime\n",
      "\t1.57s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 412.43s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.3 GB\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 0.35, 'LightGBM_r131_BAG_L1': 0.3, 'CatBoost_BAG_L1': 0.15, 'CatBoost_r177_BAG_L1': 0.15, 'RandomForestMSE_BAG_L1': 0.05}\n",
      "\t-12.9865\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 412.38s of the 412.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.59%)\n",
      "\t-13.0919\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.59s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 403.19s of the 403.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.22%)\n",
      "\t-13.099\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.13s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 394.71s of the 394.66s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/5.1 GB\n",
      "\t-13.2145\t = Validation score   (-root_mean_squared_error)\n",
      "\t138.8s\t = Training   runtime\n",
      "\t1.24s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 253.66s of the 253.61s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.47% memory usage per fold, 44.94%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=22.47%)\n",
      "\t-13.056\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.13s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 217.42s of the 217.38s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=0.1/3.8 GB\n",
      "\t-13.1498\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.83s\t = Training   runtime\n",
      "\t1.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 175.57s of the 175.52s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.80% memory usage per fold, 43.21%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.80%)\n",
      "\t-13.6186\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.76s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 149.58s of the 149.53s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.56% memory usage per fold, 46.22%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.56%)\n",
      "\t-13.136\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.25s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 122.37s of the 122.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.87%)\n",
      "\t-13.6638\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.13s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 104.60s of the 104.55s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.25% memory usage per fold, 52.99%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=13.25%)\n",
      "\t-13.1995\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.62s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 69.91s of the 69.86s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.81% memory usage per fold, 41.61%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.81%)\n",
      "\t-13.0514\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.53s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 39.49s of the 39.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.30%)\n",
      "\t-13.4753\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.41s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 21.91s of the 21.87s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.14% memory usage per fold, 40.54%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.14%)\n",
      "\t-13.1057\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.46s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -0.33s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.7 GB\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 0.28, 'LightGBM_r131_BAG_L1': 0.24, 'CatBoost_BAG_L1': 0.12, 'CatBoost_r177_BAG_L1': 0.12, 'ExtraTreesMSE_BAG_L2': 0.08, 'XGBoost_BAG_L2': 0.08, 'CatBoost_r177_BAG_L2': 0.08}\n",
      "\t-12.9829\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1337.47s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 320.0 rows/s (1930 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test 데이터 예측 중...\n",
      "==================================================\n",
      "Saved submission_autogluon_lastK.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# ----------------------\n",
    "# 0. 설정\n",
    "# ----------------------\n",
    "BASE_PATH = \"open_track1/\"\n",
    "PATH_TRAIN = os.path.join(BASE_PATH, \"train.csv\")\n",
    "PATH_TEST = os.path.join(BASE_PATH, \"test.csv\")\n",
    "PATH_MATCH_INFO = os.path.join(BASE_PATH, \"match_info.csv\")\n",
    "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "K = 20   # 마지막 K 이벤트 사용 (20~32 사이 선택)\n",
    "\n",
    "# ----------------------\n",
    "# 1. 데이터 로드\n",
    "# ----------------------\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test_index = pd.read_csv(PATH_TEST)\n",
    "match_info = pd.read_csv(PATH_MATCH_INFO)\n",
    "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
    "\n",
    "test_events_list = []\n",
    "for _, row in test_index.iterrows():\n",
    "    # path가 \"./test/...\" 형식이므로 BASE_PATH와 결합\n",
    "    test_path = os.path.join(BASE_PATH, row[\"path\"].lstrip(\"./\"))\n",
    "    df_ep = pd.read_csv(test_path)\n",
    "    test_events_list.append(df_ep)\n",
    "\n",
    "test_events = pd.concat(test_events_list, ignore_index=True)\n",
    "\n",
    "train[\"is_train\"] = 1\n",
    "test_events[\"is_train\"] = 0\n",
    "\n",
    "events = pd.concat([train, test_events], ignore_index=True)\n",
    "\n",
    "# ----------------------\n",
    "# 2. 기본 정렬 + episode 내 인덱스\n",
    "# ----------------------\n",
    "events = events.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
    "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
    "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
    "\n",
    "# ----------------------\n",
    "# 3. 시간/공간 feature\n",
    "# ----------------------\n",
    "# Δt\n",
    "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
    "events[\"dt\"] = events[\"time_seconds\"] - events[\"prev_time\"]\n",
    "events[\"dt\"] = events[\"dt\"].fillna(0.0)\n",
    "\n",
    "# 이동량/거리\n",
    "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
    "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
    "events[\"dist\"] = np.sqrt(events[\"dx\"]**2 + events[\"dy\"]**2)\n",
    "\n",
    "# 속도 (dt=0 보호)\n",
    "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
    "\n",
    "# zone / lane (필요시 범위 조정)\n",
    "events[\"x_zone\"] = (events[\"start_x\"] / (105/7)).astype(int).clip(0, 6)\n",
    "events[\"lane\"] = pd.cut(\n",
    "    events[\"start_y\"],\n",
    "    bins=[0, 68/3, 2*68/3, 68],\n",
    "    labels=[0, 1, 2],\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 4. 라벨 및 episode-level 메타 (train 전용)\n",
    "# ----------------------\n",
    "train_events = events[events[\"is_train\"] == 1].copy()\n",
    "\n",
    "last_events = (\n",
    "    train_events\n",
    "    .groupby(\"game_episode\", as_index=False)\n",
    "    .tail(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "labels = last_events[[\"game_episode\", \"end_x\", \"end_y\"]].rename(\n",
    "    columns={\"end_x\": \"target_x\", \"end_y\": \"target_y\"}\n",
    ")\n",
    "\n",
    "# episode-level 메타 (마지막 이벤트 기준)\n",
    "ep_meta = last_events[[\"game_episode\", \"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"]].copy()\n",
    "ep_meta = ep_meta.rename(columns={\"team_id\": \"final_team_id\"})\n",
    "\n",
    "# game_clock (분 단위, 0~90+)\n",
    "ep_meta[\"game_clock_min\"] = np.where(\n",
    "    ep_meta[\"period_id\"] == 1,\n",
    "    ep_meta[\"time_seconds\"] / 60.0,\n",
    "    45.0 + ep_meta[\"time_seconds\"] / 60.0\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5. 공격 팀 플래그 (final_team vs 상대)\n",
    "# ----------------------\n",
    "# final_team_id를 전체 events에 붙임\n",
    "events = events.merge(\n",
    "    ep_meta[[\"game_episode\", \"final_team_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 6. 입력용 events에서 마지막 이벤트 타깃 정보 가리기\n",
    "# ----------------------\n",
    "# is_last 플래그\n",
    "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
    "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
    "\n",
    "# labels는 이미 뽑아놨으니, 입력쪽에서만 end_x, end_y, dx, dy, dist, speed 지움\n",
    "mask_last = events[\"is_last\"] == 1\n",
    "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
    "    events.loc[mask_last, col] = np.nan\n",
    "\n",
    "# ----------------------\n",
    "# 7. 카테고리 인코딩 (type_name, result_name, team_id 등)\n",
    "# ----------------------\n",
    "events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
    "events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA_RES__\")\n",
    "\n",
    "le_type = LabelEncoder()\n",
    "le_res = LabelEncoder()\n",
    "\n",
    "events[\"type_id\"] = le_type.fit_transform(events[\"type_name\"])\n",
    "events[\"res_id\"] = le_res.fit_transform(events[\"result_name\"])\n",
    "\n",
    "# team_id는 그대로 써도 되지만, 문자열이면 숫자로 매핑\n",
    "if events[\"team_id\"].dtype == \"object\":\n",
    "    le_team = LabelEncoder()\n",
    "    events[\"team_id_enc\"] = le_team.fit_transform(events[\"team_id\"])\n",
    "else:\n",
    "    events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 8. 마지막 K 이벤트만 사용 (lastK)\n",
    "# ----------------------\n",
    "# rev_idx: 0이 마지막 이벤트\n",
    "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\n",
    "    lambda s: s.max() - s\n",
    ")\n",
    "\n",
    "lastK = events[events[\"rev_idx\"] < K].copy()\n",
    "\n",
    "# pos_in_K: 0~(K-1), 앞쪽 패딩 고려해서 뒤에 실제 이벤트가 모이게\n",
    "def assign_pos_in_K(df):\n",
    "    df = df.sort_values(\"event_idx\")  # 오래된 → 최근\n",
    "    L = len(df)\n",
    "    df = df.copy()\n",
    "    df[\"pos_in_K\"] = np.arange(K - L, K)\n",
    "    return df\n",
    "\n",
    "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
    "\n",
    "# ----------------------\n",
    "# 9. wide feature pivot\n",
    "# ----------------------\n",
    "# 사용할 이벤트 피처 선택\n",
    "num_cols = [\n",
    "    \"start_x\", \"start_y\",\n",
    "    \"end_x\", \"end_y\",\n",
    "    \"dx\", \"dy\", \"dist\", \"speed\",\n",
    "    \"dt\",\n",
    "    \"ep_idx_norm\",\n",
    "    \"x_zone\", \"lane\",\n",
    "    \"is_final_team\",\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"type_id\",\n",
    "    \"res_id\",\n",
    "    \"team_id_enc\",\n",
    "    \"is_home\",\n",
    "    \"period_id\",\n",
    "    \"is_last\",\n",
    "]\n",
    "\n",
    "feature_cols = num_cols + cat_cols\n",
    "\n",
    "wide = lastK[[\"game_episode\", \"pos_in_K\"] + feature_cols].copy()\n",
    "\n",
    "# 숫자형 pivot\n",
    "wide_num = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=num_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 범주형 pivot\n",
    "wide_cat = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=cat_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 컬럼 이름 평탄화\n",
    "wide_num.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_num.columns]\n",
    "wide_cat.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_cat.columns]\n",
    "\n",
    "X = pd.concat([wide_num, wide_cat], axis=1).reset_index()  # game_episode 포함\n",
    "\n",
    "# episode-level 메타 붙이기\n",
    "X = X.merge(ep_meta[[\"game_episode\", \"game_id\", \"game_clock_min\", \"final_team_id\", \"is_home\", \"period_id\"]],\n",
    "            on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# train 라벨 붙이기\n",
    "X = X.merge(labels, on=\"game_episode\", how=\"left\")  # test는 NaN\n",
    "\n",
    "# ----------------------\n",
    "# 10. train/test 분리\n",
    "# ----------------------\n",
    "train_mask = X[\"game_episode\"].isin(labels[\"game_episode\"])\n",
    "X_train = X[train_mask].copy()\n",
    "X_test = X[~train_mask].copy()\n",
    "\n",
    "y_train_x = X_train[\"target_x\"].astype(float)\n",
    "y_train_y = X_train[\"target_y\"].astype(float)\n",
    "\n",
    "# 모델 입력에서 빼야 할 컬럼들\n",
    "drop_cols = [\n",
    "    \"game_episode\",\n",
    "    \"game_id\",\n",
    "    \"target_x\",\n",
    "    \"target_y\",\n",
    "]\n",
    "\n",
    "X_train_feat = X_train.drop(columns=drop_cols)\n",
    "X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns])\n",
    "\n",
    "# NaN 채우기 (LGBM은 NaN 다루긴 하지만, 깔끔하게)\n",
    "X_train_feat = X_train_feat.fillna(0)\n",
    "X_test_feat = X_test_feat.fillna(0)\n",
    "\n",
    "# ----------------------\n",
    "# 11. AutoGluon 학습\n",
    "# ----------------------\n",
    "# X 좌표 예측 모델\n",
    "print(\"=\" * 50)\n",
    "print(\"X 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_x = X_train_feat.copy()\n",
    "train_data_x[\"target_x\"] = y_train_x\n",
    "\n",
    "predictor_x = TabularPredictor(\n",
    "    label=\"target_x\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=\"ag_models_x\"  # 모델 저장 경로\n",
    ").fit(\n",
    "    train_data=train_data_x,\n",
    "    time_limit=1800,  # 30분 (필요에 따라 조정: 600=10분, 1800=30분, 3600=1시간)\n",
    "    presets=\"best_quality\",  # 최고 품질 모드 (빠른 테스트: \"good_quality\" 또는 \"medium_quality\")\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Y 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_y = X_train_feat.copy()\n",
    "train_data_y[\"target_y\"] = y_train_y\n",
    "\n",
    "predictor_y = TabularPredictor(\n",
    "    label=\"target_y\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=\"ag_models_y\"  # 모델 저장 경로\n",
    ").fit(\n",
    "    train_data=train_data_y,\n",
    "    time_limit=1800,  # 30분 (필요에 따라 조정: 600=10분, 1800=30분, 3600=1시간)\n",
    "    presets=\"best_quality\",  # 최고 품질 모드 (빠른 테스트: \"good_quality\" 또는 \"medium_quality\")\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 12. test 예측\n",
    "# ----------------------\n",
    "print(\"=\" * 50)\n",
    "print(\"Test 데이터 예측 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pred_x = predictor_x.predict(X_test_feat)\n",
    "pred_y = predictor_y.predict(X_test_feat)\n",
    "\n",
    "# 필드 범위로 클립\n",
    "pred_x = np.clip(pred_x, 0, 105)\n",
    "pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "# ----------------------\n",
    "# 13. submission 생성\n",
    "# ----------------------\n",
    "sub = sample_sub.copy()\n",
    "\n",
    "# X_test에는 game_episode가 있으니, test_index와 align\n",
    "pred_df = X_test[[\"game_episode\"]].copy()\n",
    "pred_df[\"end_x\"] = pred_x\n",
    "pred_df[\"end_y\"] = pred_y\n",
    "\n",
    "sub = sub.drop(columns=[\"end_x\", \"end_y\"], errors=\"ignore\")\n",
    "sub = sub.merge(pred_df, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "sub.to_csv(\"submission_autogluon_lastK.csv\", index=False)\n",
    "print(\"Saved submission_autogluon_lastK.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11416c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "모델 성능 확인\n",
      "==================================================\n",
      "\n",
      "[X 좌표 모델 - 리더보드]\n",
      "                  model  score_val              eval_metric  pred_time_val  \\\n",
      "0   WeightedEnsemble_L3 -11.738371  root_mean_squared_error      11.849119   \n",
      "1   WeightedEnsemble_L2 -11.742939  root_mean_squared_error       2.881452   \n",
      "2       CatBoost_BAG_L2 -11.793024  root_mean_squared_error       8.744761   \n",
      "3  CatBoost_r177_BAG_L2 -11.801320  root_mean_squared_error       8.742393   \n",
      "4     LightGBMXT_BAG_L2 -11.809666  root_mean_squared_error       8.838531   \n",
      "5  LightGBM_r131_BAG_L1 -11.815296  root_mean_squared_error       0.538745   \n",
      "6  ExtraTreesMSE_BAG_L2 -11.836941  root_mean_squared_error       9.913169   \n",
      "7       LightGBM_BAG_L2 -11.842222  root_mean_squared_error       8.818286   \n",
      "8  LightGBM_r131_BAG_L2 -11.843725  root_mean_squared_error       9.158671   \n",
      "9  CatBoost_r177_BAG_L1 -11.851047  root_mean_squared_error       0.025972   \n",
      "\n",
      "      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
      "0  1071.772211                0.000380           0.025954            3   \n",
      "1   314.131895                0.000243           0.025303            2   \n",
      "2   895.968007                0.028599          34.757715            2   \n",
      "3   890.402805                0.026230          29.192513            2   \n",
      "4   866.318889                0.122368           5.108597            2   \n",
      "5    38.796379                0.538745          38.796379            1   \n",
      "6   898.794128                1.197007          37.583836            2   \n",
      "7   867.266157                0.102123           6.055865            2   \n",
      "8   879.238077                0.442509          18.027785            2   \n",
      "9    43.870559                0.025972          43.870559            1   \n",
      "\n",
      "   can_infer  fit_order  \n",
      "0       True         32  \n",
      "1       True         17  \n",
      "2       True         21  \n",
      "3       True         27  \n",
      "4       True         18  \n",
      "5       True         12  \n",
      "6       True         22  \n",
      "7       True         19  \n",
      "8       True         29  \n",
      "9       True         10  \n",
      "\n",
      "[Y 좌표 모델 - 리더보드]\n",
      "                  model  score_val              eval_metric  pred_time_val  \\\n",
      "0   WeightedEnsemble_L3 -12.982933  root_mean_squared_error      10.684680   \n",
      "1   WeightedEnsemble_L2 -12.986481  root_mean_squared_error       2.251936   \n",
      "2  LightGBM_r131_BAG_L1 -13.031335  root_mean_squared_error       0.722700   \n",
      "3  CatBoost_r177_BAG_L2 -13.051425  root_mean_squared_error       9.209039   \n",
      "4       CatBoost_BAG_L2 -13.055990  root_mean_squared_error       9.219853   \n",
      "5  LightGBMLarge_BAG_L1 -13.078690  root_mean_squared_error       0.219312   \n",
      "6     LightGBMXT_BAG_L2 -13.091889  root_mean_squared_error       9.356584   \n",
      "7       LightGBM_BAG_L2 -13.099008  root_mean_squared_error       9.283466   \n",
      "8  LightGBM_r131_BAG_L2 -13.105734  root_mean_squared_error       9.446670   \n",
      "9       LightGBM_BAG_L1 -13.108974  root_mean_squared_error       0.192661   \n",
      "\n",
      "     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
      "0  973.441204                0.000754           0.054684            3   \n",
      "1  308.282725                0.000372           0.034269            2   \n",
      "2   16.968154                0.722700          16.968154            1   \n",
      "3  908.305333                0.024867          28.525223            2   \n",
      "4  913.911644                0.035681          34.131534            2   \n",
      "5   38.666364                0.219312          38.666364            1   \n",
      "6  885.374091                0.172411           5.593981            2   \n",
      "7  885.910720                0.099294           6.130610            2   \n",
      "8  899.238176                0.262497          19.458066            2   \n",
      "9   12.825840                0.192661          12.825840            1   \n",
      "\n",
      "   can_infer  fit_order  \n",
      "0       True         32  \n",
      "1       True         19  \n",
      "2       True         12  \n",
      "3       True         29  \n",
      "4       True         23  \n",
      "5       True          9  \n",
      "6       True         20  \n",
      "7       True         21  \n",
      "8       True         31  \n",
      "9       True          2  \n",
      "\n",
      "==================================================\n",
      "Train 데이터 성능 평가\n",
      "==================================================\n",
      "\n",
      "X 좌표 RMSE (Train): 8.3561\n",
      "Y 좌표 RMSE (Train): 9.0770\n",
      "\n",
      "==================================================\n",
      "제출 파일 확인\n",
      "==================================================\n",
      "\n",
      "제출 파일 행 수: 2414\n",
      "\n",
      "제출 파일 샘플:\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  63.180412  11.654820\n",
      "1     153363_2  34.504665  51.694386\n",
      "2     153363_6  37.633560  60.892597\n",
      "3     153363_7  56.163864  11.569715\n",
      "4     153363_8  80.444220  12.854674\n",
      "5     153363_9  73.571740  64.039986\n",
      "6    153363_10  63.962627  14.813020\n",
      "7    153363_12  72.740830  11.019011\n",
      "8    153363_13  33.860480  62.532597\n",
      "9    153363_15  74.677490  13.754055\n",
      "\n",
      "제출 파일 통계:\n",
      "             end_x        end_y\n",
      "count  2414.000000  2414.000000\n",
      "mean     67.012160    33.838051\n",
      "std      19.963615    19.499156\n",
      "min       9.634449     2.673091\n",
      "25%      52.645016    14.025308\n",
      "50%      71.694882    34.616065\n",
      "75%      83.744680    53.174136\n",
      "max     100.539696    65.979220\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 결과 확인 (학습 완료 후 실행)\n",
    "# ----------------------\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 저장된 모델 로드\n",
    "predictor_x = TabularPredictor.load(\"ag_models_x\")\n",
    "predictor_y = TabularPredictor.load(\"ag_models_y\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"모델 성능 확인\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# X 좌표 모델 리더보드\n",
    "print(\"\\n[X 좌표 모델 - 리더보드]\")\n",
    "leaderboard_x = predictor_x.leaderboard(silent=True)\n",
    "print(leaderboard_x.head(10))\n",
    "\n",
    "# Y 좌표 모델 리더보드\n",
    "print(\"\\n[Y 좌표 모델 - 리더보드]\")\n",
    "leaderboard_y = predictor_y.leaderboard(silent=True)\n",
    "print(leaderboard_y.head(10))\n",
    "\n",
    "# Train 데이터로 성능 평가 (첫 번째 셀 실행 후 사용 가능)\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Train 데이터 성능 평가\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # X 좌표 평가\n",
    "    y_pred_train_x = predictor_x.predict(X_train_feat)\n",
    "    rmse_x = np.sqrt(np.mean((y_train_x - y_pred_train_x) ** 2))\n",
    "    print(f\"\\nX 좌표 RMSE (Train): {rmse_x:.4f}\")\n",
    "    \n",
    "    # Y 좌표 평가\n",
    "    y_pred_train_y = predictor_y.predict(X_train_feat)\n",
    "    rmse_y = np.sqrt(np.mean((y_train_y - y_pred_train_y) ** 2))\n",
    "    print(f\"Y 좌표 RMSE (Train): {rmse_y:.4f}\")\n",
    "except NameError:\n",
    "    print(\"\\n(첫 번째 셀을 먼저 실행해야 Train 데이터 평가가 가능합니다)\")\n",
    "\n",
    "# 제출 파일 확인\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"제출 파일 확인\")\n",
    "print(\"=\" * 50)\n",
    "sub = pd.read_csv(\"submission_autogluon_lastK.csv\")\n",
    "print(f\"\\n제출 파일 행 수: {len(sub)}\")\n",
    "print(f\"\\n제출 파일 샘플:\")\n",
    "print(sub.head(10))\n",
    "print(f\"\\n제출 파일 통계:\")\n",
    "print(sub.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
