{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8477ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/1906484028.py:190: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/1906484028.py:276: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train_feat.fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/1906484028.py:277: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test_feat.fillna(0)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.33 GB / 16.00 GB (20.8%)\n",
      "Disk Space Avail:   10.17 GB / 460.43 GB (2.2%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "빠른 테스트 모드: CatBoost + LightGBM만 사용 (10분)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "X 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_corner/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 478\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3406.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.81 MB (1.7% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 434 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 392 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  62 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t454 features in original data used to generate 454 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.84 MB (1.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 99.62s of the 149.47s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.99% memory usage per fold, 43.96%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.99%)\n",
      "\t-11.9302\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.1s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 77.33s of the 127.18s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.74% memory usage per fold, 73.48%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=36.74%)\n",
      "\t-11.9104\t = Validation score   (-root_mean_squared_error)\n",
      "\t67.58s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.47s of the 57.49s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/2.8 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.529, 'LightGBM_BAG_L1': 0.471}\n",
      "\t-11.8404\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 57.44s of the 57.30s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.11% memory usage per fold, 40.45%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.11%)\n",
      "\t-11.996\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.25s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 41.80s of the 41.67s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.84% memory usage per fold, 53.68%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=26.84%)\n",
      "\t-11.9174\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.73s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.47s of the 2.17s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.6 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.529, 'LightGBM_BAG_L1': 0.471}\n",
      "\t-11.8404\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 147.94s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5024.3 rows/s (1715 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.6 GB\n",
      "\t3.54s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.0/3.6 GB\n",
      "\t8.67s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.529, 'LightGBM_BAG_L1': 0.471}\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.5 GB\n",
      "\t1.37s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.0/3.6 GB\n",
      "\t3.19s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.529, 'LightGBM_BAG_L1': 0.471}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 17.27s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_corner/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val   fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3_FULL     -11.758618 -11.840438  root_mean_squared_error        0.021143          None  12.227436                 0.000965                   None           0.010742            3       True          6\n",
      "1  WeightedEnsemble_L2_FULL     -11.758618 -11.840438  root_mean_squared_error        0.021428          None  12.234931                 0.001250                   None           0.018237            2       True          3\n",
      "2      CatBoost_BAG_L2_FULL     -11.767737 -11.917434  root_mean_squared_error        0.029207          None  15.404923                 0.009029                   None           3.188229            2       True          5\n",
      "3      LightGBM_BAG_L2_FULL     -11.806399 -11.996012  root_mean_squared_error        0.026517          None  13.586564                 0.006339                   None           1.369870            2       True          4\n",
      "4      CatBoost_BAG_L1_FULL     -11.833158 -11.910448  root_mean_squared_error        0.008660          None   8.673582                 0.008660                   None           8.673582            1       True          2\n",
      "5      LightGBM_BAG_L1_FULL     -11.837549 -11.930182  root_mean_squared_error        0.011518          None   3.543112                 0.011518                   None           3.543112            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t166s\t = DyStack   runtime |\t434s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 434s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_corner\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 478\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3601.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.91 MB (1.8% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 433 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 392 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t453 features in original data used to generate 453 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.06 MB (1.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 288.90s of the 433.45s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.02% memory usage per fold, 44.07%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.02%)\n",
      "\t-11.8871\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.29s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 270.17s of the 414.72s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.98% memory usage per fold, 57.95%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=28.98%)\n",
      "\t-11.8554\t = Validation score   (-root_mean_squared_error)\n",
      "\t80.36s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 332.78s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.6 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.556, 'LightGBM_BAG_L1': 0.444}\n",
      "\t-11.795\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 332.76s of the 332.74s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.76% memory usage per fold, 43.04%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.76%)\n",
      "\t-11.9087\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.31s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 319.41s of the 319.39s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.50% memory usage per fold, 62.99%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=31.50%)\n",
      "\t-11.8524\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.91s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 278.90s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.6 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.52, 'LightGBM_BAG_L1': 0.44, 'CatBoost_BAG_L2': 0.04}\n",
      "\t-11.7957\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 155.25s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 9659.8 rows/s (1930 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.6 GB\n",
      "\t2.59s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.0/3.9 GB\n",
      "\t7.64s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.556, 'LightGBM_BAG_L1': 0.444}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.7 GB\n",
      "\t1.31s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.0/3.7 GB\n",
      "\t2.39s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.52, 'LightGBM_BAG_L1': 0.44, 'CatBoost_BAG_L2': 0.04}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 14.34s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_corner\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.67 GB / 16.00 GB (22.9%)\n",
      "Disk Space Avail:   10.13 GB / 460.43 GB (2.2%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_corner/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 478\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Y 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3813.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.81 MB (1.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 434 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 392 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  62 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t454 features in original data used to generate 454 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.84 MB (1.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 99.66s of the 149.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.78%)\n",
      "\t-13.1141\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.84s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 88.14s of the 138.00s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.48% memory usage per fold, 52.97%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=26.48%)\n",
      "\t-13.1526\t = Validation score   (-root_mean_squared_error)\n",
      "\t74.88s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.53s of the 61.46s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.7 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.565, 'CatBoost_BAG_L1': 0.435}\n",
      "\t-13.0601\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 61.44s of the 61.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.64%)\n",
      "\t-13.1644\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.77s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 51.08s of the 51.07s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.49% memory usage per fold, 62.98%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=31.49%)\n",
      "\t-13.1075\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.88s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.53s of the 9.55s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.4 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.5, 'CatBoost_BAG_L1': 0.375, 'LightGBM_BAG_L2': 0.083, 'CatBoost_BAG_L2': 0.042}\n",
      "\t-13.0592\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 140.58s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 4201.2 rows/s (1715 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.6 GB\n",
      "\t1.89s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.0/3.8 GB\n",
      "\t7.62s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.565, 'CatBoost_BAG_L1': 0.435}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.8 GB\n",
      "\t1.39s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.0/3.7 GB\n",
      "\t2.22s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.5, 'CatBoost_BAG_L1': 0.375, 'LightGBM_BAG_L2': 0.083, 'CatBoost_BAG_L2': 0.042}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 13.48s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_corner/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val   fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3_FULL     -13.419346 -13.059222  root_mean_squared_error        0.028543          None  13.143661                 0.001053                   None           0.011883            3       True          6\n",
      "1      CatBoost_BAG_L2_FULL     -13.420460 -13.107484  root_mean_squared_error        0.022264          None  11.740061                 0.008173                   None           2.224069            2       True          5\n",
      "2  WeightedEnsemble_L2_FULL     -13.427884 -13.060111  root_mean_squared_error        0.015012          None   9.522565                 0.000921                   None           0.006573            2       True          3\n",
      "3      LightGBM_BAG_L2_FULL     -13.458448 -13.164360  root_mean_squared_error        0.019317          None  10.907709                 0.005226                   None           1.391717            2       True          4\n",
      "4      LightGBM_BAG_L1_FULL     -13.473385 -13.114088  root_mean_squared_error        0.006583          None   1.891320                 0.006583                   None           1.891320            1       True          1\n",
      "5      CatBoost_BAG_L1_FULL     -13.522044 -13.152571  root_mean_squared_error        0.007508          None   7.624672                 0.007508                   None           7.624672            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t155s\t = DyStack   runtime |\t445s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 445s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_corner\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 478\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3854.70 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.91 MB (1.7% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 433 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 392 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t453 features in original data used to generate 453 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.06 MB (1.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 296.56s of the 444.95s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.40% memory usage per fold, 41.60%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.40%)\n",
      "\t-13.137\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.6s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 281.82s of the 430.21s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.37% memory usage per fold, 54.74%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=27.37%)\n",
      "\t-13.1634\t = Validation score   (-root_mean_squared_error)\n",
      "\t102.73s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 325.92s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.3 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.55, 'CatBoost_BAG_L1': 0.45}\n",
      "\t-13.0812\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 325.89s of the 325.86s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.49% memory usage per fold, 45.94%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.49%)\n",
      "\t-13.1986\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.07s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 305.55s of the 305.52s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 30.39% memory usage per fold, 60.77%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=30.39%)\n",
      "\t-13.1237\t = Validation score   (-root_mean_squared_error)\n",
      "\t53.46s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 250.22s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.4 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.48, 'CatBoost_BAG_L1': 0.4, 'LightGBM_BAG_L2': 0.08, 'CatBoost_BAG_L2': 0.04}\n",
      "\t-13.0807\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 195.38s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 5206.6 rows/s (1930 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.3 GB\n",
      "\t2.35s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.0/3.5 GB\n",
      "\t10.06s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.55, 'CatBoost_BAG_L1': 0.45}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.2 GB\n",
      "\t1.51s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.0/3.2 GB\n",
      "\t2.16s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.48, 'CatBoost_BAG_L1': 0.4, 'LightGBM_BAG_L2': 0.08, 'CatBoost_BAG_L2': 0.04}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 16.53s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_corner\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test 데이터 예측 중...\n",
      "==================================================\n",
      "Saved submission_autogluon_corner.csv\n",
      "✓ X 모델 백업 완료: backups/corner_features_20251225_214316/ag_models_x_corner\n",
      "✓ Y 모델 백업 완료: backups/corner_features_20251225_214316/ag_models_y_corner\n",
      "✓ 제출 파일 백업 완료: backups/corner_features_20251225_214316/submission_autogluon_corner.csv\n",
      "\n",
      "백업 완료: backups/corner_features_20251225_214316\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# ----------------------\n",
    "# 0. 설정\n",
    "# ----------------------\n",
    "BASE_PATH = \"open_track1/\"\n",
    "PATH_TRAIN = os.path.join(BASE_PATH, \"train.csv\")\n",
    "PATH_TEST = os.path.join(BASE_PATH, \"test.csv\")\n",
    "PATH_MATCH_INFO = os.path.join(BASE_PATH, \"match_info.csv\")\n",
    "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "K = 20   # 마지막 K 이벤트 사용 (20~32 사이 선택)\n",
    "\n",
    "# ----------------------\n",
    "# 1. 데이터 로드\n",
    "# ----------------------\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test_index = pd.read_csv(PATH_TEST)\n",
    "match_info = pd.read_csv(PATH_MATCH_INFO)\n",
    "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
    "\n",
    "test_events_list = []\n",
    "for _, row in test_index.iterrows():\n",
    "    # path가 \"./test/...\" 형식이므로 BASE_PATH와 결합\n",
    "    test_path = os.path.join(BASE_PATH, row[\"path\"].lstrip(\"./\"))\n",
    "    df_ep = pd.read_csv(test_path)\n",
    "    test_events_list.append(df_ep)\n",
    "\n",
    "test_events = pd.concat(test_events_list, ignore_index=True)\n",
    "\n",
    "train[\"is_train\"] = 1\n",
    "test_events[\"is_train\"] = 0\n",
    "\n",
    "events = pd.concat([train, test_events], ignore_index=True)\n",
    "\n",
    "# ----------------------\n",
    "# 2. 기본 정렬 + episode 내 인덱스\n",
    "# ----------------------\n",
    "events = events.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
    "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
    "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
    "\n",
    "# ----------------------\n",
    "# 3. 시간/공간 feature\n",
    "# ----------------------\n",
    "# Δt\n",
    "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
    "events[\"dt\"] = events[\"time_seconds\"] - events[\"prev_time\"]\n",
    "events[\"dt\"] = events[\"dt\"].fillna(0.0)\n",
    "\n",
    "# 이동량/거리\n",
    "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
    "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
    "events[\"dist\"] = np.sqrt(events[\"dx\"]**2 + events[\"dy\"]**2)\n",
    "\n",
    "# 속도 (dt=0 보호)\n",
    "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
    "\n",
    "# zone / lane (필요시 범위 조정)\n",
    "events[\"x_zone\"] = (events[\"start_x\"] / (105/7)).astype(int).clip(0, 6)\n",
    "events[\"lane\"] = pd.cut(\n",
    "    events[\"start_y\"],\n",
    "    bins=[0, 68/3, 2*68/3, 68],\n",
    "    labels=[0, 1, 2],\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 3-1. 코너 관련 피처 (Corner Features)\n",
    "# ----------------------\n",
    "# 골대까지 각도 계산 (라디안 → 도)\n",
    "# 골대 중앙: (105, 34)\n",
    "events[\"angle_to_goal\"] = np.arctan2(\n",
    "    34 - events[\"start_y\"],\n",
    "    105 - events[\"start_x\"]\n",
    ") * 180 / np.pi\n",
    "\n",
    "# 코너까지 최단 거리 계산\n",
    "# 공격 방향 코너: (105, 0) 상단, (105, 68) 하단\n",
    "events[\"dist_corner_top\"] = np.sqrt((105 - events[\"start_x\"])**2 + (0 - events[\"start_y\"])**2)\n",
    "events[\"dist_corner_bottom\"] = np.sqrt((105 - events[\"start_x\"])**2 + (68 - events[\"start_y\"])**2)\n",
    "events[\"dist_to_nearest_corner\"] = events[[\"dist_corner_top\", \"dist_corner_bottom\"]].min(axis=1)\n",
    "\n",
    "# 코너 구역 플래그 (X > 100 이면서 Y < 5 또는 Y > 63)\n",
    "events[\"is_corner_area\"] = ((events[\"start_x\"] > 100) & \n",
    "                            ((events[\"start_y\"] < 5) | (events[\"start_y\"] > 63))).astype(int)\n",
    "\n",
    "# 인터랙션 피처 1: angle_to_goal × is_corner_area\n",
    "# 코너 구역일 때만 각도의 영향이 강하게 나타남\n",
    "events[\"angle_goal_x_corner\"] = events[\"angle_to_goal\"] * events[\"is_corner_area\"]\n",
    "\n",
    "# 인터랙션 피처 2: dist_to_nearest_corner × angle_to_goal\n",
    "# 코너 거리와 각도의 조합\n",
    "events[\"dist_corner_x_angle\"] = events[\"dist_to_nearest_corner\"] * events[\"angle_to_goal\"]\n",
    "\n",
    "# ----------------------\n",
    "# 4. 라벨 및 episode-level 메타 (train 전용)\n",
    "# ----------------------\n",
    "train_events = events[events[\"is_train\"] == 1].copy()\n",
    "\n",
    "last_events = (\n",
    "    train_events\n",
    "    .groupby(\"game_episode\", as_index=False)\n",
    "    .tail(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "labels = last_events[[\"game_episode\", \"end_x\", \"end_y\"]].rename(\n",
    "    columns={\"end_x\": \"target_x\", \"end_y\": \"target_y\"}\n",
    ")\n",
    "\n",
    "# episode-level 메타 (마지막 이벤트 기준)\n",
    "ep_meta = last_events[[\"game_episode\", \"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"]].copy()\n",
    "ep_meta = ep_meta.rename(columns={\"team_id\": \"final_team_id\"})\n",
    "\n",
    "# game_clock (분 단위, 0~90+)\n",
    "ep_meta[\"game_clock_min\"] = np.where(\n",
    "    ep_meta[\"period_id\"] == 1,\n",
    "    ep_meta[\"time_seconds\"] / 60.0,\n",
    "    45.0 + ep_meta[\"time_seconds\"] / 60.0\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5. 공격 팀 플래그 (final_team vs 상대)\n",
    "# ----------------------\n",
    "# final_team_id를 전체 events에 붙임\n",
    "events = events.merge(\n",
    "    ep_meta[[\"game_episode\", \"final_team_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 6. 입력용 events에서 마지막 이벤트 타깃 정보 가리기\n",
    "# ----------------------\n",
    "# is_last 플래그\n",
    "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
    "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
    "\n",
    "# labels는 이미 뽑아놨으니, 입력쪽에서만 end_x, end_y, dx, dy, dist, speed 지움\n",
    "mask_last = events[\"is_last\"] == 1\n",
    "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
    "    events.loc[mask_last, col] = np.nan\n",
    "\n",
    "# ----------------------\n",
    "# 7. 카테고리 인코딩 (type_name, result_name, team_id 등)\n",
    "# ----------------------\n",
    "events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
    "events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA_RES__\")\n",
    "\n",
    "le_type = LabelEncoder()\n",
    "le_res = LabelEncoder()\n",
    "\n",
    "events[\"type_id\"] = le_type.fit_transform(events[\"type_name\"])\n",
    "events[\"res_id\"] = le_res.fit_transform(events[\"result_name\"])\n",
    "\n",
    "# team_id는 그대로 써도 되지만, 문자열이면 숫자로 매핑\n",
    "if events[\"team_id\"].dtype == \"object\":\n",
    "    le_team = LabelEncoder()\n",
    "    events[\"team_id_enc\"] = le_team.fit_transform(events[\"team_id\"])\n",
    "else:\n",
    "    events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 8. 마지막 K 이벤트만 사용 (lastK)\n",
    "# ----------------------\n",
    "# rev_idx: 0이 마지막 이벤트\n",
    "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\n",
    "    lambda s: s.max() - s\n",
    ")\n",
    "\n",
    "lastK = events[events[\"rev_idx\"] < K].copy()\n",
    "\n",
    "# pos_in_K: 0~(K-1), 앞쪽 패딩 고려해서 뒤에 실제 이벤트가 모이게\n",
    "def assign_pos_in_K(df):\n",
    "    df = df.sort_values(\"event_idx\")  # 오래된 → 최근\n",
    "    L = len(df)\n",
    "    df = df.copy()\n",
    "    df[\"pos_in_K\"] = np.arange(K - L, K)\n",
    "    return df\n",
    "\n",
    "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
    "\n",
    "# ----------------------\n",
    "# 9. wide feature pivot\n",
    "# ----------------------\n",
    "# 사용할 이벤트 피처 선택\n",
    "num_cols = [\n",
    "    \"start_x\", \"start_y\",\n",
    "    \"end_x\", \"end_y\",\n",
    "    \"dx\", \"dy\", \"dist\", \"speed\",\n",
    "    \"dt\",\n",
    "    \"ep_idx_norm\",\n",
    "    \"x_zone\", \"lane\",\n",
    "    \"is_final_team\",\n",
    "    # 코너 관련 피처\n",
    "    \"angle_to_goal\",\n",
    "    \"dist_to_nearest_corner\",\n",
    "    \"is_corner_area\",\n",
    "    \"angle_goal_x_corner\",  # 인터랙션: angle_to_goal × is_corner_area\n",
    "    \"dist_corner_x_angle\",  # 인터랙션: dist_to_nearest_corner × angle_to_goal\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"type_id\",\n",
    "    \"res_id\",\n",
    "    \"team_id_enc\",\n",
    "    \"is_home\",\n",
    "    \"period_id\",\n",
    "    \"is_last\",\n",
    "]\n",
    "\n",
    "feature_cols = num_cols + cat_cols\n",
    "\n",
    "wide = lastK[[\"game_episode\", \"pos_in_K\"] + feature_cols].copy()\n",
    "\n",
    "# 숫자형 pivot\n",
    "wide_num = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=num_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 범주형 pivot\n",
    "wide_cat = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=cat_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 컬럼 이름 평탄화\n",
    "wide_num.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_num.columns]\n",
    "wide_cat.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_cat.columns]\n",
    "\n",
    "X = pd.concat([wide_num, wide_cat], axis=1).reset_index()  # game_episode 포함\n",
    "\n",
    "# episode-level 메타 붙이기\n",
    "X = X.merge(ep_meta[[\"game_episode\", \"game_id\", \"game_clock_min\", \"final_team_id\", \"is_home\", \"period_id\"]],\n",
    "            on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# train 라벨 붙이기\n",
    "X = X.merge(labels, on=\"game_episode\", how=\"left\")  # test는 NaN\n",
    "\n",
    "# ----------------------\n",
    "# 10. train/test 분리\n",
    "# ----------------------\n",
    "train_mask = X[\"game_episode\"].isin(labels[\"game_episode\"])\n",
    "X_train = X[train_mask].copy()\n",
    "X_test = X[~train_mask].copy()\n",
    "\n",
    "y_train_x = X_train[\"target_x\"].astype(float)\n",
    "y_train_y = X_train[\"target_y\"].astype(float)\n",
    "\n",
    "# 모델 입력에서 빼야 할 컬럼들\n",
    "drop_cols = [\n",
    "    \"game_episode\",\n",
    "    \"game_id\",\n",
    "    \"target_x\",\n",
    "    \"target_y\",\n",
    "]\n",
    "\n",
    "X_train_feat = X_train.drop(columns=drop_cols)\n",
    "X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns])\n",
    "\n",
    "# NaN 채우기 (LGBM은 NaN 다루긴 하지만, 깔끔하게)\n",
    "X_train_feat = X_train_feat.fillna(0)\n",
    "X_test_feat = X_test_feat.fillna(0)\n",
    "\n",
    "# ----------------------\n",
    "# 11. AutoGluon 학습\n",
    "# ----------------------\n",
    "# 빠른 테스트 모드: CatBoost와 LightGBM만 사용 (10~20분)\n",
    "# 새로운 피처 추가/변경 시 빠르게 테스트하기 위한 설정\n",
    "FAST_TEST_MODE = True  # False로 변경하면 모든 모델 사용 (best_quality)\n",
    "\n",
    "if FAST_TEST_MODE:\n",
    "    # CatBoost와 LightGBM만 선택\n",
    "    hyperparameters = {\n",
    "        'CAT': {},\n",
    "        'GBM': {},  # LightGBM\n",
    "    }\n",
    "    time_limit = 600  # 10분 (10분: 600, 15분: 900, 20분: 1200)\n",
    "    presets = \"good_quality\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"빠른 테스트 모드: CatBoost + LightGBM만 사용 (10분)\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    # 모든 모델 사용 (최종 제출용)\n",
    "    hyperparameters = None\n",
    "    time_limit = 1800  # 30분\n",
    "    presets = \"best_quality\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"전체 모델 학습 모드: 모든 모델 사용 (30분)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# X 좌표 예측 모델\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"X 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_x = X_train_feat.copy()\n",
    "train_data_x[\"target_x\"] = y_train_x\n",
    "\n",
    "predictor_x = TabularPredictor(\n",
    "    label=\"target_x\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=\"ag_models_x_corner\"  # 모델 저장 경로 (코너 피처 추가 버전)\n",
    ").fit(\n",
    "    train_data=train_data_x,\n",
    "    time_limit=time_limit,\n",
    "    presets=presets,\n",
    "    hyperparameters=hyperparameters,  # 특정 모델만 선택\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Y 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_y = X_train_feat.copy()\n",
    "train_data_y[\"target_y\"] = y_train_y\n",
    "\n",
    "predictor_y = TabularPredictor(\n",
    "    label=\"target_y\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=\"ag_models_y_corner\"  # 모델 저장 경로 (코너 피처 추가 버전)\n",
    ").fit(\n",
    "    train_data=train_data_y,\n",
    "    time_limit=time_limit,\n",
    "    presets=presets,\n",
    "    hyperparameters=hyperparameters,  # 특정 모델만 선택\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 12. test 예측\n",
    "# ----------------------\n",
    "print(\"=\" * 50)\n",
    "print(\"Test 데이터 예측 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pred_x = predictor_x.predict(X_test_feat)\n",
    "pred_y = predictor_y.predict(X_test_feat)\n",
    "\n",
    "# 필드 범위로 클립\n",
    "pred_x = np.clip(pred_x, 0, 105)\n",
    "pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "# ----------------------\n",
    "# 13. submission 생성\n",
    "# ----------------------\n",
    "sub = sample_sub.copy()\n",
    "\n",
    "# X_test에는 game_episode가 있으니, test_index와 align\n",
    "pred_df = X_test[[\"game_episode\"]].copy()\n",
    "pred_df[\"end_x\"] = pred_x\n",
    "pred_df[\"end_y\"] = pred_y\n",
    "\n",
    "sub = sub.drop(columns=[\"end_x\", \"end_y\"], errors=\"ignore\")\n",
    "sub = sub.merge(pred_df, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "sub.to_csv(\"submission_autogluon_corner.csv\", index=False)\n",
    "print(\"Saved submission_autogluon_corner.csv\")\n",
    "\n",
    "# ----------------------\n",
    "# 14. 백업 생성 (모델 및 제출 파일)\n",
    "# ----------------------\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "backup_dir = \"backups\"\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_subdir = os.path.join(backup_dir, f\"corner_features_{timestamp}\")\n",
    "os.makedirs(backup_subdir, exist_ok=True)\n",
    "\n",
    "# 모델 백업\n",
    "if os.path.exists(\"ag_models_x_corner\"):\n",
    "    shutil.copytree(\"ag_models_x_corner\", os.path.join(backup_subdir, \"ag_models_x_corner\"))\n",
    "    print(f\"✓ X 모델 백업 완료: {backup_subdir}/ag_models_x_corner\")\n",
    "\n",
    "if os.path.exists(\"ag_models_y_corner\"):\n",
    "    shutil.copytree(\"ag_models_y_corner\", os.path.join(backup_subdir, \"ag_models_y_corner\"))\n",
    "    print(f\"✓ Y 모델 백업 완료: {backup_subdir}/ag_models_y_corner\")\n",
    "\n",
    "# 제출 파일 백업\n",
    "if os.path.exists(\"submission_autogluon_corner.csv\"):\n",
    "    shutil.copy(\"submission_autogluon_corner.csv\", os.path.join(backup_subdir, \"submission_autogluon_corner.csv\"))\n",
    "    print(f\"✓ 제출 파일 백업 완료: {backup_subdir}/submission_autogluon_corner.csv\")\n",
    "\n",
    "print(f\"\\n백업 완료: {backup_subdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11416c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "모델 성능 확인\n",
      "==================================================\n",
      "\n",
      "[X 좌표 모델 - 리더보드]\n",
      "                      model  score_val              eval_metric  \\\n",
      "0       WeightedEnsemble_L2 -11.794978  root_mean_squared_error   \n",
      "1       WeightedEnsemble_L3 -11.795724  root_mean_squared_error   \n",
      "2           CatBoost_BAG_L2 -11.852409  root_mean_squared_error   \n",
      "3           CatBoost_BAG_L1 -11.855368  root_mean_squared_error   \n",
      "4           LightGBM_BAG_L1 -11.887144  root_mean_squared_error   \n",
      "5           LightGBM_BAG_L2 -11.908741  root_mean_squared_error   \n",
      "6  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
      "7  WeightedEnsemble_L2_FULL        NaN  root_mean_squared_error   \n",
      "8      LightGBM_BAG_L2_FULL        NaN  root_mean_squared_error   \n",
      "9      LightGBM_BAG_L1_FULL        NaN  root_mean_squared_error   \n",
      "\n",
      "   pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
      "0       0.200022   96.659272                0.000258           0.007887   \n",
      "1       0.234547  135.566955                0.000268           0.009845   \n",
      "2       0.234279  135.557110                0.034515          38.905725   \n",
      "3       0.043607   80.358840                0.043607          80.358840   \n",
      "4       0.156158   16.292545                0.156158          16.292545   \n",
      "5       0.263487  107.956742                0.063723          11.305357   \n",
      "6            NaN   12.632240                     NaN           0.009845   \n",
      "7            NaN   10.240267                     NaN           0.007887   \n",
      "8            NaN   11.541891                     NaN           1.309511   \n",
      "9            NaN    2.590872                     NaN           2.590872   \n",
      "\n",
      "   stack_level  can_infer  fit_order  \n",
      "0            2      False          3  \n",
      "1            3      False          6  \n",
      "2            2      False          5  \n",
      "3            1      False          2  \n",
      "4            1      False          1  \n",
      "5            2      False          4  \n",
      "6            3       True         12  \n",
      "7            2       True          9  \n",
      "8            2       True         10  \n",
      "9            1       True          7  \n",
      "\n",
      "[Y 좌표 모델 - 리더보드]\n",
      "                      model  score_val              eval_metric  \\\n",
      "0       WeightedEnsemble_L3 -13.080747  root_mean_squared_error   \n",
      "1       WeightedEnsemble_L2 -13.081222  root_mean_squared_error   \n",
      "2           CatBoost_BAG_L2 -13.123653  root_mean_squared_error   \n",
      "3           LightGBM_BAG_L1 -13.136993  root_mean_squared_error   \n",
      "4           CatBoost_BAG_L1 -13.163446  root_mean_squared_error   \n",
      "5           LightGBM_BAG_L2 -13.198637  root_mean_squared_error   \n",
      "6  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
      "7  WeightedEnsemble_L2_FULL        NaN  root_mean_squared_error   \n",
      "8      LightGBM_BAG_L2_FULL        NaN  root_mean_squared_error   \n",
      "9      LightGBM_BAG_L1_FULL        NaN  root_mean_squared_error   \n",
      "\n",
      "   pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
      "0       0.370964  186.871002                0.000320           0.011737   \n",
      "1       0.170204  115.335331                0.000308           0.011267   \n",
      "2       0.241820  168.786332                0.071924          53.462268   \n",
      "3       0.094806   12.598685                0.094806          12.598685   \n",
      "4       0.075090  102.725380                0.075090         102.725380   \n",
      "5       0.298720  133.396997                0.128824          18.072933   \n",
      "6            NaN   16.085638                     NaN           0.011737   \n",
      "7            NaN   12.422236                     NaN           0.011267   \n",
      "8            NaN   13.916911                     NaN           1.505942   \n",
      "9            NaN    2.353684                     NaN           2.353684   \n",
      "\n",
      "   stack_level  can_infer  fit_order  \n",
      "0            3      False          6  \n",
      "1            2      False          3  \n",
      "2            2      False          5  \n",
      "3            1      False          1  \n",
      "4            1      False          2  \n",
      "5            2      False          4  \n",
      "6            3       True         12  \n",
      "7            2       True          9  \n",
      "8            2       True         10  \n",
      "9            1       True          7  \n",
      "\n",
      "==================================================\n",
      "Train 데이터 성능 평가\n",
      "==================================================\n",
      "\n",
      "X 좌표 RMSE (Train): 9.8269\n",
      "Y 좌표 RMSE (Train): 11.0701\n",
      "\n",
      "==================================================\n",
      "제출 파일 확인\n",
      "==================================================\n",
      "\n",
      "제출 파일 행 수: 2414\n",
      "\n",
      "제출 파일 샘플:\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  65.197620  12.400709\n",
      "1     153363_2  31.849530  51.872770\n",
      "2     153363_6  36.991020  61.805996\n",
      "3     153363_7  53.802883   9.591954\n",
      "4     153363_8  80.905876  12.403799\n",
      "5     153363_9  75.528595  64.819190\n",
      "6    153363_10  65.005260  13.403336\n",
      "7    153363_12  71.343140  10.068212\n",
      "8    153363_13  32.080220  62.612720\n",
      "9    153363_15  73.991646  12.675773\n",
      "\n",
      "제출 파일 통계:\n",
      "             end_x        end_y\n",
      "count  2414.000000  2414.000000\n",
      "mean     67.067293    33.759323\n",
      "std      20.647933    20.181764\n",
      "min       7.984481     1.513225\n",
      "25%      52.250754    13.354973\n",
      "50%      71.580835    34.225417\n",
      "75%      84.573495    53.773434\n",
      "max     100.572860    67.255550\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 결과 확인 (학습 완료 후 실행)\n",
    "# ----------------------\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 저장된 모델 로드 (코너 피처 추가 버전)\n",
    "predictor_x = TabularPredictor.load(\"ag_models_x_corner\")\n",
    "predictor_y = TabularPredictor.load(\"ag_models_y_corner\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"모델 성능 확인\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# X 좌표 모델 리더보드\n",
    "print(\"\\n[X 좌표 모델 - 리더보드]\")\n",
    "leaderboard_x = predictor_x.leaderboard(silent=True)\n",
    "print(leaderboard_x.head(10))\n",
    "\n",
    "# Y 좌표 모델 리더보드\n",
    "print(\"\\n[Y 좌표 모델 - 리더보드]\")\n",
    "leaderboard_y = predictor_y.leaderboard(silent=True)\n",
    "print(leaderboard_y.head(10))\n",
    "\n",
    "# Train 데이터로 성능 평가 (첫 번째 셀 실행 후 사용 가능)\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Train 데이터 성능 평가\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # X 좌표 평가\n",
    "    y_pred_train_x = predictor_x.predict(X_train_feat)\n",
    "    rmse_x = np.sqrt(np.mean((y_train_x - y_pred_train_x) ** 2))\n",
    "    print(f\"\\nX 좌표 RMSE (Train): {rmse_x:.4f}\")\n",
    "    \n",
    "    # Y 좌표 평가\n",
    "    y_pred_train_y = predictor_y.predict(X_train_feat)\n",
    "    rmse_y = np.sqrt(np.mean((y_train_y - y_pred_train_y) ** 2))\n",
    "    print(f\"Y 좌표 RMSE (Train): {rmse_y:.4f}\")\n",
    "except NameError:\n",
    "    print(\"\\n(첫 번째 셀을 먼저 실행해야 Train 데이터 평가가 가능합니다)\")\n",
    "\n",
    "# 제출 파일 확인\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"제출 파일 확인\")\n",
    "print(\"=\" * 50)\n",
    "sub = pd.read_csv(\"submission_autogluon_corner.csv\")\n",
    "print(f\"\\n제출 파일 행 수: {len(sub)}\")\n",
    "print(f\"\\n제출 파일 샘플:\")\n",
    "print(sub.head(10))\n",
    "print(f\"\\n제출 파일 통계:\")\n",
    "print(sub.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
