{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8477ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_name 고유값 개수: 26\n",
      "type_name 샘플 (상위 20개): ['Pass' 'Carry' 'Interception' 'Clearance' 'Duel' 'Recovery'\n",
      " 'Intervention' 'Pass_Corner' 'Goal Kick' 'Tackle' 'Error' 'Take-On'\n",
      " 'Throw-In' 'Pass_Freekick' 'Cross' 'Block' 'Shot' 'Parry'\n",
      " 'Aerial Clearance' 'Catch']\n",
      "\n",
      "Cross 관련 type_name 발견: True\n",
      "Long Ball 관련 type_name 발견: True\n",
      "\n",
      "✅ A안 적용: type_id 기반 실제 이벤트 타입 사용\n",
      "\n",
      "✅ A안: Cross type_id = [5]\n",
      "✅ A안: LongBall type_id = []\n",
      "Cross 이벤트 개수: 4099\n",
      "LongBall 이벤트 개수: 0\n",
      "✅ A안: Score 및 Interaction 피처 생성 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/640807727.py:310: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 세트피스 킥 피처 추가 완료 (24개 숫자형 피처)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/640807727.py:437: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train_feat.fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_20885/640807727.py:438: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test_feat.fillna(0)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       2.69 GB / 16.00 GB (16.8%)\n",
      "Disk Space Avail:   5.31 GB / 460.43 GB (1.2%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "빠른 테스트 모드: CatBoost + LightGBM만 사용 (10분)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "X 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_typeid_based/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 602\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2821.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.79 MB (2.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 83 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 86): ['angle_x_cross_score_19', 'cross_score_19', 'dist_x_longball_score_0', 'dist_x_longball_score_1', 'dist_x_longball_score_2', 'dist_x_longball_score_3', 'dist_x_longball_score_4', 'dist_x_longball_score_5', 'dist_x_longball_score_6', 'dist_x_longball_score_7', 'dist_x_longball_score_8', 'dist_x_longball_score_9', 'dist_x_longball_score_10', 'dist_x_longball_score_11', 'dist_x_longball_score_12', 'dist_x_longball_score_13', 'dist_x_longball_score_14', 'dist_x_longball_score_15', 'dist_x_longball_score_16', 'dist_x_longball_score_17', 'dist_x_longball_score_18', 'dist_x_longball_score_19', 'dx_x_longball_score_0', 'dx_x_longball_score_1', 'dx_x_longball_score_2', 'dx_x_longball_score_3', 'dx_x_longball_score_4', 'dx_x_longball_score_5', 'dx_x_longball_score_6', 'dx_x_longball_score_7', 'dx_x_longball_score_8', 'dx_x_longball_score_9', 'dx_x_longball_score_10', 'dx_x_longball_score_11', 'dx_x_longball_score_12', 'dx_x_longball_score_13', 'dx_x_longball_score_14', 'dx_x_longball_score_15', 'dx_x_longball_score_16', 'dx_x_longball_score_17', 'dx_x_longball_score_18', 'dx_x_longball_score_19', 'is_final_team_19', 'long_ball_score_0', 'long_ball_score_1', 'long_ball_score_2', 'long_ball_score_3', 'long_ball_score_4', 'long_ball_score_5', 'long_ball_score_6', 'long_ball_score_7', 'long_ball_score_8', 'long_ball_score_9', 'long_ball_score_10', 'long_ball_score_11', 'long_ball_score_12', 'long_ball_score_13', 'long_ball_score_14', 'long_ball_score_15', 'long_ball_score_16', 'long_ball_score_17', 'long_ball_score_18', 'long_ball_score_19', 'start_x_x_cross_score_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19', 'has_long_ball_in_K', 'avg_longball_score_in_K']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 493 | ['angle_to_goal_0', 'angle_to_goal_1', 'angle_to_goal_2', 'angle_to_goal_3', 'angle_to_goal_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 432 | ['angle_to_goal_0', 'angle_to_goal_1', 'angle_to_goal_2', 'angle_to_goal_3', 'angle_to_goal_4', ...]\n",
      "\t\t('int', ['bool']) :  81 | ['cross_score_0', 'cross_score_1', 'cross_score_2', 'cross_score_3', 'cross_score_4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t513 features in original data used to generate 513 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.28 MB (1.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 99.55s of the 149.36s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.81% memory usage per fold, 59.24%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=14.81%)\n",
      "\t-11.9294\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.55s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 75.82s of the 125.63s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.61% memory usage per fold, 69.22%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=34.61%)\n",
      "\t-11.8899\t = Validation score   (-root_mean_squared_error)\n",
      "\t66.25s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.36s of the 57.13s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.5 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.562, 'LightGBM_BAG_L1': 0.438}\n",
      "\t-11.8294\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 57.10s of the 57.03s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.37% memory usage per fold, 41.50%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.37%)\n",
      "\t-11.9667\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.22s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 41.54s of the 41.47s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.74% memory usage per fold, 65.48%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=32.74%)\n",
      "\t-11.9136\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.1s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.36s of the 1.61s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.2 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.562, 'LightGBM_BAG_L1': 0.438}\n",
      "\t-11.8294\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 148.49s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3608.7 rows/s (1715 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.8 GB\n",
      "\t2.71s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.1/3.6 GB\n",
      "\t6.37s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.562, 'LightGBM_BAG_L1': 0.438}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.5 GB\n",
      "\t1.62s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.1/3.5 GB\n",
      "\t2.9s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.562, 'LightGBM_BAG_L1': 0.438}\n",
      "\t0.03s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 14.18s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_typeid_based/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val   fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2_FULL     -11.780192 -11.829440  root_mean_squared_error        0.025451          None   9.093136                 0.001053                   None           0.010741            2       True          3\n",
      "1  WeightedEnsemble_L3_FULL     -11.780192 -11.829440  root_mean_squared_error        0.025545          None   9.115973                 0.001147                   None           0.033578            3       True          6\n",
      "2      CatBoost_BAG_L2_FULL     -11.789096 -11.913592  root_mean_squared_error        0.034196          None  11.987343                 0.009798                   None           2.904948            2       True          5\n",
      "3      CatBoost_BAG_L1_FULL     -11.828769 -11.889885  root_mean_squared_error        0.013850          None   6.374441                 0.013850                   None           6.374441            1       True          2\n",
      "4      LightGBM_BAG_L2_FULL     -11.866795 -11.966660  root_mean_squared_error        0.030495          None  10.703233                 0.006097                   None           1.620837            2       True          4\n",
      "5      LightGBM_BAG_L1_FULL     -11.887535 -11.929389  root_mean_squared_error        0.010548          None   2.707954                 0.010548                   None           2.707954            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t163s\t = DyStack   runtime |\t437s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 437s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_typeid_based\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 602\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3613.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 78.52 MB (2.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 83 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 86): ['angle_x_cross_score_19', 'cross_score_19', 'dist_x_longball_score_0', 'dist_x_longball_score_1', 'dist_x_longball_score_2', 'dist_x_longball_score_3', 'dist_x_longball_score_4', 'dist_x_longball_score_5', 'dist_x_longball_score_6', 'dist_x_longball_score_7', 'dist_x_longball_score_8', 'dist_x_longball_score_9', 'dist_x_longball_score_10', 'dist_x_longball_score_11', 'dist_x_longball_score_12', 'dist_x_longball_score_13', 'dist_x_longball_score_14', 'dist_x_longball_score_15', 'dist_x_longball_score_16', 'dist_x_longball_score_17', 'dist_x_longball_score_18', 'dist_x_longball_score_19', 'dx_x_longball_score_0', 'dx_x_longball_score_1', 'dx_x_longball_score_2', 'dx_x_longball_score_3', 'dx_x_longball_score_4', 'dx_x_longball_score_5', 'dx_x_longball_score_6', 'dx_x_longball_score_7', 'dx_x_longball_score_8', 'dx_x_longball_score_9', 'dx_x_longball_score_10', 'dx_x_longball_score_11', 'dx_x_longball_score_12', 'dx_x_longball_score_13', 'dx_x_longball_score_14', 'dx_x_longball_score_15', 'dx_x_longball_score_16', 'dx_x_longball_score_17', 'dx_x_longball_score_18', 'dx_x_longball_score_19', 'is_final_team_19', 'long_ball_score_0', 'long_ball_score_1', 'long_ball_score_2', 'long_ball_score_3', 'long_ball_score_4', 'long_ball_score_5', 'long_ball_score_6', 'long_ball_score_7', 'long_ball_score_8', 'long_ball_score_9', 'long_ball_score_10', 'long_ball_score_11', 'long_ball_score_12', 'long_ball_score_13', 'long_ball_score_14', 'long_ball_score_15', 'long_ball_score_16', 'long_ball_score_17', 'long_ball_score_18', 'long_ball_score_19', 'start_x_x_cross_score_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19', 'has_long_ball_in_K', 'avg_longball_score_in_K']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_to_goal_0', 'angle_to_goal_1', 'angle_to_goal_2', 'angle_to_goal_3', 'angle_to_goal_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 432 | ['angle_to_goal_0', 'angle_to_goal_1', 'angle_to_goal_2', 'angle_to_goal_3', 'angle_to_goal_4', ...]\n",
      "\t\t('int', ['bool']) :  80 | ['cross_score_0', 'cross_score_1', 'cross_score_2', 'cross_score_3', 'cross_score_4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 52.05 MB (1.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 290.56s of the 435.95s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.62% memory usage per fold, 50.47%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=12.62%)\n",
      "\t-11.8918\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.2s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 268.01s of the 413.40s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.42% memory usage per fold, 70.85%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=35.42%)\n",
      "\t-11.855\t = Validation score   (-root_mean_squared_error)\n",
      "\t94.42s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 317.26s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.2 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.56, 'LightGBM_BAG_L1': 0.44}\n",
      "\t-11.7929\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 317.22s of the 317.16s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.10% memory usage per fold, 52.40%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=13.10%)\n",
      "\t-11.9104\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.03s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 301.93s of the 301.87s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 38.79% memory usage per fold, 77.57%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=38.79%)\n",
      "\t-11.8606\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.78s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 253.41s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.2 GB\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.56, 'LightGBM_BAG_L1': 0.44}\n",
      "\t-11.7929\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 183.33s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7196.8 rows/s (1930 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.4/3.2 GB\n",
      "\t3.29s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.1/3.3 GB\n",
      "\t8.48s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.56, 'LightGBM_BAG_L1': 0.44}\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.4/3.5 GB\n",
      "\t1.41s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.1/3.5 GB\n",
      "\t2.51s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.56, 'LightGBM_BAG_L1': 0.44}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 16.16s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_typeid_based\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       3.54 GB / 16.00 GB (22.1%)\n",
      "Disk Space Avail:   4.17 GB / 460.43 GB (0.9%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Y 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_typeid_based/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 602\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3695.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.79 MB (1.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 83 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 86): ['angle_x_cross_score_19', 'cross_score_19', 'dist_x_longball_score_0', 'dist_x_longball_score_1', 'dist_x_longball_score_2', 'dist_x_longball_score_3', 'dist_x_longball_score_4', 'dist_x_longball_score_5', 'dist_x_longball_score_6', 'dist_x_longball_score_7', 'dist_x_longball_score_8', 'dist_x_longball_score_9', 'dist_x_longball_score_10', 'dist_x_longball_score_11', 'dist_x_longball_score_12', 'dist_x_longball_score_13', 'dist_x_longball_score_14', 'dist_x_longball_score_15', 'dist_x_longball_score_16', 'dist_x_longball_score_17', 'dist_x_longball_score_18', 'dist_x_longball_score_19', 'dx_x_longball_score_0', 'dx_x_longball_score_1', 'dx_x_longball_score_2', 'dx_x_longball_score_3', 'dx_x_longball_score_4', 'dx_x_longball_score_5', 'dx_x_longball_score_6', 'dx_x_longball_score_7', 'dx_x_longball_score_8', 'dx_x_longball_score_9', 'dx_x_longball_score_10', 'dx_x_longball_score_11', 'dx_x_longball_score_12', 'dx_x_longball_score_13', 'dx_x_longball_score_14', 'dx_x_longball_score_15', 'dx_x_longball_score_16', 'dx_x_longball_score_17', 'dx_x_longball_score_18', 'dx_x_longball_score_19', 'is_final_team_19', 'long_ball_score_0', 'long_ball_score_1', 'long_ball_score_2', 'long_ball_score_3', 'long_ball_score_4', 'long_ball_score_5', 'long_ball_score_6', 'long_ball_score_7', 'long_ball_score_8', 'long_ball_score_9', 'long_ball_score_10', 'long_ball_score_11', 'long_ball_score_12', 'long_ball_score_13', 'long_ball_score_14', 'long_ball_score_15', 'long_ball_score_16', 'long_ball_score_17', 'long_ball_score_18', 'long_ball_score_19', 'start_x_x_cross_score_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19', 'has_long_ball_in_K', 'avg_longball_score_in_K']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 493 | ['angle_to_goal_0', 'angle_to_goal_1', 'angle_to_goal_2', 'angle_to_goal_3', 'angle_to_goal_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 432 | ['angle_to_goal_0', 'angle_to_goal_1', 'angle_to_goal_2', 'angle_to_goal_3', 'angle_to_goal_4', ...]\n",
      "\t\t('int', ['bool']) :  81 | ['cross_score_0', 'cross_score_1', 'cross_score_2', 'cross_score_3', 'cross_score_4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t513 features in original data used to generate 513 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.28 MB (1.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 99.60s of the 149.44s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.25% memory usage per fold, 45.00%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.25%)\n",
      "\t-13.1251\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.23s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 81.05s of the 130.89s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.71% memory usage per fold, 59.42%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=29.71%)\n",
      "\t-13.1686\t = Validation score   (-root_mean_squared_error)\n",
      "\t69.35s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.44s of the 59.88s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.5 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.579, 'CatBoost_BAG_L1': 0.421}\n",
      "\t-13.0751\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 59.84s of the 59.82s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.19% memory usage per fold, 44.75%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.19%)\n",
      "\t-13.1899\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.56s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 42.77s of the 42.75s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.04% memory usage per fold, 56.08%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=28.04%)\n",
      "\t-13.1286\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.3s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.44s of the 1.67s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.4 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.533, 'CatBoost_BAG_L1': 0.4, 'LightGBM_BAG_L2': 0.067}\n",
      "\t-13.0744\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 148.62s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 6104.0 rows/s (1715 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.4 GB\n",
      "\t2.09s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.1/3.6 GB\n",
      "\t8.48s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.579, 'CatBoost_BAG_L1': 0.421}\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.7 GB\n",
      "\t1.41s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.1/3.7 GB\n",
      "\t2.36s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.533, 'CatBoost_BAG_L1': 0.4, 'LightGBM_BAG_L2': 0.067}\n",
      "\t0.07s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 14.83s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_typeid_based/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val   fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      LightGBM_BAG_L2_FULL     -13.389924 -13.189853  root_mean_squared_error        0.025953          None  11.982565                 0.006153                   None           1.414126            2       True          4\n",
      "1  WeightedEnsemble_L3_FULL     -13.426123 -13.074369  root_mean_squared_error        0.027068          None  12.047771                 0.001115                   None           0.065206            3       True          6\n",
      "2      CatBoost_BAG_L2_FULL     -13.430961 -13.128582  root_mean_squared_error        0.030543          None  12.928971                 0.010743                   None           2.360532            2       True          5\n",
      "3  WeightedEnsemble_L2_FULL     -13.437779 -13.075106  root_mean_squared_error        0.021433          None  10.585700                 0.001633                   None           0.017261            2       True          3\n",
      "4      LightGBM_BAG_L1_FULL     -13.494372 -13.125083  root_mean_squared_error        0.008635          None   2.087783                 0.008635                   None           2.087783            1       True          1\n",
      "5      CatBoost_BAG_L1_FULL     -13.506956 -13.168574  root_mean_squared_error        0.011165          None   8.480656                 0.011165                   None           8.480656            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t164s\t = DyStack   runtime |\t436s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 436s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_typeid_based\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 602\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3839.70 MB\n",
      "\tTrain Data (Original)  Memory Usage: 78.52 MB (2.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 83 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 86): ['angle_x_cross_score_19', 'cross_score_19', 'dist_x_longball_score_0', 'dist_x_longball_score_1', 'dist_x_longball_score_2', 'dist_x_longball_score_3', 'dist_x_longball_score_4', 'dist_x_longball_score_5', 'dist_x_longball_score_6', 'dist_x_longball_score_7', 'dist_x_longball_score_8', 'dist_x_longball_score_9', 'dist_x_longball_score_10', 'dist_x_longball_score_11', 'dist_x_longball_score_12', 'dist_x_longball_score_13', 'dist_x_longball_score_14', 'dist_x_longball_score_15', 'dist_x_longball_score_16', 'dist_x_longball_score_17', 'dist_x_longball_score_18', 'dist_x_longball_score_19', 'dx_x_longball_score_0', 'dx_x_longball_score_1', 'dx_x_longball_score_2', 'dx_x_longball_score_3', 'dx_x_longball_score_4', 'dx_x_longball_score_5', 'dx_x_longball_score_6', 'dx_x_longball_score_7', 'dx_x_longball_score_8', 'dx_x_longball_score_9', 'dx_x_longball_score_10', 'dx_x_longball_score_11', 'dx_x_longball_score_12', 'dx_x_longball_score_13', 'dx_x_longball_score_14', 'dx_x_longball_score_15', 'dx_x_longball_score_16', 'dx_x_longball_score_17', 'dx_x_longball_score_18', 'dx_x_longball_score_19', 'is_final_team_19', 'long_ball_score_0', 'long_ball_score_1', 'long_ball_score_2', 'long_ball_score_3', 'long_ball_score_4', 'long_ball_score_5', 'long_ball_score_6', 'long_ball_score_7', 'long_ball_score_8', 'long_ball_score_9', 'long_ball_score_10', 'long_ball_score_11', 'long_ball_score_12', 'long_ball_score_13', 'long_ball_score_14', 'long_ball_score_15', 'long_ball_score_16', 'long_ball_score_17', 'long_ball_score_18', 'long_ball_score_19', 'start_x_x_cross_score_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19', 'has_long_ball_in_K', 'avg_longball_score_in_K']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_to_goal_0', 'angle_to_goal_1', 'angle_to_goal_2', 'angle_to_goal_3', 'angle_to_goal_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 432 | ['angle_to_goal_0', 'angle_to_goal_1', 'angle_to_goal_2', 'angle_to_goal_3', 'angle_to_goal_4', ...]\n",
      "\t\t('int', ['bool']) :  80 | ['cross_score_0', 'cross_score_1', 'cross_score_2', 'cross_score_3', 'cross_score_4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 52.05 MB (1.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 290.04s of the 435.17s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.87% memory usage per fold, 47.46%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=11.87%)\n",
      "\t-13.1275\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.64s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 273.01s of the 418.14s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.27% memory usage per fold, 58.55%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=29.27%)\n",
      "\t-13.1717\t = Validation score   (-root_mean_squared_error)\n",
      "\t105.91s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 310.25s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/2.6 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.579, 'CatBoost_BAG_L1': 0.421}\n",
      "\t-13.0774\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 310.12s of the 310.04s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.61% memory usage per fold, 62.45%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=15.61%)\n",
      "\t-13.2076\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.44s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 290.07s of the 289.99s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.19% memory usage per fold, 64.39%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=32.19%)\n",
      "\t-13.1199\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.94s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 244.27s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.9 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.56, 'CatBoost_BAG_L1': 0.4, 'CatBoost_BAG_L2': 0.04}\n",
      "\t-13.0776\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 191.83s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 8641.7 rows/s (1930 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.4/3.7 GB\n",
      "\t2.02s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.1/3.8 GB\n",
      "\t9.34s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.579, 'CatBoost_BAG_L1': 0.421}\n",
      "\t0.06s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.4/4.0 GB\n",
      "\t2.11s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=1.1/4.0 GB\n",
      "\t2.21s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.56, 'CatBoost_BAG_L1': 0.4, 'CatBoost_BAG_L2': 0.04}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 16.25s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_typeid_based\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test 데이터 예측 중...\n",
      "==================================================\n",
      "Saved submission_autogluon_typeid_based.csv\n",
      "✓ X 모델 백업 완료: backups/typeid_based_20251226_010150/ag_models_x_typeid_based\n",
      "✓ Y 모델 백업 완료: backups/typeid_based_20251226_010150/ag_models_y_typeid_based\n",
      "✓ 제출 파일 백업 완료: backups/typeid_based_20251226_010150/submission_autogluon_typeid_based.csv\n",
      "\n",
      "백업 완료: backups/typeid_based_20251226_010150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# ----------------------\n",
    "# 0. 설정\n",
    "# ----------------------\n",
    "BASE_PATH = \"open_track1/\"\n",
    "PATH_TRAIN = os.path.join(BASE_PATH, \"train.csv\")\n",
    "PATH_TEST = os.path.join(BASE_PATH, \"test.csv\")\n",
    "PATH_MATCH_INFO = os.path.join(BASE_PATH, \"match_info.csv\")\n",
    "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "K = 20   # 마지막 K 이벤트 사용 (20~32 사이 선택)\n",
    "\n",
    "# ----------------------\n",
    "# 1. 데이터 로드\n",
    "# ----------------------\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test_index = pd.read_csv(PATH_TEST)\n",
    "match_info = pd.read_csv(PATH_MATCH_INFO)\n",
    "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
    "\n",
    "test_events_list = []\n",
    "for _, row in test_index.iterrows():\n",
    "    # path가 \"./test/...\" 형식이므로 BASE_PATH와 결합\n",
    "    test_path = os.path.join(BASE_PATH, row[\"path\"].lstrip(\"./\"))\n",
    "    df_ep = pd.read_csv(test_path)\n",
    "    test_events_list.append(df_ep)\n",
    "\n",
    "test_events = pd.concat(test_events_list, ignore_index=True)\n",
    "\n",
    "train[\"is_train\"] = 1\n",
    "test_events[\"is_train\"] = 0\n",
    "\n",
    "events = pd.concat([train, test_events], ignore_index=True)\n",
    "\n",
    "# ----------------------\n",
    "# 2. 기본 정렬 + episode 내 인덱스\n",
    "# ----------------------\n",
    "events = events.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
    "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
    "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
    "\n",
    "# ----------------------\n",
    "# 3. 시간/공간 feature\n",
    "# ----------------------\n",
    "# Δt\n",
    "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
    "events[\"dt\"] = events[\"time_seconds\"] - events[\"prev_time\"]\n",
    "events[\"dt\"] = events[\"dt\"].fillna(0.0)\n",
    "\n",
    "# 이동량/거리\n",
    "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
    "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
    "events[\"dist\"] = np.sqrt(events[\"dx\"]**2 + events[\"dy\"]**2)\n",
    "\n",
    "# 속도 (dt=0 보호)\n",
    "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
    "\n",
    "# zone / lane (필요시 범위 조정)\n",
    "events[\"x_zone\"] = (events[\"start_x\"] / (105/7)).astype(int).clip(0, 6)\n",
    "events[\"lane\"] = pd.cut(\n",
    "    events[\"start_y\"],\n",
    "    bins=[0, 68/3, 2*68/3, 68],\n",
    "    labels=[0, 1, 2],\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 3-1. 코너 관련 피처 (Corner Features)\n",
    "# ----------------------\n",
    "# 골대까지 각도 계산 (라디안 → 도)\n",
    "# 골대 중앙: (105, 34)\n",
    "events[\"angle_to_goal\"] = np.arctan2(\n",
    "    34 - events[\"start_y\"],\n",
    "    105 - events[\"start_x\"]\n",
    ") * 180 / np.pi\n",
    "\n",
    "# 코너까지 최단 거리 계산\n",
    "# 공격 방향 코너: (105, 0) 상단, (105, 68) 하단\n",
    "events[\"dist_corner_top\"] = np.sqrt((105 - events[\"start_x\"])**2 + (0 - events[\"start_y\"])**2)\n",
    "events[\"dist_corner_bottom\"] = np.sqrt((105 - events[\"start_x\"])**2 + (68 - events[\"start_y\"])**2)\n",
    "events[\"dist_to_nearest_corner\"] = events[[\"dist_corner_top\", \"dist_corner_bottom\"]].min(axis=1)\n",
    "\n",
    "# 코너 구역 플래그 (X > 100 이면서 Y < 5 또는 Y > 63)\n",
    "events[\"is_corner_area\"] = ((events[\"start_x\"] > 100) & \n",
    "                            ((events[\"start_y\"] < 5) | (events[\"start_y\"] > 63))).astype(int)\n",
    "\n",
    "# 인터랙션 피처 1: angle_to_goal × is_corner_area\n",
    "# 코너 구역일 때만 각도의 영향이 강하게 나타남\n",
    "events[\"angle_x_is_corner\"] = events[\"angle_to_goal\"] * events[\"is_corner_area\"]\n",
    "\n",
    "# 인터랙션 피처 2: dist_to_nearest_corner × angle_to_goal\n",
    "# 코너 거리와 각도의 조합\n",
    "events[\"corner_dist_x_angle\"] = events[\"dist_to_nearest_corner\"] * events[\"angle_to_goal\"]\n",
    "\n",
    "# ----------------------\n",
    "# 3-2. 세트피스 킥 피처 (Set-Piece Kick Features)\n",
    "# ----------------------\n",
    "# 전략: type_id 기반 (A안) 또는 Score 기반 (B안) 선택\n",
    "# 먼저 type_name에 실제 이벤트 타입이 있는지 확인\n",
    "\n",
    "# type_name 확인 (임시로 type_id 인코딩 전에 확인)\n",
    "type_name_unique = events[\"type_name\"].unique()\n",
    "print(f\"type_name 고유값 개수: {len(type_name_unique)}\")\n",
    "print(f\"type_name 샘플 (상위 20개): {type_name_unique[:20]}\")\n",
    "\n",
    "# Cross, Long Ball 관련 키워드 검색 (대소문자 무시)\n",
    "cross_keywords = [\"cross\", \"Cross\", \"CROSS\"]\n",
    "longball_keywords = [\"long\", \"Long\", \"LONG\", \"ball\", \"Ball\", \"BALL\"]\n",
    "\n",
    "has_cross_type = any(any(kw in str(tn) for kw in cross_keywords) for tn in type_name_unique)\n",
    "has_longball_type = any(any(kw in str(tn) for kw in longball_keywords) for tn in type_name_unique)\n",
    "\n",
    "print(f\"\\nCross 관련 type_name 발견: {has_cross_type}\")\n",
    "print(f\"Long Ball 관련 type_name 발견: {has_longball_type}\")\n",
    "\n",
    "# A안: type_id 기반 (가장 추천) - 실제 이벤트 타입 사용\n",
    "if has_cross_type or has_longball_type:\n",
    "    print(\"\\n✅ A안 적용: type_id 기반 실제 이벤트 타입 사용\")\n",
    "    \n",
    "    # type_id 인코딩 후에 매핑 (아래 7번 섹션에서 인코딩됨)\n",
    "    # 여기서는 플래그만 설정하고, 7번 섹션 이후에 실제 매핑 수행\n",
    "    USE_TYPE_ID_BASED = True\n",
    "else:\n",
    "    print(\"\\n⚠️  A안 불가: type_id에 Cross/LongBall 없음 → B안(Score 기반) 적용\")\n",
    "    USE_TYPE_ID_BASED = False\n",
    "\n",
    "# B안: Score 기반 (연속형) - 부드러운 확률 점수 사용\n",
    "if not USE_TYPE_ID_BASED:\n",
    "    # 1) cross_score (0~1): 크로스일 확률 점수\n",
    "    side = (events[\"lane\"].isin([0, 2])).astype(float)  # 측면이면 1\n",
    "    deep = (events[\"start_x\"] / 105).clip(0, 1)  # 깊을수록 1\n",
    "    near_corner = (1 / (events[\"dist_to_nearest_corner\"] + 1)).clip(0, 1)  # 코너 가까울수록 큼\n",
    "    open_angle = (np.abs(events[\"angle_to_goal\"]) / 180).clip(0, 1)  # 각도 정규화\n",
    "    \n",
    "    events[\"cross_score\"] = (0.4 * side + 0.2 * deep + 0.2 * near_corner + 0.2 * open_angle)\n",
    "    \n",
    "    # 2) long_ball_score (0~1): 롱볼일 확률 점수\n",
    "    # dist, dx가 유효한 경우만 계산\n",
    "    dist_max = events[\"dist\"].max()\n",
    "    if dist_max > 0:\n",
    "        dist_norm = (events[\"dist\"] / dist_max).clip(0, 1)\n",
    "    else:\n",
    "        dist_norm = pd.Series(0.0, index=events.index)\n",
    "    \n",
    "    dx_norm = (np.abs(events[\"dx\"]) / 105).clip(0, 1)\n",
    "    dt_inv = (1 / (events[\"dt\"] + 1)).clip(0, 1)\n",
    "    \n",
    "    # NaN 체크\n",
    "    valid_mask = events[\"dist\"].notna() & events[\"dx\"].notna()\n",
    "    events[\"long_ball_score\"] = np.where(\n",
    "        valid_mask,\n",
    "        0.5 * dist_norm + 0.3 * dx_norm + 0.2 * dt_inv,\n",
    "        0.0\n",
    "    )\n",
    "    \n",
    "    print(\"✅ B안 적용: cross_score, long_ball_score 생성 완료\")\n",
    "\n",
    "# ----------------------\n",
    "# 4. 라벨 및 episode-level 메타 (train 전용)\n",
    "# ----------------------\n",
    "train_events = events[events[\"is_train\"] == 1].copy()\n",
    "\n",
    "last_events = (\n",
    "    train_events\n",
    "    .groupby(\"game_episode\", as_index=False)\n",
    "    .tail(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "labels = last_events[[\"game_episode\", \"end_x\", \"end_y\"]].rename(\n",
    "    columns={\"end_x\": \"target_x\", \"end_y\": \"target_y\"}\n",
    ")\n",
    "\n",
    "# episode-level 메타 (마지막 이벤트 기준)\n",
    "ep_meta = last_events[[\"game_episode\", \"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"]].copy()\n",
    "ep_meta = ep_meta.rename(columns={\"team_id\": \"final_team_id\"})\n",
    "\n",
    "# game_clock (분 단위, 0~90+)\n",
    "ep_meta[\"game_clock_min\"] = np.where(\n",
    "    ep_meta[\"period_id\"] == 1,\n",
    "    ep_meta[\"time_seconds\"] / 60.0,\n",
    "    45.0 + ep_meta[\"time_seconds\"] / 60.0\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5. 공격 팀 플래그 (final_team vs 상대)\n",
    "# ----------------------\n",
    "# final_team_id를 전체 events에 붙임\n",
    "events = events.merge(\n",
    "    ep_meta[[\"game_episode\", \"final_team_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 6. 입력용 events에서 마지막 이벤트 타깃 정보 가리기\n",
    "# ----------------------\n",
    "# is_last 플래그\n",
    "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
    "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
    "\n",
    "# labels는 이미 뽑아놨으니, 입력쪽에서만 end_x, end_y, dx, dy, dist, speed 지움\n",
    "mask_last = events[\"is_last\"] == 1\n",
    "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
    "    events.loc[mask_last, col] = np.nan\n",
    "\n",
    "# ----------------------\n",
    "# 7. 카테고리 인코딩 (type_name, result_name, team_id 등)\n",
    "# ----------------------\n",
    "events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
    "events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA_RES__\")\n",
    "\n",
    "le_type = LabelEncoder()\n",
    "le_res = LabelEncoder()\n",
    "\n",
    "events[\"type_id\"] = le_type.fit_transform(events[\"type_name\"])\n",
    "events[\"res_id\"] = le_res.fit_transform(events[\"result_name\"])\n",
    "\n",
    "# A안: type_id 기반 실제 이벤트 타입 매핑 (type_id 인코딩 후)\n",
    "if USE_TYPE_ID_BASED:\n",
    "    # type_name → type_id 매핑: le_type.classes_는 배열이고, 인덱스가 type_id\n",
    "    # 예: le_type.classes_[0] = \"Pass\", le_type.classes_[1] = \"Cross\" → type_id 1이 Cross\n",
    "    \n",
    "    # Cross 관련 type_id 찾기\n",
    "    cross_type_ids = []\n",
    "    longball_type_ids = []\n",
    "    \n",
    "    for type_id in range(len(le_type.classes_)):\n",
    "        type_name = le_type.classes_[type_id]\n",
    "        type_name_str = str(type_name).lower()\n",
    "        if \"cross\" in type_name_str:\n",
    "            cross_type_ids.append(type_id)\n",
    "        if (\"long\" in type_name_str or \"ball\" in type_name_str) and \"long\" in type_name_str:\n",
    "            longball_type_ids.append(type_id)\n",
    "    \n",
    "    print(f\"\\n✅ A안: Cross type_id = {cross_type_ids}\")\n",
    "    print(f\"✅ A안: LongBall type_id = {longball_type_ids}\")\n",
    "    \n",
    "    # type_id 기반 이진 플래그 생성\n",
    "    events[\"is_cross\"] = events[\"type_id\"].isin(cross_type_ids).astype(int)\n",
    "    events[\"is_long_ball\"] = events[\"type_id\"].isin(longball_type_ids).astype(int)\n",
    "    \n",
    "    print(f\"Cross 이벤트 개수: {events['is_cross'].sum()}\")\n",
    "    print(f\"LongBall 이벤트 개수: {events['is_long_ball'].sum()}\")\n",
    "else:\n",
    "    # B안: Score 기반이므로 is_cross, is_long_ball은 생성하지 않음\n",
    "    # (cross_score, long_ball_score만 사용)\n",
    "    pass\n",
    "\n",
    "# team_id는 그대로 써도 되지만, 문자열이면 숫자로 매핑\n",
    "if events[\"team_id\"].dtype == \"object\":\n",
    "    le_team = LabelEncoder()\n",
    "    events[\"team_id_enc\"] = le_team.fit_transform(events[\"team_id\"])\n",
    "else:\n",
    "    events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 7-1. 세트피스 킥 Score 및 Interaction 피처 생성 (lastK 생성 전)\n",
    "# ----------------------\n",
    "# A안 또는 B안에 따라 Score 및 Interaction 피처 생성\n",
    "if USE_TYPE_ID_BASED:\n",
    "    # A안: type_id 기반 이진 플래그를 Score로 변환 + Interaction\n",
    "    events[\"cross_score\"] = events[\"is_cross\"].astype(float)  # 이진을 float로 변환\n",
    "    events[\"long_ball_score\"] = events[\"is_long_ball\"].astype(float)\n",
    "    \n",
    "    # Score 기반 interaction (게이팅)\n",
    "    events[\"start_x_x_cross_score\"] = events[\"start_x\"] * events[\"cross_score\"]\n",
    "    events[\"angle_x_cross_score\"] = events[\"angle_to_goal\"] * events[\"cross_score\"]\n",
    "    events[\"dist_x_longball_score\"] = events[\"dist\"].fillna(0) * events[\"long_ball_score\"]\n",
    "    events[\"dx_x_longball_score\"] = events[\"dx\"].fillna(0) * events[\"long_ball_score\"]\n",
    "    \n",
    "    print(\"✅ A안: Score 및 Interaction 피처 생성 완료\")\n",
    "else:\n",
    "    # B안: 이미 cross_score, long_ball_score가 생성되어 있음\n",
    "    # Interaction만 추가\n",
    "    events[\"start_x_x_cross_score\"] = events[\"start_x\"] * events[\"cross_score\"]\n",
    "    events[\"angle_x_cross_score\"] = events[\"angle_to_goal\"] * events[\"cross_score\"]\n",
    "    events[\"dist_x_longball_score\"] = events[\"dist\"].fillna(0) * events[\"long_ball_score\"]\n",
    "    events[\"dx_x_longball_score\"] = events[\"dx\"].fillna(0) * events[\"long_ball_score\"]\n",
    "    \n",
    "    print(\"✅ B안: Interaction 피처 생성 완료\")\n",
    "\n",
    "# ----------------------\n",
    "# 8. 마지막 K 이벤트만 사용 (lastK)\n",
    "# ----------------------\n",
    "# rev_idx: 0이 마지막 이벤트\n",
    "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\n",
    "    lambda s: s.max() - s\n",
    ")\n",
    "\n",
    "lastK = events[events[\"rev_idx\"] < K].copy()\n",
    "\n",
    "# pos_in_K: 0~(K-1), 앞쪽 패딩 고려해서 뒤에 실제 이벤트가 모이게\n",
    "def assign_pos_in_K(df):\n",
    "    df = df.sort_values(\"event_idx\")  # 오래된 → 최근\n",
    "    L = len(df)\n",
    "    df = df.copy()\n",
    "    df[\"pos_in_K\"] = np.arange(K - L, K)\n",
    "    return df\n",
    "\n",
    "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
    "\n",
    "# ----------------------\n",
    "# 9. wide feature pivot\n",
    "# ----------------------\n",
    "# 사용할 이벤트 피처 선택\n",
    "num_cols = [\n",
    "    \"start_x\", \"start_y\",\n",
    "    \"end_x\", \"end_y\",\n",
    "    \"dx\", \"dy\", \"dist\", \"speed\",\n",
    "    \"dt\",\n",
    "    \"ep_idx_norm\",\n",
    "    \"x_zone\", \"lane\",\n",
    "    \"is_final_team\",\n",
    "    # 코너 관련 피처\n",
    "    \"angle_to_goal\",\n",
    "    \"dist_to_nearest_corner\",\n",
    "    \"is_corner_area\",  # 유지 (중요도 낮게)\n",
    "    \"angle_x_is_corner\",  # 인터랙션: angle_to_goal × is_corner_area\n",
    "    \"corner_dist_x_angle\",  # 인터랙션: dist_to_nearest_corner × angle_to_goal\n",
    "]\n",
    "\n",
    "# 세트피스 킥 피처: A안 또는 B안에 따라 추가\n",
    "# (피처는 이미 7-1 섹션에서 events에 생성됨)\n",
    "num_cols.extend([\n",
    "    \"cross_score\",  # 연속형 score\n",
    "    \"long_ball_score\",  # 연속형 score\n",
    "    \"start_x_x_cross_score\",  # interaction\n",
    "    \"angle_x_cross_score\",  # interaction\n",
    "    \"dist_x_longball_score\",  # interaction\n",
    "    \"dx_x_longball_score\",  # interaction\n",
    "])\n",
    "print(f\"\\n✅ 세트피스 킥 피처 추가 완료 ({len(num_cols)}개 숫자형 피처)\")\n",
    "\n",
    "cat_cols = [\n",
    "    \"type_id\",\n",
    "    \"res_id\",\n",
    "    \"team_id_enc\",\n",
    "    \"is_home\",\n",
    "    \"period_id\",\n",
    "    \"is_last\",\n",
    "]\n",
    "\n",
    "feature_cols = num_cols + cat_cols\n",
    "\n",
    "wide = lastK[[\"game_episode\", \"pos_in_K\"] + feature_cols].copy()\n",
    "\n",
    "# 숫자형 pivot\n",
    "wide_num = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=num_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 범주형 pivot\n",
    "wide_cat = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=cat_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 컬럼 이름 평탄화\n",
    "wide_num.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_num.columns]\n",
    "wide_cat.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_cat.columns]\n",
    "\n",
    "X = pd.concat([wide_num, wide_cat], axis=1).reset_index()  # game_episode 포함\n",
    "\n",
    "# episode-level 메타 붙이기\n",
    "X = X.merge(ep_meta[[\"game_episode\", \"game_id\", \"game_clock_min\", \"final_team_id\", \"is_home\", \"period_id\"]],\n",
    "            on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# episode-level 세트피스 킥 피처 추가\n",
    "# 마지막 K개 이벤트 중 크로스/롱볼 발생 여부 또는 평균 score\n",
    "lastK_for_stats = lastK[lastK[\"rev_idx\"] > 0].copy()  # 마지막 이벤트 제외 (rev_idx > 0)\n",
    "\n",
    "if USE_TYPE_ID_BASED:\n",
    "    # A안: 이진 플래그 합계\n",
    "    ep_kick_stats = lastK_for_stats.groupby(\"game_episode\").agg({\n",
    "        \"is_cross\": \"sum\",  # 마지막 K개 중 크로스 발생 횟수\n",
    "        \"is_long_ball\": \"sum\",  # 마지막 K개 중 롱볼 발생 횟수\n",
    "        \"cross_score\": \"mean\",  # 평균 score (추가 정보)\n",
    "        \"long_ball_score\": \"mean\",  # 평균 score (추가 정보)\n",
    "    }).reset_index()\n",
    "    ep_kick_stats.columns = [\"game_episode\", \"has_cross_in_K\", \"has_long_ball_in_K\", \n",
    "                              \"avg_cross_score_in_K\", \"avg_longball_score_in_K\"]\n",
    "else:\n",
    "    # B안: Score 평균만 사용\n",
    "    ep_kick_stats = lastK_for_stats.groupby(\"game_episode\").agg({\n",
    "        \"cross_score\": \"mean\",  # 평균 score\n",
    "        \"long_ball_score\": \"mean\",  # 평균 score\n",
    "    }).reset_index()\n",
    "    ep_kick_stats.columns = [\"game_episode\", \"avg_cross_score_in_K\", \"avg_longball_score_in_K\"]\n",
    "\n",
    "X = X.merge(ep_kick_stats, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# NaN 채우기\n",
    "for col in ep_kick_stats.columns:\n",
    "    if col != \"game_episode\":\n",
    "        X[col] = X[col].fillna(0)\n",
    "\n",
    "# train 라벨 붙이기\n",
    "X = X.merge(labels, on=\"game_episode\", how=\"left\")  # test는 NaN\n",
    "\n",
    "# ----------------------\n",
    "# 10. train/test 분리\n",
    "# ----------------------\n",
    "train_mask = X[\"game_episode\"].isin(labels[\"game_episode\"])\n",
    "X_train = X[train_mask].copy()\n",
    "X_test = X[~train_mask].copy()\n",
    "\n",
    "y_train_x = X_train[\"target_x\"].astype(float)\n",
    "y_train_y = X_train[\"target_y\"].astype(float)\n",
    "\n",
    "# 모델 입력에서 빼야 할 컬럼들\n",
    "drop_cols = [\n",
    "    \"game_episode\",\n",
    "    \"game_id\",\n",
    "    \"target_x\",\n",
    "    \"target_y\",\n",
    "]\n",
    "\n",
    "X_train_feat = X_train.drop(columns=drop_cols)\n",
    "X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns])\n",
    "\n",
    "# NaN 채우기 (LGBM은 NaN 다루긴 하지만, 깔끔하게)\n",
    "X_train_feat = X_train_feat.fillna(0)\n",
    "X_test_feat = X_test_feat.fillna(0)\n",
    "\n",
    "# ----------------------\n",
    "# 11. AutoGluon 학습\n",
    "# ----------------------\n",
    "# 빠른 테스트 모드: CatBoost와 LightGBM만 사용 (10~20분)\n",
    "# 새로운 피처 추가/변경 시 빠르게 테스트하기 위한 설정\n",
    "FAST_TEST_MODE = True  # False로 변경하면 모든 모델 사용 (best_quality)\n",
    "\n",
    "if FAST_TEST_MODE:\n",
    "    # CatBoost와 LightGBM만 선택\n",
    "    hyperparameters = {\n",
    "        'CAT': {},\n",
    "        'GBM': {},  # LightGBM\n",
    "    }\n",
    "    time_limit = 600  # 10분 (10분: 600, 15분: 900, 20분: 1200)\n",
    "    presets = \"good_quality\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"빠른 테스트 모드: CatBoost + LightGBM만 사용 (10분)\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    # 모든 모델 사용 (최종 제출용)\n",
    "    hyperparameters = None\n",
    "    time_limit = 1800  # 30분\n",
    "    presets = \"best_quality\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"전체 모델 학습 모드: 모든 모델 사용 (30분)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# X 좌표 예측 모델\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"X 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_x = X_train_feat.copy()\n",
    "train_data_x[\"target_x\"] = y_train_x\n",
    "\n",
    "# 모델 경로 결정 (A안/B안에 따라)\n",
    "model_suffix = \"typeid_based\" if USE_TYPE_ID_BASED else \"score_based\"\n",
    "\n",
    "predictor_x = TabularPredictor(\n",
    "    label=\"target_x\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=f\"ag_models_x_{model_suffix}\"  # 모델 저장 경로\n",
    ").fit(\n",
    "    train_data=train_data_x,\n",
    "    time_limit=time_limit,\n",
    "    presets=presets,\n",
    "    hyperparameters=hyperparameters,  # 특정 모델만 선택\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Y 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_y = X_train_feat.copy()\n",
    "train_data_y[\"target_y\"] = y_train_y\n",
    "\n",
    "predictor_y = TabularPredictor(\n",
    "    label=\"target_y\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=f\"ag_models_y_{model_suffix}\"  # 모델 저장 경로\n",
    ").fit(\n",
    "    train_data=train_data_y,\n",
    "    time_limit=time_limit,\n",
    "    presets=presets,\n",
    "    hyperparameters=hyperparameters,  # 특정 모델만 선택\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 12. test 예측\n",
    "# ----------------------\n",
    "print(\"=\" * 50)\n",
    "print(\"Test 데이터 예측 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pred_x = predictor_x.predict(X_test_feat)\n",
    "pred_y = predictor_y.predict(X_test_feat)\n",
    "\n",
    "# 필드 범위로 클립\n",
    "pred_x = np.clip(pred_x, 0, 105)\n",
    "pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "# ----------------------\n",
    "# 13. submission 생성\n",
    "# ----------------------\n",
    "sub = sample_sub.copy()\n",
    "\n",
    "# X_test에는 game_episode가 있으니, test_index와 align\n",
    "pred_df = X_test[[\"game_episode\"]].copy()\n",
    "pred_df[\"end_x\"] = pred_x\n",
    "pred_df[\"end_y\"] = pred_y\n",
    "\n",
    "sub = sub.drop(columns=[\"end_x\", \"end_y\"], errors=\"ignore\")\n",
    "sub = sub.merge(pred_df, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "submission_filename = f\"submission_autogluon_{model_suffix}.csv\"\n",
    "sub.to_csv(submission_filename, index=False)\n",
    "print(f\"Saved {submission_filename}\")\n",
    "\n",
    "# ----------------------\n",
    "# 14. 백업 생성 (모델 및 제출 파일)\n",
    "# ----------------------\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "backup_dir = \"backups\"\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_subdir = os.path.join(backup_dir, f\"{model_suffix}_{timestamp}\")\n",
    "os.makedirs(backup_subdir, exist_ok=True)\n",
    "\n",
    "# 모델 백업\n",
    "model_x_path = f\"ag_models_x_{model_suffix}\"\n",
    "model_y_path = f\"ag_models_y_{model_suffix}\"\n",
    "\n",
    "if os.path.exists(model_x_path):\n",
    "    shutil.copytree(model_x_path, os.path.join(backup_subdir, model_x_path))\n",
    "    print(f\"✓ X 모델 백업 완료: {backup_subdir}/{model_x_path}\")\n",
    "\n",
    "if os.path.exists(model_y_path):\n",
    "    shutil.copytree(model_y_path, os.path.join(backup_subdir, model_y_path))\n",
    "    print(f\"✓ Y 모델 백업 완료: {backup_subdir}/{model_y_path}\")\n",
    "\n",
    "# 제출 파일 백업\n",
    "if os.path.exists(submission_filename):\n",
    "    shutil.copy(submission_filename, os.path.join(backup_subdir, submission_filename))\n",
    "    print(f\"✓ 제출 파일 백업 완료: {backup_subdir}/{submission_filename}\")\n",
    "\n",
    "print(f\"\\n백업 완료: {backup_subdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11416c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "모델 성능 확인\n",
      "==================================================\n",
      "\n",
      "[X 좌표 모델 - 리더보드]\n",
      "                      model  score_val              eval_metric  \\\n",
      "0       WeightedEnsemble_L2 -11.792851  root_mean_squared_error   \n",
      "1       WeightedEnsemble_L3 -11.792851  root_mean_squared_error   \n",
      "2           CatBoost_BAG_L1 -11.855033  root_mean_squared_error   \n",
      "3           CatBoost_BAG_L2 -11.860589  root_mean_squared_error   \n",
      "4           LightGBM_BAG_L1 -11.891816  root_mean_squared_error   \n",
      "5           LightGBM_BAG_L2 -11.910419  root_mean_squared_error   \n",
      "6  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
      "7  WeightedEnsemble_L2_FULL        NaN  root_mean_squared_error   \n",
      "8      LightGBM_BAG_L2_FULL        NaN  root_mean_squared_error   \n",
      "9      LightGBM_BAG_L1_FULL        NaN  root_mean_squared_error   \n",
      "\n",
      "   pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
      "0       0.268435  114.639654                0.000298           0.017744   \n",
      "1       0.268458  114.631147                0.000321           0.009237   \n",
      "2       0.073927   94.417714                0.073927          94.417714   \n",
      "3       0.310896  161.402567                0.042759          46.780657   \n",
      "4       0.194209   20.204196                0.194209          20.204196   \n",
      "5       0.349503  127.656603                0.081366          13.034693   \n",
      "6            NaN   11.787060                     NaN           0.009237   \n",
      "7            NaN   11.795567                     NaN           0.017744   \n",
      "8            NaN   13.185467                     NaN           1.407644   \n",
      "9            NaN    3.294811                     NaN           3.294811   \n",
      "\n",
      "   stack_level  can_infer  fit_order  \n",
      "0            2      False          3  \n",
      "1            3      False          6  \n",
      "2            1      False          2  \n",
      "3            2      False          5  \n",
      "4            1      False          1  \n",
      "5            2      False          4  \n",
      "6            3       True         12  \n",
      "7            2       True          9  \n",
      "8            2       True         10  \n",
      "9            1       True          7  \n",
      "\n",
      "[Y 좌표 모델 - 리더보드]\n",
      "                      model  score_val              eval_metric  \\\n",
      "0       WeightedEnsemble_L2 -13.077400  root_mean_squared_error   \n",
      "1       WeightedEnsemble_L3 -13.077617  root_mean_squared_error   \n",
      "2           CatBoost_BAG_L2 -13.119920  root_mean_squared_error   \n",
      "3           LightGBM_BAG_L1 -13.127468  root_mean_squared_error   \n",
      "4           CatBoost_BAG_L1 -13.171727  root_mean_squared_error   \n",
      "5           LightGBM_BAG_L2 -13.207640  root_mean_squared_error   \n",
      "6  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
      "7  WeightedEnsemble_L2_FULL        NaN  root_mean_squared_error   \n",
      "8      LightGBM_BAG_L2_FULL        NaN  root_mean_squared_error   \n",
      "9      LightGBM_BAG_L1_FULL        NaN  root_mean_squared_error   \n",
      "\n",
      "   pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
      "0       0.223932  120.610022                0.000683           0.064292   \n",
      "1       0.265326  164.499202                0.000388           0.011903   \n",
      "2       0.264938  164.487299                0.041689          43.941569   \n",
      "3       0.128584   14.635136                0.128584          14.635136   \n",
      "4       0.094666  105.910594                0.094666         105.910594   \n",
      "5       0.344503  136.989206                0.121254          16.443476   \n",
      "6            NaN   13.589965                     NaN           0.011903   \n",
      "7            NaN   11.432148                     NaN           0.064292   \n",
      "8            NaN   13.481362                     NaN           2.113506   \n",
      "9            NaN    2.023680                     NaN           2.023680   \n",
      "\n",
      "   stack_level  can_infer  fit_order  \n",
      "0            2      False          3  \n",
      "1            3      False          6  \n",
      "2            2      False          5  \n",
      "3            1      False          1  \n",
      "4            1      False          2  \n",
      "5            2      False          4  \n",
      "6            3       True         12  \n",
      "7            2       True          9  \n",
      "8            2       True         10  \n",
      "9            1       True          7  \n",
      "\n",
      "==================================================\n",
      "Train 데이터 성능 평가\n",
      "==================================================\n",
      "\n",
      "X 좌표 RMSE (Train): 9.6867\n",
      "Y 좌표 RMSE (Train): 11.1872\n",
      "\n",
      "==================================================\n",
      "제출 파일 확인\n",
      "==================================================\n",
      "\n",
      "제출 파일 행 수: 2414\n",
      "\n",
      "제출 파일 샘플:\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  64.719420  12.339897\n",
      "1     153363_2  34.016330  51.578686\n",
      "2     153363_6  36.483772  61.815983\n",
      "3     153363_7  55.272150  10.204851\n",
      "4     153363_8  80.473060  12.723977\n",
      "5     153363_9  74.801980  64.809370\n",
      "6    153363_10  64.076710  11.920504\n",
      "7    153363_12  71.190420  10.036727\n",
      "8    153363_13  32.905857  63.007526\n",
      "9    153363_15  74.136490  12.881967\n",
      "\n",
      "제출 파일 통계:\n",
      "             end_x        end_y\n",
      "count  2414.000000  2414.000000\n",
      "mean     67.129921    33.743630\n",
      "std      20.577291    20.155345\n",
      "min       8.095508     1.027295\n",
      "25%      52.516423    13.440117\n",
      "50%      71.831835    33.613718\n",
      "75%      84.485308    53.606716\n",
      "max     100.895660    67.235880\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 결과 확인 (학습 완료 후 실행)\n",
    "# ----------------------\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 저장된 모델 로드 (type_id 기반 또는 score 기반)\n",
    "# 모델 경로 확인: ag_models_x_typeid_based 또는 ag_models_x_score_based\n",
    "import os\n",
    "model_suffix = \"typeid_based\" if os.path.exists(\"ag_models_x_typeid_based\") else \"score_based\"\n",
    "if not os.path.exists(f\"ag_models_x_{model_suffix}\"):\n",
    "    model_suffix = \"setpiece\"  # 이전 버전 호환\n",
    "\n",
    "predictor_x = TabularPredictor.load(f\"ag_models_x_{model_suffix}\")\n",
    "predictor_y = TabularPredictor.load(f\"ag_models_y_{model_suffix}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"모델 성능 확인\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# X 좌표 모델 리더보드\n",
    "print(\"\\n[X 좌표 모델 - 리더보드]\")\n",
    "leaderboard_x = predictor_x.leaderboard(silent=True)\n",
    "print(leaderboard_x.head(10))\n",
    "\n",
    "# Y 좌표 모델 리더보드\n",
    "print(\"\\n[Y 좌표 모델 - 리더보드]\")\n",
    "leaderboard_y = predictor_y.leaderboard(silent=True)\n",
    "print(leaderboard_y.head(10))\n",
    "\n",
    "# Train 데이터로 성능 평가 (첫 번째 셀 실행 후 사용 가능)\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Train 데이터 성능 평가\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # X 좌표 평가\n",
    "    y_pred_train_x = predictor_x.predict(X_train_feat)\n",
    "    rmse_x = np.sqrt(np.mean((y_train_x - y_pred_train_x) ** 2))\n",
    "    print(f\"\\nX 좌표 RMSE (Train): {rmse_x:.4f}\")\n",
    "    \n",
    "    # Y 좌표 평가\n",
    "    y_pred_train_y = predictor_y.predict(X_train_feat)\n",
    "    rmse_y = np.sqrt(np.mean((y_train_y - y_pred_train_y) ** 2))\n",
    "    print(f\"Y 좌표 RMSE (Train): {rmse_y:.4f}\")\n",
    "except NameError:\n",
    "    print(\"\\n(첫 번째 셀을 먼저 실행해야 Train 데이터 평가가 가능합니다)\")\n",
    "\n",
    "# 제출 파일 확인\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"제출 파일 확인\")\n",
    "print(\"=\" * 50)\n",
    "submission_filename = f\"submission_autogluon_{model_suffix}.csv\"\n",
    "if not os.path.exists(submission_filename):\n",
    "    submission_filename = \"submission_autogluon_setpiece.csv\"  # 이전 버전 호환\n",
    "\n",
    "sub = pd.read_csv(submission_filename)\n",
    "print(f\"\\n제출 파일 행 수: {len(sub)}\")\n",
    "print(f\"\\n제출 파일 샘플:\")\n",
    "print(sub.head(10))\n",
    "print(f\"\\n제출 파일 통계:\")\n",
    "print(sub.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
