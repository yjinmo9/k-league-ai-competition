{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8477ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 핵심 피처 3개 추가 완료 (dist_to_sideline, angle_to_goal_center, time_pos_inter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_3945/2429465320.py:211: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_3945/2429465320.py:301: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_train_feat = X_train_feat.fillna(0)\n",
      "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_3945/2429465320.py:302: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_feat = X_test_feat.fillna(0)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       4.99 GB / 16.00 GB (31.2%)\n",
      "Disk Space Avail:   14.77 GB / 460.43 GB (3.2%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LightGBM만 사용 (30분)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "X 좌표 모델 학습 시작...\n",
      "==================================================\n",
      "\n",
      "모델 저장 경로: ag_models_x_lgbm_sideline_goalangle_timepos, ag_models_y_lgbm_sideline_goalangle_timepos\n",
      "사용 모델: LightGBM만\n",
      "사용 피처: dist_to_sideline, angle_to_goal_center, time_pos_inter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 450s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_lgbm_sideline_goalangle_timepos/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5145.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.09 MB (1.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['time_pos_inter_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['time_pos_inter_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 493 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  62 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t513 features in original data used to generate 513 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.02 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 299.61s of the 449.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.06%)\n",
      "\t-11.9277\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.6s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 434.16s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.7 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t-11.9277\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 434.15s of the 434.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.31%)\n",
      "\t-12.0507\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.04s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 422.25s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.9 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t-11.9277\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 27.8s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7066.9 rows/s (1715 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/5.9 GB\n",
      "\t3.31s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.0s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/5.0 GB\n",
      "\t1.48s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.0s\t = Training   runtime\n",
      "Updated best model to \"LightGBM_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBM_BAG_L1_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 5.06s ... Best model: \"LightGBM_BAG_L1_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_lgbm_sideline_goalangle_timepos/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val  fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      LightGBM_BAG_L1_FULL     -11.816443 -11.927729  root_mean_squared_error        0.008962          None  3.305432                 0.008962                   None           3.305432            1       True          1\n",
      "1  WeightedEnsemble_L3_FULL     -11.816443 -11.927729  root_mean_squared_error        0.009884          None  3.310408                 0.000922                   None           0.004976            3       True          4\n",
      "2  WeightedEnsemble_L2_FULL     -11.816443 -11.927729  root_mean_squared_error        0.009968          None  3.309775                 0.001006                   None           0.004343            2       True          2\n",
      "3      LightGBM_BAG_L2_FULL     -11.911150 -12.050654  root_mean_squared_error        0.014344          None  4.783682                 0.005382                   None           1.478250            2       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t33s\t = DyStack   runtime |\t1767s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1767s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_lgbm_sideline_goalangle_timepos\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4804.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (1.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1177.14s of the 1766.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.42%)\n",
      "\t-11.8453\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.57s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1747.79s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.7 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t-11.8453\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1747.77s of the 1747.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.50%)\n",
      "\t-11.9381\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.08s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1736.14s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.0 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.96, 'LightGBM_BAG_L2': 0.04}\n",
      "\t-11.8453\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 30.63s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 5320.0 rows/s (1930 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.4/6.4 GB\n",
      "\t3.06s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.0s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.4/5.9 GB\n",
      "\t1.69s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.96, 'LightGBM_BAG_L2': 0.04}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 5.12s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_lgbm_sideline_goalangle_timepos\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       5.67 GB / 16.00 GB (35.5%)\n",
      "Disk Space Avail:   14.71 GB / 460.43 GB (3.2%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n",
      "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 450s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_lgbm_sideline_goalangle_timepos/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    13720\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5871.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.09 MB (1.1% of available memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Y 좌표 모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['time_pos_inter_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 3 | ['time_pos_inter_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 493 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  62 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t513 features in original data used to generate 513 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.02 MB (0.8% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.48s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 299.60s of the 449.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.03%)\n",
      "\t-13.0941\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.57s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 437.70s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.0 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t-13.0941\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 437.67s of the 437.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.26%)\n",
      "\t-13.2414\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.73s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 426.71s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/4.3 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t-13.0941\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 23.39s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 12345.7 rows/s (1715 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/5.0 GB\n",
      "\t2.27s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/4.7 GB\n",
      "\t1.6s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.03s\t = Training   runtime\n",
      "Updated best model to \"LightGBM_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBM_BAG_L1_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 4.28s ... Best model: \"LightGBM_BAG_L1_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_lgbm_sideline_goalangle_timepos/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val  fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      LightGBM_BAG_L2_FULL     -13.368783 -13.241372  root_mean_squared_error        0.012773          None  3.870013                 0.006545                   None           1.600312            2       True          3\n",
      "1      LightGBM_BAG_L1_FULL     -13.415632 -13.094108  root_mean_squared_error        0.006228          None  2.269701                 0.006228                   None           2.269701            1       True          1\n",
      "2  WeightedEnsemble_L2_FULL     -13.415632 -13.094108  root_mean_squared_error        0.007038          None  2.284594                 0.000810                   None           0.014893            2       True          2\n",
      "3  WeightedEnsemble_L3_FULL     -13.415632 -13.094108  root_mean_squared_error        0.007142          None  2.303960                 0.000914                   None           0.034259            3       True          4\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t28s\t = DyStack   runtime |\t1772s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1772s\n",
      "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_lgbm_sideline_goalangle_timepos\"\n",
      "Train Data Rows:    15435\n",
      "Train Data Columns: 538\n",
      "Label Column:       target_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4817.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.98 MB (1.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  : 1 | ['is_home']\n",
      "\t\t('float', []) : 4 | ['time_pos_inter_19', 'type_id_19', 'final_team_id', 'period_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :   1 | ['is_home_19']\n",
      "\t\t('float', [])  : 492 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 451 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
      "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.01 MB (1.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1180.60s of the 1771.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.18%)\n",
      "\t-13.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.45s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1754.23s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/6.1 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t-13.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1754.22s of the 1754.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.91%)\n",
      "\t-13.2517\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.53s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1742.58s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.2 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t-13.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 29.38s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10780.4 rows/s (1930 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.4/5.4 GB\n",
      "\t2.13s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.0s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.4/5.6 GB\n",
      "\t1.52s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.01s\t = Training   runtime\n",
      "Updated best model to \"LightGBM_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBM_BAG_L1_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 3.9s ... Best model: \"LightGBM_BAG_L1_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_lgbm_sideline_goalangle_timepos\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test 데이터 예측 중...\n",
      "==================================================\n",
      "Saved submission_autogluon_lgbm_sideline_goalangle_timepos.csv\n",
      "\n",
      "학습 및 예측 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# ----------------------\n",
    "# 0. 설정\n",
    "# ----------------------\n",
    "BASE_PATH = \"open_track1/\"\n",
    "PATH_TRAIN = os.path.join(BASE_PATH, \"train.csv\")\n",
    "PATH_TEST = os.path.join(BASE_PATH, \"test.csv\")\n",
    "PATH_MATCH_INFO = os.path.join(BASE_PATH, \"match_info.csv\")\n",
    "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "K = 20   # 마지막 K 이벤트 사용 (20~32 사이 선택)\n",
    "\n",
    "# ----------------------\n",
    "# 1. 데이터 로드\n",
    "# ----------------------\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test_index = pd.read_csv(PATH_TEST)\n",
    "match_info = pd.read_csv(PATH_MATCH_INFO)\n",
    "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
    "\n",
    "test_events_list = []\n",
    "for _, row in test_index.iterrows():\n",
    "    # path가 \"./test/...\" 형식이므로 BASE_PATH와 결합\n",
    "    test_path = os.path.join(BASE_PATH, row[\"path\"].lstrip(\"./\"))\n",
    "    df_ep = pd.read_csv(test_path)\n",
    "    test_events_list.append(df_ep)\n",
    "\n",
    "test_events = pd.concat(test_events_list, ignore_index=True)\n",
    "\n",
    "train[\"is_train\"] = 1\n",
    "test_events[\"is_train\"] = 0\n",
    "\n",
    "events = pd.concat([train, test_events], ignore_index=True)\n",
    "\n",
    "# ----------------------\n",
    "# 2. 기본 정렬 + episode 내 인덱스\n",
    "# ----------------------\n",
    "events = events.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
    "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
    "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
    "\n",
    "# ----------------------\n",
    "# 3. 시간/공간 feature\n",
    "# ----------------------\n",
    "# Δt\n",
    "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
    "events[\"dt\"] = events[\"time_seconds\"] - events[\"prev_time\"]\n",
    "events[\"dt\"] = events[\"dt\"].fillna(0.0)\n",
    "\n",
    "# 이동량/거리\n",
    "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
    "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
    "events[\"dist\"] = np.sqrt(events[\"dx\"]**2 + events[\"dy\"]**2)\n",
    "\n",
    "# 속도 (dt=0 보호)\n",
    "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
    "\n",
    "# zone / lane (필요시 범위 조정)\n",
    "events[\"x_zone\"] = (events[\"start_x\"] / (105/7)).astype(int).clip(0, 6)\n",
    "events[\"lane\"] = pd.cut(\n",
    "    events[\"start_y\"],\n",
    "    bins=[0, 68/3, 2*68/3, 68],\n",
    "    labels=[0, 1, 2],\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 3-1. 코너 관련 피처 (Corner Features)\n",
    "# ----------------------\n",
    "# 골대까지 각도 계산 (라디안 → 도)\n",
    "# 골대 중앙: (105, 34)\n",
    "events[\"angle_to_goal\"] = np.arctan2(\n",
    "    34 - events[\"start_y\"],\n",
    "    105 - events[\"start_x\"]\n",
    ") * 180 / np.pi\n",
    "\n",
    "# 코너까지 최단 거리 계산\n",
    "# 공격 방향 코너: (105, 0) 상단, (105, 68) 하단\n",
    "events[\"dist_corner_top\"] = np.sqrt((105 - events[\"start_x\"])**2 + (0 - events[\"start_y\"])**2)\n",
    "events[\"dist_corner_bottom\"] = np.sqrt((105 - events[\"start_x\"])**2 + (68 - events[\"start_y\"])**2)\n",
    "events[\"dist_to_nearest_corner\"] = events[[\"dist_corner_top\", \"dist_corner_bottom\"]].min(axis=1)\n",
    "\n",
    "# 코너 구역 플래그 (X > 100 이면서 Y < 5 또는 Y > 63)\n",
    "events[\"is_corner_area\"] = ((events[\"start_x\"] > 100) & \n",
    "                            ((events[\"start_y\"] < 5) | (events[\"start_y\"] > 63))).astype(int)\n",
    "\n",
    "# 인터랙션 피처 1: angle_to_goal × is_corner_area\n",
    "# 코너 구역일 때만 각도의 영향이 강하게 나타남\n",
    "events[\"angle_goal_x_corner\"] = events[\"angle_to_goal\"] * events[\"is_corner_area\"]\n",
    "\n",
    "# 인터랙션 피처 2: dist_to_nearest_corner × angle_to_goal\n",
    "# 코너 거리와 각도의 조합\n",
    "events[\"dist_corner_x_angle\"] = events[\"dist_to_nearest_corner\"] * events[\"angle_to_goal\"]\n",
    "\n",
    "# ----------------------\n",
    "# 3-2. 새로운 핵심 피처 3개 (고득점용)\n",
    "# ----------------------\n",
    "# ① 사이드라인 압박도 (dist_to_sideline)\n",
    "# 의도: 터치라인에 붙을수록 패스 각도가 제한되는 '공간적 제약'을 수치화합니다.\n",
    "events[\"dist_to_sideline\"] = events[\"start_y\"].apply(lambda y: min(y, 68-y))\n",
    "\n",
    "# ② 골대 중심과의 물리적 각도 (angle_to_goal_center)\n",
    "# 의도: 단순히 원점 기준 각도가 아니라, 목적지인 골대를 기준으로 한 각도를 줍니다.\n",
    "# (라디안 단위로 저장 - 기존 angle_to_goal은 도 단위)\n",
    "events[\"angle_to_goal_center\"] = np.arctan2(\n",
    "    34 - events[\"start_y\"], \n",
    "    105 - events[\"start_x\"]\n",
    ")\n",
    "\n",
    "# ③ 시간 × 전진 위치 상호작용 (time_pos_interaction)\n",
    "# 의도: \"후반전 + 상대 진영\"이라는 조건이 만났을 때의 특수 상황을 강조합니다.\n",
    "events[\"time_pos_inter\"] = events[\"ep_idx_norm\"] * events[\"start_x\"]\n",
    "\n",
    "print(\"✓ 핵심 피처 3개 추가 완료 (dist_to_sideline, angle_to_goal_center, time_pos_inter)\")\n",
    "\n",
    "# ----------------------\n",
    "# 4. 라벨 및 episode-level 메타 (train 전용)\n",
    "# ----------------------\n",
    "train_events = events[events[\"is_train\"] == 1].copy()\n",
    "\n",
    "last_events = (\n",
    "    train_events\n",
    "    .groupby(\"game_episode\", as_index=False)\n",
    "    .tail(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "labels = last_events[[\"game_episode\", \"end_x\", \"end_y\"]].rename(\n",
    "    columns={\"end_x\": \"target_x\", \"end_y\": \"target_y\"}\n",
    ")\n",
    "\n",
    "# episode-level 메타 (마지막 이벤트 기준)\n",
    "ep_meta = last_events[[\"game_episode\", \"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"]].copy()\n",
    "ep_meta = ep_meta.rename(columns={\"team_id\": \"final_team_id\"})\n",
    "\n",
    "# game_clock (분 단위, 0~90+)\n",
    "ep_meta[\"game_clock_min\"] = np.where(\n",
    "    ep_meta[\"period_id\"] == 1,\n",
    "    ep_meta[\"time_seconds\"] / 60.0,\n",
    "    45.0 + ep_meta[\"time_seconds\"] / 60.0\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5. 공격 팀 플래그 (final_team vs 상대)\n",
    "# ----------------------\n",
    "# final_team_id를 전체 events에 붙임\n",
    "events = events.merge(\n",
    "    ep_meta[[\"game_episode\", \"final_team_id\"]],\n",
    "    on=\"game_episode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 6. 입력용 events에서 마지막 이벤트 타깃 정보 가리기\n",
    "# ----------------------\n",
    "# is_last 플래그\n",
    "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
    "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
    "\n",
    "# labels는 이미 뽑아놨으니, 입력쪽에서만 end_x, end_y, dx, dy, dist, speed 지움\n",
    "mask_last = events[\"is_last\"] == 1\n",
    "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
    "    events.loc[mask_last, col] = np.nan\n",
    "\n",
    "# ----------------------\n",
    "# 7. 카테고리 인코딩 (type_name, result_name, team_id 등)\n",
    "# ----------------------\n",
    "events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
    "events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA_RES__\")\n",
    "\n",
    "le_type = LabelEncoder()\n",
    "le_res = LabelEncoder()\n",
    "\n",
    "events[\"type_id\"] = le_type.fit_transform(events[\"type_name\"])\n",
    "events[\"res_id\"] = le_res.fit_transform(events[\"result_name\"])\n",
    "\n",
    "# team_id는 그대로 써도 되지만, 문자열이면 숫자로 매핑\n",
    "if events[\"team_id\"].dtype == \"object\":\n",
    "    le_team = LabelEncoder()\n",
    "    events[\"team_id_enc\"] = le_team.fit_transform(events[\"team_id\"])\n",
    "else:\n",
    "    events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 8. 마지막 K 이벤트만 사용 (lastK)\n",
    "# ----------------------\n",
    "# rev_idx: 0이 마지막 이벤트\n",
    "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\n",
    "    lambda s: s.max() - s\n",
    ")\n",
    "\n",
    "lastK = events[events[\"rev_idx\"] < K].copy()\n",
    "\n",
    "# pos_in_K: 0~(K-1), 앞쪽 패딩 고려해서 뒤에 실제 이벤트가 모이게\n",
    "def assign_pos_in_K(df):\n",
    "    df = df.sort_values(\"event_idx\")  # 오래된 → 최근\n",
    "    L = len(df)\n",
    "    df = df.copy()\n",
    "    df[\"pos_in_K\"] = np.arange(K - L, K)\n",
    "    return df\n",
    "\n",
    "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
    "\n",
    "# ----------------------\n",
    "# 9. wide feature pivot\n",
    "# ----------------------\n",
    "# 사용할 이벤트 피처 선택\n",
    "num_cols = [\n",
    "    \"start_x\", \"start_y\",\n",
    "    \"end_x\", \"end_y\",\n",
    "    \"dx\", \"dy\", \"dist\", \"speed\",\n",
    "    \"dt\",\n",
    "    \"ep_idx_norm\",\n",
    "    \"x_zone\", \"lane\",\n",
    "    \"is_final_team\",\n",
    "    # 코너 관련 피처\n",
    "    \"angle_to_goal\",\n",
    "    \"dist_to_nearest_corner\",\n",
    "    \"is_corner_area\",\n",
    "    \"angle_goal_x_corner\",  # 인터랙션: angle_to_goal × is_corner_area\n",
    "    \"dist_corner_x_angle\",  # 인터랙션: dist_to_nearest_corner × angle_to_goal\n",
    "    # 새로운 핵심 피처 3개\n",
    "    \"dist_to_sideline\",      # 사이드라인 압박도\n",
    "    \"angle_to_goal_center\",  # 골대 중심과의 물리적 각도\n",
    "    \"time_pos_inter\",        # 시간 × 전진 위치 상호작용\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"type_id\",\n",
    "    \"res_id\",\n",
    "    \"team_id_enc\",\n",
    "    \"is_home\",\n",
    "    \"period_id\",\n",
    "    \"is_last\",\n",
    "]\n",
    "\n",
    "feature_cols = num_cols + cat_cols\n",
    "\n",
    "wide = lastK[[\"game_episode\", \"pos_in_K\"] + feature_cols].copy()\n",
    "\n",
    "# 숫자형 pivot\n",
    "wide_num = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=num_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 범주형 pivot\n",
    "wide_cat = wide.pivot_table(\n",
    "    index=\"game_episode\",\n",
    "    columns=\"pos_in_K\",\n",
    "    values=cat_cols,\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# 컬럼 이름 평탄화\n",
    "wide_num.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_num.columns]\n",
    "wide_cat.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_cat.columns]\n",
    "\n",
    "X = pd.concat([wide_num, wide_cat], axis=1).reset_index()  # game_episode 포함\n",
    "\n",
    "# episode-level 메타 붙이기\n",
    "X = X.merge(ep_meta[[\"game_episode\", \"game_id\", \"game_clock_min\", \"final_team_id\", \"is_home\", \"period_id\"]],\n",
    "            on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# train 라벨 붙이기\n",
    "X = X.merge(labels, on=\"game_episode\", how=\"left\")  # test는 NaN\n",
    "\n",
    "# ----------------------\n",
    "# 10. train/test 분리\n",
    "# ----------------------\n",
    "train_mask = X[\"game_episode\"].isin(labels[\"game_episode\"])\n",
    "X_train = X[train_mask].copy()\n",
    "X_test = X[~train_mask].copy()\n",
    "\n",
    "y_train_x = X_train[\"target_x\"].astype(float)\n",
    "y_train_y = X_train[\"target_y\"].astype(float)\n",
    "\n",
    "# 모델 입력에서 빼야 할 컬럼들\n",
    "drop_cols = [\n",
    "    \"game_episode\",\n",
    "    \"game_id\",\n",
    "    \"target_x\",\n",
    "    \"target_y\",\n",
    "]\n",
    "\n",
    "X_train_feat = X_train.drop(columns=drop_cols)\n",
    "X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns])\n",
    "\n",
    "# NaN 채우기 (LGBM은 NaN 다루긴 하지만, 깔끔하게)\n",
    "X_train_feat = X_train_feat.fillna(0)\n",
    "X_test_feat = X_test_feat.fillna(0)\n",
    "\n",
    "# ----------------------\n",
    "# 11. AutoGluon 학습\n",
    "# ----------------------\n",
    "# CatBoost와 LightGBM 사용 (30분)\n",
    "hyperparameters = {\n",
    "    'CAT': {},\n",
    "    'GBM': {},  # LightGBM\n",
    "}\n",
    "time_limit = 1800  # 30분\n",
    "presets = \"good_quality\"\n",
    "print(\"=\" * 50)\n",
    "print(\"CatBoost + LightGBM 사용 (30분)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# X 좌표 예측 모델\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"X 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_x = X_train_feat.copy()\n",
    "train_data_x[\"target_x\"] = y_train_x\n",
    "\n",
    "# 모델 저장 경로\n",
    "model_suffix = \"sideline_goalangle_timepos\"\n",
    "model_path_x = f\"ag_models_x_{model_suffix}\"\n",
    "model_path_y = f\"ag_models_y_{model_suffix}\"\n",
    "\n",
    "print(f\"\\n모델 저장 경로: {model_path_x}, {model_path_y}\")\n",
    "print(f\"사용 모델: CatBoost + LightGBM\")\n",
    "print(f\"사용 피처: dist_to_sideline, angle_to_goal_center, time_pos_inter\")\n",
    "\n",
    "predictor_x = TabularPredictor(\n",
    "    label=\"target_x\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=model_path_x\n",
    ").fit(\n",
    "    train_data=train_data_x,\n",
    "    time_limit=time_limit,\n",
    "    presets=presets,\n",
    "    hyperparameters=hyperparameters,  # 특정 모델만 선택\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Y 좌표 모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_data_y = X_train_feat.copy()\n",
    "train_data_y[\"target_y\"] = y_train_y\n",
    "\n",
    "predictor_y = TabularPredictor(\n",
    "    label=\"target_y\",\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=model_path_y\n",
    ").fit(\n",
    "    train_data=train_data_y,\n",
    "    time_limit=time_limit,\n",
    "    presets=presets,\n",
    "    hyperparameters=hyperparameters,  # 특정 모델만 선택\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 12. test 예측\n",
    "# ----------------------\n",
    "print(\"=\" * 50)\n",
    "print(\"Test 데이터 예측 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pred_x = predictor_x.predict(X_test_feat)\n",
    "pred_y = predictor_y.predict(X_test_feat)\n",
    "\n",
    "# 필드 범위로 클립\n",
    "pred_x = np.clip(pred_x, 0, 105)\n",
    "pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "# ----------------------\n",
    "# 13. submission 생성\n",
    "# ----------------------\n",
    "sub = sample_sub.copy()\n",
    "\n",
    "# X_test에는 game_episode가 있으니, test_index와 align\n",
    "pred_df = X_test[[\"game_episode\"]].copy()\n",
    "pred_df[\"end_x\"] = pred_x\n",
    "pred_df[\"end_y\"] = pred_y\n",
    "\n",
    "sub = sub.drop(columns=[\"end_x\", \"end_y\"], errors=\"ignore\")\n",
    "sub = sub.merge(pred_df, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "# 제출 파일 이름 생성\n",
    "submission_filename = f\"submission_autogluon_{model_suffix}.csv\"\n",
    "sub.to_csv(submission_filename, index=False)\n",
    "print(f\"Saved {submission_filename}\")\n",
    "print(\"\\n학습 및 예측 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11416c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 경로: ag_models_x_lgbm_sideline_goalangle_timepos, ag_models_y_lgbm_sideline_goalangle_timepos\n",
      "제출 파일: submission_autogluon_lgbm_sideline_goalangle_timepos.csv\n",
      "✓ 모델 로드 완료\n",
      "======================================================================\n",
      "======================================================================\n",
      "3단계 검증법: 성능의 실체 확인\n",
      "======================================================================\n",
      "======================================================================\n",
      "\n",
      "1단계: Feature Importance 확인 건너뜀 (RUN_STEP1_FEATURE_IMPORTANCE = False)\n",
      "\n",
      "======================================================================\n",
      "2단계: 리더보드 점수(RMSE) 확인\n",
      "======================================================================\n",
      "체크포인트: X와 Y 좌표의 RMSE가 각각 0.1~0.2 이상 떨어졌는지 확인\n",
      "(11.7 -> 11.5 등)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[X 좌표 모델 - 리더보드 (상위 5개)]\n",
      "                      model  score_val              eval_metric  \\\n",
      "0       WeightedEnsemble_L3 -11.845297  root_mean_squared_error   \n",
      "1           LightGBM_BAG_L1 -11.845349  root_mean_squared_error   \n",
      "2       WeightedEnsemble_L2 -11.845349  root_mean_squared_error   \n",
      "3           LightGBM_BAG_L2 -11.938057  root_mean_squared_error   \n",
      "4  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
      "\n",
      "   pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
      "0       0.368086  24.660502                0.006063           0.007562   \n",
      "1       0.252336  15.568067                0.252336          15.568067   \n",
      "2       0.252793  15.572903                0.000457           0.004836   \n",
      "3       0.362023  24.652940                0.109687           9.084873   \n",
      "4            NaN   4.758645                     NaN           0.007562   \n",
      "\n",
      "   stack_level  can_infer  fit_order  \n",
      "0            3      False          4  \n",
      "1            1      False          1  \n",
      "2            2      False          2  \n",
      "3            2      False          3  \n",
      "4            3       True          8  \n",
      "\n",
      "✓ 최고 X 좌표 RMSE: 11.8453\n",
      "\n",
      "[Y 좌표 모델 - 리더보드 (상위 5개)]\n",
      "                      model  score_val              eval_metric  \\\n",
      "0           LightGBM_BAG_L1 -13.129595  root_mean_squared_error   \n",
      "1       WeightedEnsemble_L3 -13.129595  root_mean_squared_error   \n",
      "2       WeightedEnsemble_L2 -13.129595  root_mean_squared_error   \n",
      "3           LightGBM_BAG_L2 -13.251709  root_mean_squared_error   \n",
      "4  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
      "\n",
      "   pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
      "0       0.178982  14.449641                0.178982          14.449641   \n",
      "1       0.179327  14.455088                0.000345           0.005447   \n",
      "2       0.179354  14.454177                0.000371           0.004536   \n",
      "3       0.325817  23.982806                0.146835           9.533165   \n",
      "4            NaN   2.139948                     NaN           0.005447   \n",
      "\n",
      "   stack_level  can_infer  fit_order  \n",
      "0            1      False          1  \n",
      "1            3      False          4  \n",
      "2            2      False          2  \n",
      "3            2      False          3  \n",
      "4            3       True          8  \n",
      "\n",
      "✓ 최고 Y 좌표 RMSE: 13.1296\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Train 데이터 성능 평가]\n",
      "----------------------------------------------------------------------\n",
      "X 좌표 RMSE (Train): 9.4285\n",
      "Y 좌표 RMSE (Train): 11.2725\n",
      "\n",
      "✓ 전체 RMSE (Train): 14.6958\n",
      "\n",
      "======================================================================\n",
      "3단계: 특정 상황에서의 예측값 확인\n",
      "======================================================================\n",
      "체크포인트: 타겟팅한 상황(예: 후반전)에서의 예측값이\n",
      "'상식적'으로 변했는지 제출 파일의 통계를 확인\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[전체 예측값 통계]\n",
      "  end_x - mean: 67.29, std: 20.70, min: 7.91, max: 100.44\n",
      "  end_y - mean: 33.70, std: 20.07, min: 1.36, max: 66.14\n",
      "\n",
      "  ⚠ period_id 값이 모두 NaN입니다.\n",
      "\n",
      "[제출 파일 기본 정보]\n",
      "  제출 파일 행 수: 2414\n",
      "\n",
      "  제출 파일 샘플:\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  63.775967  10.493587\n",
      "1     153363_2  36.052906  48.944935\n",
      "2     153363_6  38.378952  61.249460\n",
      "3     153363_7  54.431310   9.693746\n",
      "4     153363_8  79.708790  11.446351\n",
      "5     153363_9  76.959680  65.317080\n",
      "6    153363_10  65.994700  12.850666\n",
      "7    153363_12  73.788650  11.550148\n",
      "8    153363_13  33.819800  62.924725\n",
      "9    153363_15  74.700820  12.304502\n",
      "\n",
      "  제출 파일 통계:\n",
      "             end_x        end_y\n",
      "count  2414.000000  2414.000000\n",
      "mean     67.294854    33.697293\n",
      "std      20.701835    20.074920\n",
      "min       7.911255     1.361638\n",
      "25%      51.958196    13.533295\n",
      "50%      72.114415    33.918665\n",
      "75%      84.990740    53.680691\n",
      "max     100.437070    66.138410\n",
      "\n",
      "======================================================================\n",
      "3단계 검증 완료!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 결과 확인 및 3단계 검증법 (학습 완료 후 실행)\n",
    "# ----------------------\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------\n",
    "# 검증 단계 실행 플래그\n",
    "# ----------------------\n",
    "RUN_STEP1_FEATURE_IMPORTANCE = False   # 1단계: Feature Importance 확인\n",
    "RUN_STEP2_LEADERBOARD = True          # 2단계: 리더보드 점수(RMSE) 확인\n",
    "RUN_STEP3_SITUATIONAL_ANALYSIS = True # 3단계: 특정 상황에서의 예측값 확인\n",
    "\n",
    "# 모델 경로\n",
    "model_suffix = \"sideline_goalangle_timepos\"\n",
    "model_path_x = f\"ag_models_x_{model_suffix}\"\n",
    "model_path_y = f\"ag_models_y_{model_suffix}\"\n",
    "submission_filename = f\"submission_autogluon_{model_suffix}.csv\"\n",
    "\n",
    "print(f\"모델 경로: {model_path_x}, {model_path_y}\")\n",
    "print(f\"제출 파일: {submission_filename}\")\n",
    "\n",
    "# 저장된 모델 로드\n",
    "try:\n",
    "    predictor_x = TabularPredictor.load(model_path_x)\n",
    "    predictor_y = TabularPredictor.load(model_path_y)\n",
    "    print(\"✓ 모델 로드 완료\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ 모델 로드 실패: {e}\")\n",
    "    print(\"Cell 0을 먼저 실행하여 모델을 학습하세요.\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"=\" * 70)\n",
    "print(\"3단계 검증법: 성능의 실체 확인\")\n",
    "print(\"=\" * 70)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# 1단계: AutoGluon의 feature_importance 확인\n",
    "# ============================================================\n",
    "if RUN_STEP1_FEATURE_IMPORTANCE:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"1단계: Feature Importance 확인\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"체크포인트: 새로 만든 피처가 상위 10위 안에 들어오는지 확인\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # 새로 추가한 피처 목록 (3개)\n",
    "        new_features = [\n",
    "            \"dist_to_sideline\",      # 사이드라인 압박도\n",
    "            \"angle_to_goal_center\",  # 골대 중심과의 물리적 각도\n",
    "            \"time_pos_inter\"          # 시간 × 전진 위치 상호작용\n",
    "        ]\n",
    "        \n",
    "        # X 좌표 모델의 feature importance\n",
    "        print(\"\\n[X 좌표 모델 - Feature Importance (상위 20개)]\")\n",
    "        importance_x = predictor_x.feature_importance(data=train_data_x)\n",
    "        \n",
    "        # Series인지 DataFrame인지 확인하여 처리\n",
    "        if isinstance(importance_x, pd.DataFrame):\n",
    "            # DataFrame인 경우 첫 번째 컬럼 사용\n",
    "            importance_x = importance_x.iloc[:, 0]\n",
    "        \n",
    "        importance_x_sorted = importance_x.sort_values(ascending=False).head(20)\n",
    "        print(importance_x_sorted)\n",
    "        \n",
    "        print(\"\\n[새로 추가한 피처의 중요도 순위 (X 좌표)]\")\n",
    "        for feat in new_features:\n",
    "            # 피처 이름이 pivot된 형태일 수 있으므로 패턴 매칭\n",
    "            matching_features = [f for f in importance_x_sorted.index if feat in str(f)]\n",
    "            if matching_features:\n",
    "                for mf in matching_features:\n",
    "                    rank = list(importance_x_sorted.index).index(mf) + 1\n",
    "                    importance_val = importance_x_sorted[mf]\n",
    "                    print(f\"  {mf}: 순위 {rank}위, 중요도 {importance_val:.4f}\")\n",
    "            else:\n",
    "                # 전체에서 찾기\n",
    "                all_matching = [f for f in importance_x.index if feat in str(f)]\n",
    "                if all_matching:\n",
    "                    for mf in all_matching[:3]:  # 상위 3개만 표시\n",
    "                        rank = list(importance_x.sort_values(ascending=False).index).index(mf) + 1\n",
    "                        importance_val = importance_x[mf]\n",
    "                        print(f\"  {mf}: 순위 {rank}위, 중요도 {importance_val:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {feat}: 피처를 찾을 수 없음\")\n",
    "        \n",
    "        # Y 좌표 모델의 feature importance\n",
    "        print(\"\\n[Y 좌표 모델 - Feature Importance (상위 20개)]\")\n",
    "        importance_y = predictor_y.feature_importance(data=train_data_y)\n",
    "        \n",
    "        # Series인지 DataFrame인지 확인하여 처리\n",
    "        if isinstance(importance_y, pd.DataFrame):\n",
    "            # DataFrame인 경우 첫 번째 컬럼 사용\n",
    "            importance_y = importance_y.iloc[:, 0]\n",
    "        \n",
    "        importance_y_sorted = importance_y.sort_values(ascending=False).head(20)\n",
    "        print(importance_y_sorted)\n",
    "        \n",
    "        print(\"\\n[새로 추가한 피처의 중요도 순위 (Y 좌표)]\")\n",
    "        for feat in new_features:\n",
    "            matching_features = [f for f in importance_y_sorted.index if feat in str(f)]\n",
    "            if matching_features:\n",
    "                for mf in matching_features:\n",
    "                    rank = list(importance_y_sorted.index).index(mf) + 1\n",
    "                    importance_val = importance_y_sorted[mf]\n",
    "                    print(f\"  {mf}: 순위 {rank}위, 중요도 {importance_val:.4f}\")\n",
    "            else:\n",
    "                # 전체에서 찾기\n",
    "                all_matching = [f for f in importance_y.index if feat in str(f)]\n",
    "                if all_matching:\n",
    "                    for mf in all_matching[:3]:  # 상위 3개만 표시\n",
    "                        rank = list(importance_y.sort_values(ascending=False).index).index(mf) + 1\n",
    "                        importance_val = importance_y[mf]\n",
    "                        print(f\"  {mf}: 순위 {rank}위, 중요도 {importance_val:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {feat}: 피처를 찾을 수 없음\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Feature importance 확인 중 오류 발생: {e}\")\n",
    "        print(f\"오류 상세: {traceback.format_exc()}\")\n",
    "        print(\"(모델이 학습되지 않았거나 데이터가 없는 경우 발생할 수 있습니다)\")\n",
    "else:\n",
    "    print(\"\\n1단계: Feature Importance 확인 건너뜀 (RUN_STEP1_FEATURE_IMPORTANCE = False)\")\n",
    "\n",
    "# ============================================================\n",
    "# 2단계: 리더보드 점수(RMSE) 비교\n",
    "# ============================================================\n",
    "if RUN_STEP2_LEADERBOARD:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"2단계: 리더보드 점수(RMSE) 확인\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"체크포인트: X와 Y 좌표의 RMSE가 각각 0.1~0.2 이상 떨어졌는지 확인\")\n",
    "    print(\"(11.7 -> 11.5 등)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # X 좌표 모델 리더보드\n",
    "    print(\"\\n[X 좌표 모델 - 리더보드 (상위 5개)]\")\n",
    "    leaderboard_x = predictor_x.leaderboard(silent=True)\n",
    "    print(leaderboard_x.head(5))\n",
    "    best_rmse_x = abs(leaderboard_x.iloc[0]['score_val'])\n",
    "    print(f\"\\n✓ 최고 X 좌표 RMSE: {best_rmse_x:.4f}\")\n",
    "    \n",
    "    # Y 좌표 모델 리더보드\n",
    "    print(\"\\n[Y 좌표 모델 - 리더보드 (상위 5개)]\")\n",
    "    leaderboard_y = predictor_y.leaderboard(silent=True)\n",
    "    print(leaderboard_y.head(5))\n",
    "    best_rmse_y = abs(leaderboard_y.iloc[0]['score_val'])\n",
    "    print(f\"\\n✓ 최고 Y 좌표 RMSE: {best_rmse_y:.4f}\")\n",
    "    \n",
    "    # Train 데이터로 성능 평가\n",
    "    try:\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"[Train 데이터 성능 평가]\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # X 좌표 평가\n",
    "        y_pred_train_x = predictor_x.predict(X_train_feat)\n",
    "        rmse_x = np.sqrt(np.mean((y_train_x - y_pred_train_x) ** 2))\n",
    "        print(f\"X 좌표 RMSE (Train): {rmse_x:.4f}\")\n",
    "        \n",
    "        # Y 좌표 평가\n",
    "        y_pred_train_y = predictor_y.predict(X_train_feat)\n",
    "        rmse_y = np.sqrt(np.mean((y_train_y - y_pred_train_y) ** 2))\n",
    "        print(f\"Y 좌표 RMSE (Train): {rmse_y:.4f}\")\n",
    "        \n",
    "        print(f\"\\n✓ 전체 RMSE (Train): {np.sqrt(rmse_x**2 + rmse_y**2):.4f}\")\n",
    "    except NameError:\n",
    "        print(\"\\n(첫 번째 셀을 먼저 실행해야 Train 데이터 평가가 가능합니다)\")\n",
    "else:\n",
    "    print(\"\\n2단계: 리더보드 점수 확인 건너뜀 (RUN_STEP2_LEADERBOARD = False)\")\n",
    "\n",
    "# ============================================================\n",
    "# 3단계: 특정 상황(코너킥/롱볼/후반전)에서의 오차 확인\n",
    "# ============================================================\n",
    "if RUN_STEP3_SITUATIONAL_ANALYSIS:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"3단계: 특정 상황에서의 예측값 확인\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"체크포인트: 타겟팅한 상황(예: 후반전)에서의 예측값이\")\n",
    "    print(\"'상식적'으로 변했는지 제출 파일의 통계를 확인\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # 제출 파일 로드\n",
    "        sub = pd.read_csv(submission_filename)\n",
    "        \n",
    "        # test 데이터와 merge하여 상황별 분석\n",
    "        # X_test에는 이미 ep_meta가 merge되어 있음\n",
    "        if \"game_episode\" in X_test.columns:\n",
    "            # 필요한 컬럼만 선택\n",
    "            needed_cols = [\"game_episode\"]\n",
    "            if \"game_clock_min\" in X_test.columns:\n",
    "                needed_cols.append(\"game_clock_min\")\n",
    "            if \"period_id\" in X_test.columns:\n",
    "                needed_cols.append(\"period_id\")\n",
    "            \n",
    "            test_with_pred = X_test[needed_cols].copy()\n",
    "            \n",
    "            # 제출 파일과 merge\n",
    "            test_with_pred = test_with_pred.merge(\n",
    "                sub[[\"game_episode\", \"end_x\", \"end_y\"]],\n",
    "                on=\"game_episode\",\n",
    "                how=\"left\"\n",
    "            )\n",
    "        else:\n",
    "            # X_test가 없으면 제출 파일만 사용\n",
    "            test_with_pred = sub[[\"game_episode\", \"end_x\", \"end_y\"]].copy()\n",
    "            print(\"  ⚠ X_test 정보를 찾을 수 없어 전체 통계만 표시합니다.\")\n",
    "        \n",
    "        print(\"\\n[전체 예측값 통계]\")\n",
    "        print(f\"  end_x - mean: {test_with_pred['end_x'].mean():.2f}, \"\n",
    "              f\"std: {test_with_pred['end_x'].std():.2f}, \"\n",
    "              f\"min: {test_with_pred['end_x'].min():.2f}, \"\n",
    "              f\"max: {test_with_pred['end_x'].max():.2f}\")\n",
    "        print(f\"  end_y - mean: {test_with_pred['end_y'].mean():.2f}, \"\n",
    "              f\"std: {test_with_pred['end_y'].std():.2f}, \"\n",
    "              f\"min: {test_with_pred['end_y'].min():.2f}, \"\n",
    "              f\"max: {test_with_pred['end_y'].max():.2f}\")\n",
    "        \n",
    "        # 후반전 분석\n",
    "        if 'period_id' in test_with_pred.columns:\n",
    "            period_id_valid = test_with_pred['period_id'].notna().sum()\n",
    "            if period_id_valid > 0:\n",
    "                second_half = test_with_pred[test_with_pred['period_id'] == 2]\n",
    "                first_half = test_with_pred[test_with_pred['period_id'] == 1]\n",
    "                \n",
    "                if len(second_half) > 0:\n",
    "                    print(\"\\n[후반전 예측값 통계]\")\n",
    "                    print(f\"  후반전 샘플 수: {len(second_half)} ({len(second_half)/len(test_with_pred)*100:.1f}%)\")\n",
    "                    print(f\"  end_x - mean: {second_half['end_x'].mean():.2f}, \"\n",
    "                          f\"std: {second_half['end_x'].std():.2f}, \"\n",
    "                          f\"min: {second_half['end_x'].min():.2f}, \"\n",
    "                          f\"max: {second_half['end_x'].max():.2f}\")\n",
    "                    print(f\"  end_y - mean: {second_half['end_y'].mean():.2f}, \"\n",
    "                          f\"std: {second_half['end_y'].std():.2f}, \"\n",
    "                          f\"min: {second_half['end_y'].min():.2f}, \"\n",
    "                          f\"max: {second_half['end_y'].max():.2f}\")\n",
    "                    \n",
    "                    if len(first_half) > 0:\n",
    "                        print(\"\\n[전반전 예측값 통계 (비교용)]\")\n",
    "                        print(f\"  전반전 샘플 수: {len(first_half)}\")\n",
    "                        print(f\"  end_x - mean: {first_half['end_x'].mean():.2f}, \"\n",
    "                              f\"std: {first_half['end_x'].std():.2f}\")\n",
    "                        print(f\"  end_y - mean: {first_half['end_y'].mean():.2f}, \"\n",
    "                              f\"std: {first_half['end_y'].std():.2f}\")\n",
    "                else:\n",
    "                    print(\"\\n  ⚠ 후반전 데이터가 없습니다.\")\n",
    "            else:\n",
    "                print(\"\\n  ⚠ period_id 값이 모두 NaN입니다.\")\n",
    "        else:\n",
    "            print(\"\\n  ⚠ period_id 컬럼이 없어 후반전 분석을 건너뜁니다.\")\n",
    "        \n",
    "        print(\"\\n[제출 파일 기본 정보]\")\n",
    "        print(f\"  제출 파일 행 수: {len(sub)}\")\n",
    "        print(f\"\\n  제출 파일 샘플:\")\n",
    "        print(sub.head(10))\n",
    "        print(f\"\\n  제출 파일 통계:\")\n",
    "        print(sub.describe())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"특정 상황 분석 중 오류 발생: {e}\")\n",
    "        print(\"(제출 파일이 없거나 데이터 구조가 다른 경우 발생할 수 있습니다)\")\n",
    "else:\n",
    "    print(\"\\n3단계: 특정 상황 분석 건너뜀 (RUN_STEP3_SITUATIONAL_ANALYSIS = False)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3단계 검증 완료!\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
