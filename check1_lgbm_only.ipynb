{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_53944/1956295020.py:190: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
            "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_53944/1956295020.py:276: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  X_train_feat = X_train_feat.fillna(0)\n",
            "/var/folders/8g/x3kqv_gx5hdb25l1fv6qd8jw0000gn/T/ipykernel_53944/1956295020.py:277: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  X_test_feat = X_test_feat.fillna(0)\n",
            "Verbosity: 2 (Standard Logging)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "LightGBM 전용 모드: LightGBM만 사용 (10분)\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "X 좌표 모델 학습 시작...\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.13.9\n",
            "Operating System:   Darwin\n",
            "Platform Machine:   arm64\n",
            "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
            "CPU Count:          8\n",
            "Pytorch Version:    2.9.1\n",
            "CUDA Version:       CUDA is not available\n",
            "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
            "Memory Avail:       2.58 GB / 16.00 GB (16.1%)\n",
            "Disk Space Avail:   8.95 GB / 460.43 GB (1.9%)\n",
            "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
            "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
            "===================================================\n",
            "Presets specified: ['good_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
            "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
            "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 150s\n",
            "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_lgbm/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    13720\n",
            "Train Data Columns: 478\n",
            "Label Column:       target_x\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    2666.59 MB\n",
            "\tTrain Data (Original)  Memory Usage: 56.81 MB (2.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('bool', [])  : 1 | ['is_home']\n",
            "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('bool', [])   :   1 | ['is_home_19']\n",
            "\t\t('float', [])  : 434 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
            "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 392 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
            "\t\t('int', ['bool']) :  62 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
            "\t0.7s = Fit runtime\n",
            "\t454 features in original data used to generate 454 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 41.84 MB (1.5% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.78s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 99.46s of the 149.22s of remaining time.\n",
            "2025-12-25 22:01:39,614\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.67% memory usage per fold, 54.69%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=13.67%)\n",
            "/opt/miniconda3/lib/python3.13/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
            "  warnings.warn(\n",
            "\t-11.9302\t = Validation score   (-root_mean_squared_error)\n",
            "\t23.39s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.22s of the 119.75s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.5 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t-11.9302\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 119.73s of the 119.71s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.15% memory usage per fold, 40.59%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.15%)\n",
            "\t-12.0263\t = Validation score   (-root_mean_squared_error)\n",
            "\t13.65s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.22s of the 103.50s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.5 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.917, 'LightGBM_BAG_L2': 0.083}\n",
            "\t-11.9294\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 46.66s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 6035.5 rows/s (1715 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.5 GB\n",
            "\t3.02s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.01s\t = Training   runtime\n",
            "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.7 GB\n",
            "\t1.14s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.917, 'LightGBM_BAG_L2': 0.083}\n",
            "\t0.01s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 4.39s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_lgbm/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val  fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0      LightGBM_BAG_L1_FULL     -11.837549 -11.930182  root_mean_squared_error        0.010552          None  3.015844                 0.010552                   None           3.015844            1       True          1\n",
            "1  WeightedEnsemble_L2_FULL     -11.837549 -11.930182  root_mean_squared_error        0.011761          None  3.024057                 0.001209                   None           0.008213            2       True          2\n",
            "2  WeightedEnsemble_L3_FULL     -11.838541 -11.929385  root_mean_squared_error        0.018541          None  4.166012                 0.001201                   None           0.011228            3       True          4\n",
            "3      LightGBM_BAG_L2_FULL     -11.938948 -12.026260  root_mean_squared_error        0.017340          None  4.154784                 0.006788                   None           1.138940            2       True          3\n",
            "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
            "\t52s\t = DyStack   runtime |\t548s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=0.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
            "Beginning AutoGluon training ... Time limit = 548s\n",
            "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_lgbm\"\n",
            "Train Data Rows:    15435\n",
            "Train Data Columns: 478\n",
            "Label Column:       target_x\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    3849.29 MB\n",
            "\tTrain Data (Original)  Memory Usage: 63.91 MB (1.7% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('bool', [])  : 1 | ['is_home']\n",
            "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('bool', [])   :   1 | ['is_home_19']\n",
            "\t\t('float', [])  : 433 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
            "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 392 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
            "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
            "\t0.5s = Fit runtime\n",
            "\t453 features in original data used to generate 453 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 47.06 MB (1.2% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.66s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "}\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 547.76s of the 547.76s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.37% memory usage per fold, 41.47%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.37%)\n",
            "\t-11.8871\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.73s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 528.64s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.9 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t-11.8871\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.0s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.92s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 16024.9 rows/s (1930 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.8 GB\n",
            "\t2.65s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.0s\t = Training   runtime\n",
            "Updated best model to \"LightGBM_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBM_BAG_L1_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 2.78s ... Best model: \"LightGBM_BAG_L1_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_x_lgbm\")\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.13.9\n",
            "Operating System:   Darwin\n",
            "Platform Machine:   arm64\n",
            "Platform Version:   Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n",
            "CPU Count:          8\n",
            "Pytorch Version:    2.9.1\n",
            "CUDA Version:       CUDA is not available\n",
            "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
            "Memory Avail:       3.76 GB / 16.00 GB (23.5%)\n",
            "Disk Space Avail:   7.03 GB / 460.43 GB (1.5%)\n",
            "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
            "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
            "===================================================\n",
            "Presets specified: ['good_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
            "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
            "DyStack: Disabling memory safe fit mode in DyStack because GPUs were detected and num_gpus='auto' (GPUs cannot be used in memory safe fit mode). If you want to use memory safe fit mode, manually set `num_gpus=0`.\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 150s\n",
            "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_lgbm/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    13720\n",
            "Train Data Columns: 478\n",
            "Label Column:       target_y\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Y 좌표 모델 학습 시작...\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    3738.61 MB\n",
            "\tTrain Data (Original)  Memory Usage: 56.81 MB (1.5% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 3): ['final_team_id', 'is_home', 'period_id']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('bool', [])  : 1 | ['is_home']\n",
            "\t\t('float', []) : 2 | ['final_team_id', 'period_id']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('bool', [])   :   1 | ['is_home_19']\n",
            "\t\t('float', [])  : 434 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
            "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 392 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
            "\t\t('int', ['bool']) :  62 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
            "\t0.6s = Fit runtime\n",
            "\t454 features in original data used to generate 454 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 41.84 MB (1.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.7s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 99.51s of the 149.30s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.71%)\n",
            "\t-13.1141\t = Validation score   (-root_mean_squared_error)\n",
            "\t14.66s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.30s of the 131.92s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.7 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t-13.1141\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 131.91s of the 131.87s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.52%)\n",
            "\t-13.2654\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.69s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.30s of the 121.46s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.0 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t-13.1141\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 28.63s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 8136.1 rows/s (1715 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.0 GB\n",
            "\t1.94s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.01s\t = Training   runtime\n",
            "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.6 GB\n",
            "\t1.31s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.03s\t = Training   runtime\n",
            "Updated best model to \"LightGBM_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBM_BAG_L1_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 3.48s ... Best model: \"LightGBM_BAG_L1_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_lgbm/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                      model  score_holdout  score_val              eval_metric  pred_time_test pred_time_val  fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0      LightGBM_BAG_L1_FULL     -13.473385 -13.114088  root_mean_squared_error        0.006600          None  1.939211                 0.006600                   None           1.939211            1       True          1\n",
            "1  WeightedEnsemble_L3_FULL     -13.473385 -13.114088  root_mean_squared_error        0.007370          None  1.971596                 0.000770                   None           0.032385            3       True          4\n",
            "2  WeightedEnsemble_L2_FULL     -13.473385 -13.114088  root_mean_squared_error        0.007430          None  1.944578                 0.000830                   None           0.005367            2       True          2\n",
            "3      LightGBM_BAG_L2_FULL     -13.505562 -13.265418  root_mean_squared_error        0.012203          None  3.247056                 0.005603                   None           1.307845            2       True          3\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t32s\t = DyStack   runtime |\t568s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 568s\n",
            "AutoGluon will save models to \"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_lgbm\"\n",
            "Train Data Rows:    15435\n",
            "Train Data Columns: 478\n",
            "Label Column:       target_y\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    3716.35 MB\n",
            "\tTrain Data (Original)  Memory Usage: 63.91 MB (1.7% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 64 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 21): ['is_final_team_19', 'is_last_0', 'is_last_1', 'is_last_2', 'is_last_3', 'is_last_4', 'is_last_5', 'is_last_6', 'is_last_7', 'is_last_8', 'is_last_9', 'is_last_10', 'is_last_11', 'is_last_12', 'is_last_13', 'is_last_14', 'is_last_15', 'is_last_16', 'is_last_17', 'is_last_18', 'is_last_19']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 4): ['type_id_19', 'final_team_id', 'is_home', 'period_id']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('bool', [])  : 1 | ['is_home']\n",
            "\t\t('float', []) : 3 | ['type_id_19', 'final_team_id', 'period_id']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('bool', [])   :   1 | ['is_home_19']\n",
            "\t\t('float', [])  : 433 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
            "\t\t('object', []) :  19 | ['is_home_0', 'is_home_1', 'is_home_2', 'is_home_3', 'is_home_4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 392 | ['angle_goal_x_corner_0', 'angle_goal_x_corner_1', 'angle_goal_x_corner_2', 'angle_goal_x_corner_3', 'angle_goal_x_corner_4', ...]\n",
            "\t\t('int', ['bool']) :  61 | ['ep_idx_norm_19', 'is_corner_area_0', 'is_corner_area_1', 'is_corner_area_2', 'is_corner_area_3', ...]\n",
            "\t0.5s = Fit runtime\n",
            "\t453 features in original data used to generate 453 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 47.06 MB (1.3% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.55s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 377.88s of the 566.96s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.77% memory usage per fold, 43.10%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.77%)\n",
            "\t-13.137\t = Validation score   (-root_mean_squared_error)\n",
            "\t12.96s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 551.86s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.9 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t-13.137\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 551.84s of the 551.82s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.94%)\n",
            "\t-13.2516\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.44s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 542.13s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/3.8 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.96, 'LightGBM_BAG_L2': 0.04}\n",
            "\t-13.1369\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 25.44s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 8744.9 rows/s (1930 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/4.3 GB\n",
            "\t1.96s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.01s\t = Training   runtime\n",
            "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.3/3.9 GB\n",
            "\t1.36s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.96, 'LightGBM_BAG_L2': 0.04}\n",
            "\t0.01s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 3.57s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/yangjinmo/Desktop/k_league_ml/ag_models_y_lgbm\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Test 데이터 예측 중...\n",
            "==================================================\n",
            "Saved submission_autogluon_lgbm.csv\n",
            "✓ X 모델 백업 완료: backups/lgbm_only_20251225_220354/ag_models_x_lgbm\n",
            "✓ Y 모델 백업 완료: backups/lgbm_only_20251225_220354/ag_models_y_lgbm\n",
            "✓ 제출 파일 백업 완료: backups/lgbm_only_20251225_220354/submission_autogluon_lgbm.csv\n",
            "\n",
            "백업 완료: backups/lgbm_only_20251225_220354\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "# ----------------------\n",
        "# 0. 설정\n",
        "# ----------------------\n",
        "BASE_PATH = \"open_track1/\"\n",
        "PATH_TRAIN = os.path.join(BASE_PATH, \"train.csv\")\n",
        "PATH_TEST = os.path.join(BASE_PATH, \"test.csv\")\n",
        "PATH_MATCH_INFO = os.path.join(BASE_PATH, \"match_info.csv\")\n",
        "PATH_SAMPLE_SUB = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
        "\n",
        "K = 20   # 마지막 K 이벤트 사용 (20~32 사이 선택)\n",
        "\n",
        "# ----------------------\n",
        "# 1. 데이터 로드\n",
        "# ----------------------\n",
        "train = pd.read_csv(PATH_TRAIN)\n",
        "test_index = pd.read_csv(PATH_TEST)\n",
        "match_info = pd.read_csv(PATH_MATCH_INFO)\n",
        "sample_sub = pd.read_csv(PATH_SAMPLE_SUB)\n",
        "\n",
        "test_events_list = []\n",
        "for _, row in test_index.iterrows():\n",
        "    # path가 \"./test/...\" 형식이므로 BASE_PATH와 결합\n",
        "    test_path = os.path.join(BASE_PATH, row[\"path\"].lstrip(\"./\"))\n",
        "    df_ep = pd.read_csv(test_path)\n",
        "    test_events_list.append(df_ep)\n",
        "\n",
        "test_events = pd.concat(test_events_list, ignore_index=True)\n",
        "\n",
        "train[\"is_train\"] = 1\n",
        "test_events[\"is_train\"] = 0\n",
        "\n",
        "events = pd.concat([train, test_events], ignore_index=True)\n",
        "\n",
        "# ----------------------\n",
        "# 2. 기본 정렬 + episode 내 인덱스\n",
        "# ----------------------\n",
        "events = events.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
        "\n",
        "events[\"event_idx\"] = events.groupby(\"game_episode\").cumcount()\n",
        "events[\"n_events\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\") + 1\n",
        "events[\"ep_idx_norm\"] = events[\"event_idx\"] / (events[\"n_events\"] - 1).clip(lower=1)\n",
        "\n",
        "# ----------------------\n",
        "# 3. 시간/공간 feature\n",
        "# ----------------------\n",
        "# Δt\n",
        "events[\"prev_time\"] = events.groupby(\"game_episode\")[\"time_seconds\"].shift(1)\n",
        "events[\"dt\"] = events[\"time_seconds\"] - events[\"prev_time\"]\n",
        "events[\"dt\"] = events[\"dt\"].fillna(0.0)\n",
        "\n",
        "# 이동량/거리\n",
        "events[\"dx\"] = events[\"end_x\"] - events[\"start_x\"]\n",
        "events[\"dy\"] = events[\"end_y\"] - events[\"start_y\"]\n",
        "events[\"dist\"] = np.sqrt(events[\"dx\"]**2 + events[\"dy\"]**2)\n",
        "\n",
        "# 속도 (dt=0 보호)\n",
        "events[\"speed\"] = events[\"dist\"] / events[\"dt\"].replace(0, 1e-3)\n",
        "\n",
        "# zone / lane (필요시 범위 조정)\n",
        "events[\"x_zone\"] = (events[\"start_x\"] / (105/7)).astype(int).clip(0, 6)\n",
        "events[\"lane\"] = pd.cut(\n",
        "    events[\"start_y\"],\n",
        "    bins=[0, 68/3, 2*68/3, 68],\n",
        "    labels=[0, 1, 2],\n",
        "    include_lowest=True\n",
        ").astype(int)\n",
        "\n",
        "# ----------------------\n",
        "# 3-1. 코너 관련 피처 (Corner Features)\n",
        "# ----------------------\n",
        "# 골대까지 각도 계산 (라디안 → 도)\n",
        "# 골대 중앙: (105, 34)\n",
        "events[\"angle_to_goal\"] = np.arctan2(\n",
        "    34 - events[\"start_y\"],\n",
        "    105 - events[\"start_x\"]\n",
        ") * 180 / np.pi\n",
        "\n",
        "# 코너까지 최단 거리 계산\n",
        "# 공격 방향 코너: (105, 0) 상단, (105, 68) 하단\n",
        "events[\"dist_corner_top\"] = np.sqrt((105 - events[\"start_x\"])**2 + (0 - events[\"start_y\"])**2)\n",
        "events[\"dist_corner_bottom\"] = np.sqrt((105 - events[\"start_x\"])**2 + (68 - events[\"start_y\"])**2)\n",
        "events[\"dist_to_nearest_corner\"] = events[[\"dist_corner_top\", \"dist_corner_bottom\"]].min(axis=1)\n",
        "\n",
        "# 코너 구역 플래그 (X > 100 이면서 Y < 5 또는 Y > 63)\n",
        "events[\"is_corner_area\"] = ((events[\"start_x\"] > 100) & \n",
        "                            ((events[\"start_y\"] < 5) | (events[\"start_y\"] > 63))).astype(int)\n",
        "\n",
        "# 인터랙션 피처 1: angle_to_goal × is_corner_area\n",
        "# 코너 구역일 때만 각도의 영향이 강하게 나타남\n",
        "events[\"angle_goal_x_corner\"] = events[\"angle_to_goal\"] * events[\"is_corner_area\"]\n",
        "\n",
        "# 인터랙션 피처 2: dist_to_nearest_corner × angle_to_goal\n",
        "# 코너 거리와 각도의 조합\n",
        "events[\"dist_corner_x_angle\"] = events[\"dist_to_nearest_corner\"] * events[\"angle_to_goal\"]\n",
        "\n",
        "# ----------------------\n",
        "# 4. 라벨 및 episode-level 메타 (train 전용)\n",
        "# ----------------------\n",
        "train_events = events[events[\"is_train\"] == 1].copy()\n",
        "\n",
        "last_events = (\n",
        "    train_events\n",
        "    .groupby(\"game_episode\", as_index=False)\n",
        "    .tail(1)\n",
        "    .copy()\n",
        ")\n",
        "\n",
        "labels = last_events[[\"game_episode\", \"end_x\", \"end_y\"]].rename(\n",
        "    columns={\"end_x\": \"target_x\", \"end_y\": \"target_y\"}\n",
        ")\n",
        "\n",
        "# episode-level 메타 (마지막 이벤트 기준)\n",
        "ep_meta = last_events[[\"game_episode\", \"game_id\", \"team_id\", \"is_home\", \"period_id\", \"time_seconds\"]].copy()\n",
        "ep_meta = ep_meta.rename(columns={\"team_id\": \"final_team_id\"})\n",
        "\n",
        "# game_clock (분 단위, 0~90+)\n",
        "ep_meta[\"game_clock_min\"] = np.where(\n",
        "    ep_meta[\"period_id\"] == 1,\n",
        "    ep_meta[\"time_seconds\"] / 60.0,\n",
        "    45.0 + ep_meta[\"time_seconds\"] / 60.0\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# 5. 공격 팀 플래그 (final_team vs 상대)\n",
        "# ----------------------\n",
        "# final_team_id를 전체 events에 붙임\n",
        "events = events.merge(\n",
        "    ep_meta[[\"game_episode\", \"final_team_id\"]],\n",
        "    on=\"game_episode\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "events[\"is_final_team\"] = (events[\"team_id\"] == events[\"final_team_id\"]).astype(int)\n",
        "\n",
        "# ----------------------\n",
        "# 6. 입력용 events에서 마지막 이벤트 타깃 정보 가리기\n",
        "# ----------------------\n",
        "# is_last 플래그\n",
        "events[\"last_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\"max\")\n",
        "events[\"is_last\"] = (events[\"event_idx\"] == events[\"last_idx\"]).astype(int)\n",
        "\n",
        "# labels는 이미 뽑아놨으니, 입력쪽에서만 end_x, end_y, dx, dy, dist, speed 지움\n",
        "mask_last = events[\"is_last\"] == 1\n",
        "for col in [\"end_x\", \"end_y\", \"dx\", \"dy\", \"dist\", \"speed\"]:\n",
        "    events.loc[mask_last, col] = np.nan\n",
        "\n",
        "# ----------------------\n",
        "# 7. 카테고리 인코딩 (type_name, result_name, team_id 등)\n",
        "# ----------------------\n",
        "events[\"type_name\"] = events[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
        "events[\"result_name\"] = events[\"result_name\"].fillna(\"__NA_RES__\")\n",
        "\n",
        "le_type = LabelEncoder()\n",
        "le_res = LabelEncoder()\n",
        "\n",
        "events[\"type_id\"] = le_type.fit_transform(events[\"type_name\"])\n",
        "events[\"res_id\"] = le_res.fit_transform(events[\"result_name\"])\n",
        "\n",
        "# team_id는 그대로 써도 되지만, 문자열이면 숫자로 매핑\n",
        "if events[\"team_id\"].dtype == \"object\":\n",
        "    le_team = LabelEncoder()\n",
        "    events[\"team_id_enc\"] = le_team.fit_transform(events[\"team_id\"])\n",
        "else:\n",
        "    events[\"team_id_enc\"] = events[\"team_id\"].astype(int)\n",
        "\n",
        "# ----------------------\n",
        "# 8. 마지막 K 이벤트만 사용 (lastK)\n",
        "# ----------------------\n",
        "# rev_idx: 0이 마지막 이벤트\n",
        "events[\"rev_idx\"] = events.groupby(\"game_episode\")[\"event_idx\"].transform(\n",
        "    lambda s: s.max() - s\n",
        ")\n",
        "\n",
        "lastK = events[events[\"rev_idx\"] < K].copy()\n",
        "\n",
        "# pos_in_K: 0~(K-1), 앞쪽 패딩 고려해서 뒤에 실제 이벤트가 모이게\n",
        "def assign_pos_in_K(df):\n",
        "    df = df.sort_values(\"event_idx\")  # 오래된 → 최근\n",
        "    L = len(df)\n",
        "    df = df.copy()\n",
        "    df[\"pos_in_K\"] = np.arange(K - L, K)\n",
        "    return df\n",
        "\n",
        "lastK = lastK.groupby(\"game_episode\", group_keys=False).apply(assign_pos_in_K)\n",
        "\n",
        "# ----------------------\n",
        "# 9. wide feature pivot\n",
        "# ----------------------\n",
        "# 사용할 이벤트 피처 선택\n",
        "num_cols = [\n",
        "    \"start_x\", \"start_y\",\n",
        "    \"end_x\", \"end_y\",\n",
        "    \"dx\", \"dy\", \"dist\", \"speed\",\n",
        "    \"dt\",\n",
        "    \"ep_idx_norm\",\n",
        "    \"x_zone\", \"lane\",\n",
        "    \"is_final_team\",\n",
        "    # 코너 관련 피처\n",
        "    \"angle_to_goal\",\n",
        "    \"dist_to_nearest_corner\",\n",
        "    \"is_corner_area\",\n",
        "    \"angle_goal_x_corner\",  # 인터랙션: angle_to_goal × is_corner_area\n",
        "    \"dist_corner_x_angle\",  # 인터랙션: dist_to_nearest_corner × angle_to_goal\n",
        "]\n",
        "\n",
        "cat_cols = [\n",
        "    \"type_id\",\n",
        "    \"res_id\",\n",
        "    \"team_id_enc\",\n",
        "    \"is_home\",\n",
        "    \"period_id\",\n",
        "    \"is_last\",\n",
        "]\n",
        "\n",
        "feature_cols = num_cols + cat_cols\n",
        "\n",
        "wide = lastK[[\"game_episode\", \"pos_in_K\"] + feature_cols].copy()\n",
        "\n",
        "# 숫자형 pivot\n",
        "wide_num = wide.pivot_table(\n",
        "    index=\"game_episode\",\n",
        "    columns=\"pos_in_K\",\n",
        "    values=num_cols,\n",
        "    aggfunc=\"first\"\n",
        ")\n",
        "\n",
        "# 범주형 pivot\n",
        "wide_cat = wide.pivot_table(\n",
        "    index=\"game_episode\",\n",
        "    columns=\"pos_in_K\",\n",
        "    values=cat_cols,\n",
        "    aggfunc=\"first\"\n",
        ")\n",
        "\n",
        "# 컬럼 이름 평탄화\n",
        "wide_num.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_num.columns]\n",
        "wide_cat.columns = [f\"{c}_{int(pos)}\" for (c, pos) in wide_cat.columns]\n",
        "\n",
        "X = pd.concat([wide_num, wide_cat], axis=1).reset_index()  # game_episode 포함\n",
        "\n",
        "# episode-level 메타 붙이기\n",
        "X = X.merge(ep_meta[[\"game_episode\", \"game_id\", \"game_clock_min\", \"final_team_id\", \"is_home\", \"period_id\"]],\n",
        "            on=\"game_episode\", how=\"left\")\n",
        "\n",
        "# train 라벨 붙이기\n",
        "X = X.merge(labels, on=\"game_episode\", how=\"left\")  # test는 NaN\n",
        "\n",
        "# ----------------------\n",
        "# 10. train/test 분리\n",
        "# ----------------------\n",
        "train_mask = X[\"game_episode\"].isin(labels[\"game_episode\"])\n",
        "X_train = X[train_mask].copy()\n",
        "X_test = X[~train_mask].copy()\n",
        "\n",
        "y_train_x = X_train[\"target_x\"].astype(float)\n",
        "y_train_y = X_train[\"target_y\"].astype(float)\n",
        "\n",
        "# 모델 입력에서 빼야 할 컬럼들\n",
        "drop_cols = [\n",
        "    \"game_episode\",\n",
        "    \"game_id\",\n",
        "    \"target_x\",\n",
        "    \"target_y\",\n",
        "]\n",
        "\n",
        "X_train_feat = X_train.drop(columns=drop_cols)\n",
        "X_test_feat = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns])\n",
        "\n",
        "# NaN 채우기 (LGBM은 NaN 다루긴 하지만, 깔끔하게)\n",
        "X_train_feat = X_train_feat.fillna(0)\n",
        "X_test_feat = X_test_feat.fillna(0)\n",
        "\n",
        "# ----------------------\n",
        "# 11. AutoGluon 학습 (LightGBM만 사용)\n",
        "# ----------------------\n",
        "# LightGBM만 사용하는 모드\n",
        "FAST_TEST_MODE = True  # False로 변경하면 모든 모델 사용 (best_quality)\n",
        "\n",
        "if FAST_TEST_MODE:\n",
        "    # LightGBM만 선택\n",
        "    hyperparameters = {\n",
        "        'GBM': {},  # LightGBM만\n",
        "    }\n",
        "    time_limit = 600  # 10분 (10분: 600, 15분: 900, 20분: 1200)\n",
        "    presets = \"good_quality\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"LightGBM 전용 모드: LightGBM만 사용 (10분)\")\n",
        "    print(\"=\" * 50)\n",
        "else:\n",
        "    # 모든 모델 사용 (최종 제출용)\n",
        "    hyperparameters = None\n",
        "    time_limit = 1800  # 30분\n",
        "    presets = \"best_quality\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"전체 모델 학습 모드: 모든 모델 사용 (30분)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# X 좌표 예측 모델\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"X 좌표 모델 학습 시작...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "train_data_x = X_train_feat.copy()\n",
        "train_data_x[\"target_x\"] = y_train_x\n",
        "\n",
        "predictor_x = TabularPredictor(\n",
        "    label=\"target_x\",\n",
        "    problem_type=\"regression\",\n",
        "    eval_metric=\"rmse\",\n",
        "    path=\"ag_models_x_lgbm\"  # 모델 저장 경로 (LightGBM 전용)\n",
        ").fit(\n",
        "    train_data=train_data_x,\n",
        "    time_limit=time_limit,\n",
        "    presets=presets,\n",
        "    hyperparameters=hyperparameters,  # LightGBM만 선택\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Y 좌표 모델 학습 시작...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "train_data_y = X_train_feat.copy()\n",
        "train_data_y[\"target_y\"] = y_train_y\n",
        "\n",
        "predictor_y = TabularPredictor(\n",
        "    label=\"target_y\",\n",
        "    problem_type=\"regression\",\n",
        "    eval_metric=\"rmse\",\n",
        "    path=\"ag_models_y_lgbm\"  # 모델 저장 경로 (LightGBM 전용)\n",
        ").fit(\n",
        "    train_data=train_data_y,\n",
        "    time_limit=time_limit,\n",
        "    presets=presets,\n",
        "    hyperparameters=hyperparameters,  # LightGBM만 선택\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# 12. test 예측\n",
        "# ----------------------\n",
        "print(\"=\" * 50)\n",
        "print(\"Test 데이터 예측 중...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "pred_x = predictor_x.predict(X_test_feat)\n",
        "pred_y = predictor_y.predict(X_test_feat)\n",
        "\n",
        "# 필드 범위로 클립\n",
        "pred_x = np.clip(pred_x, 0, 105)\n",
        "pred_y = np.clip(pred_y, 0, 68)\n",
        "\n",
        "# ----------------------\n",
        "# 13. submission 생성\n",
        "# ----------------------\n",
        "sub = sample_sub.copy()\n",
        "\n",
        "# X_test에는 game_episode가 있으니, test_index와 align\n",
        "pred_df = X_test[[\"game_episode\"]].copy()\n",
        "pred_df[\"end_x\"] = pred_x\n",
        "pred_df[\"end_y\"] = pred_y\n",
        "\n",
        "sub = sub.drop(columns=[\"end_x\", \"end_y\"], errors=\"ignore\")\n",
        "sub = sub.merge(pred_df, on=\"game_episode\", how=\"left\")\n",
        "\n",
        "sub.to_csv(\"submission_autogluon_lgbm.csv\", index=False)\n",
        "print(\"Saved submission_autogluon_lgbm.csv\")\n",
        "\n",
        "# ----------------------\n",
        "# 14. 백업 생성 (모델 및 제출 파일)\n",
        "# ----------------------\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "backup_dir = \"backups\"\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "backup_subdir = os.path.join(backup_dir, f\"lgbm_only_{timestamp}\")\n",
        "os.makedirs(backup_subdir, exist_ok=True)\n",
        "\n",
        "# 모델 백업\n",
        "if os.path.exists(\"ag_models_x_lgbm\"):\n",
        "    shutil.copytree(\"ag_models_x_lgbm\", os.path.join(backup_subdir, \"ag_models_x_lgbm\"))\n",
        "    print(f\"✓ X 모델 백업 완료: {backup_subdir}/ag_models_x_lgbm\")\n",
        "\n",
        "if os.path.exists(\"ag_models_y_lgbm\"):\n",
        "    shutil.copytree(\"ag_models_y_lgbm\", os.path.join(backup_subdir, \"ag_models_y_lgbm\"))\n",
        "    print(f\"✓ Y 모델 백업 완료: {backup_subdir}/ag_models_y_lgbm\")\n",
        "\n",
        "# 제출 파일 백업\n",
        "if os.path.exists(\"submission_autogluon_lgbm.csv\"):\n",
        "    shutil.copy(\"submission_autogluon_lgbm.csv\", os.path.join(backup_subdir, \"submission_autogluon_lgbm.csv\"))\n",
        "    print(f\"✓ 제출 파일 백업 완료: {backup_subdir}/submission_autogluon_lgbm.csv\")\n",
        "\n",
        "print(f\"\\n백업 완료: {backup_subdir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "모델 성능 확인\n",
            "==================================================\n",
            "\n",
            "[X 좌표 모델 - 리더보드]\n",
            "                      model  score_val              eval_metric  \\\n",
            "0           LightGBM_BAG_L1 -11.887144  root_mean_squared_error   \n",
            "1       WeightedEnsemble_L2 -11.887144  root_mean_squared_error   \n",
            "2  WeightedEnsemble_L2_FULL        NaN  root_mean_squared_error   \n",
            "3      LightGBM_BAG_L1_FULL        NaN  root_mean_squared_error   \n",
            "\n",
            "   pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
            "0       0.120381  16.730280                0.120381          16.730280   \n",
            "1       0.120834  16.734374                0.000452           0.004094   \n",
            "2            NaN   2.651163                     NaN           0.004094   \n",
            "3            NaN   2.647069                     NaN           2.647069   \n",
            "\n",
            "   stack_level  can_infer  fit_order  \n",
            "0            1      False          1  \n",
            "1            2      False          2  \n",
            "2            2       True          4  \n",
            "3            1       True          3  \n",
            "\n",
            "[Y 좌표 모델 - 리더보드]\n",
            "                      model  score_val              eval_metric  \\\n",
            "0       WeightedEnsemble_L3 -13.136926  root_mean_squared_error   \n",
            "1           LightGBM_BAG_L1 -13.136993  root_mean_squared_error   \n",
            "2       WeightedEnsemble_L2 -13.136993  root_mean_squared_error   \n",
            "3           LightGBM_BAG_L2 -13.251616  root_mean_squared_error   \n",
            "4  WeightedEnsemble_L3_FULL        NaN  root_mean_squared_error   \n",
            "5  WeightedEnsemble_L2_FULL        NaN  root_mean_squared_error   \n",
            "6      LightGBM_BAG_L2_FULL        NaN  root_mean_squared_error   \n",
            "7      LightGBM_BAG_L1_FULL        NaN  root_mean_squared_error   \n",
            "\n",
            "   pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
            "0       0.220948  20.409649                0.000283           0.012017   \n",
            "1       0.100768  12.960712                0.100768          12.960712   \n",
            "2       0.101087  12.971257                0.000319           0.010545   \n",
            "3       0.220664  20.397633                0.119896           7.436921   \n",
            "4            NaN   3.325685                     NaN           0.012017   \n",
            "5            NaN   1.967900                     NaN           0.010545   \n",
            "6            NaN   3.313668                     NaN           1.356313   \n",
            "7            NaN   1.957355                     NaN           1.957355   \n",
            "\n",
            "   stack_level  can_infer  fit_order  \n",
            "0            3      False          4  \n",
            "1            1      False          1  \n",
            "2            2      False          2  \n",
            "3            2      False          3  \n",
            "4            3       True          8  \n",
            "5            2       True          6  \n",
            "6            2       True          7  \n",
            "7            1       True          5  \n",
            "\n",
            "==================================================\n",
            "Train 데이터 성능 평가\n",
            "==================================================\n",
            "\n",
            "X 좌표 RMSE (Train): 9.6201\n",
            "Y 좌표 RMSE (Train): 11.2277\n",
            "\n",
            "==================================================\n",
            "제출 파일 확인\n",
            "==================================================\n",
            "\n",
            "제출 파일 행 수: 2414\n",
            "\n",
            "제출 파일 샘플:\n",
            "  game_episode      end_x      end_y\n",
            "0     153363_1  64.482670  10.915547\n",
            "1     153363_2  31.651043  50.141830\n",
            "2     153363_6  38.425190  61.065120\n",
            "3     153363_7  55.334206  10.016491\n",
            "4     153363_8  79.883610  11.790058\n",
            "5     153363_9  75.734950  64.578606\n",
            "6    153363_10  63.722540  13.651059\n",
            "7    153363_12  72.285995  10.099366\n",
            "8    153363_13  34.382267  62.649956\n",
            "9    153363_15  75.221320  11.601659\n",
            "\n",
            "제출 파일 통계:\n",
            "             end_x        end_y\n",
            "count  2414.000000  2414.000000\n",
            "mean     67.205308    33.693072\n",
            "std      20.657997    20.062010\n",
            "min       8.878476     0.932446\n",
            "25%      51.843312    13.416126\n",
            "50%      72.010607    33.880524\n",
            "75%      84.918358    53.900626\n",
            "max      99.912530    66.494150\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# 결과 확인 (학습 완료 후 실행)\n",
        "# ----------------------\n",
        "from autogluon.tabular import TabularPredictor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 저장된 모델 로드 (LightGBM 전용)\n",
        "predictor_x = TabularPredictor.load(\"ag_models_x_lgbm\")\n",
        "predictor_y = TabularPredictor.load(\"ag_models_y_lgbm\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"모델 성능 확인\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# X 좌표 모델 리더보드\n",
        "print(\"\\n[X 좌표 모델 - 리더보드]\")\n",
        "leaderboard_x = predictor_x.leaderboard(silent=True)\n",
        "print(leaderboard_x.head(10))\n",
        "\n",
        "# Y 좌표 모델 리더보드\n",
        "print(\"\\n[Y 좌표 모델 - 리더보드]\")\n",
        "leaderboard_y = predictor_y.leaderboard(silent=True)\n",
        "print(leaderboard_y.head(10))\n",
        "\n",
        "# Train 데이터로 성능 평가 (첫 번째 셀 실행 후 사용 가능)\n",
        "try:\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Train 데이터 성능 평가\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # X 좌표 평가\n",
        "    y_pred_train_x = predictor_x.predict(X_train_feat)\n",
        "    rmse_x = np.sqrt(np.mean((y_train_x - y_pred_train_x) ** 2))\n",
        "    print(f\"\\nX 좌표 RMSE (Train): {rmse_x:.4f}\")\n",
        "    \n",
        "    # Y 좌표 평가\n",
        "    y_pred_train_y = predictor_y.predict(X_train_feat)\n",
        "    rmse_y = np.sqrt(np.mean((y_train_y - y_pred_train_y) ** 2))\n",
        "    print(f\"Y 좌표 RMSE (Train): {rmse_y:.4f}\")\n",
        "except NameError:\n",
        "    print(\"\\n(첫 번째 셀을 먼저 실행해야 Train 데이터 평가가 가능합니다)\")\n",
        "\n",
        "# 제출 파일 확인\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"제출 파일 확인\")\n",
        "print(\"=\" * 50)\n",
        "sub = pd.read_csv(\"submission_autogluon_lgbm.csv\")\n",
        "print(f\"\\n제출 파일 행 수: {len(sub)}\")\n",
        "print(f\"\\n제출 파일 샘플:\")\n",
        "print(sub.head(10))\n",
        "print(f\"\\n제출 파일 통계:\")\n",
        "print(sub.describe())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
